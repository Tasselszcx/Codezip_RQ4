"""
ç®€å•çš„å¤šé¡µ OCR + åˆå¹¶ + å¯¹æ¯”è„šæœ¬
ç”¨æ³•: 
  python ocr_and_compare.py <å›¾ç‰‡ç›®å½•> <code_id>  # å®æ—¶ OCR æ¨¡å¼
  python ocr_and_compare.py --from-cache <code_id> <ratio>  # ä»ç¼“å­˜è¯»å–æ¨¡å¼
"""
import os
import sys
import json
import glob
import base64
import time
#from openai import OpenAI
import difflib
import re


def normalize_code(text: str) -> str:
    """ä»£ç è§„èŒƒåŒ–ï¼ˆä»…ç”¨äºè¯„ä¼°æŒ‡æ ‡ï¼Œä¸æ”¹å˜ OCR åŸå§‹è¾“å‡ºçš„ä¿å­˜ï¼‰ï¼š

    - Tab â†’ 4 spacesï¼šç»Ÿä¸€ç¼©è¿›é£æ ¼
    - å»é™¤è¡Œå°¾ç©ºæ ¼ï¼šæ¶ˆé™¤ trailing spaces å™ªå£°
    - å‹ç¼©è¿ç»­ç©ºè¡Œï¼šå¤šä¸ªç©ºè¡Œ â†’ å•ä¸ªç©ºè¡Œ
    - å»é™¤é¦–å°¾ç©ºè¡Œï¼šç»Ÿä¸€æ–‡ä»¶å¤´å°¾æ ¼å¼
    """
    lines = text.splitlines()

    # Tab â†’ 4 spaces + å»é™¤è¡Œå°¾ç©ºæ ¼
    lines = [line.replace('\t', '    ').rstrip() for line in lines]

    # å‹ç¼©è¿ç»­ç©ºè¡Œ
    normalized: list[str] = []
    prev_blank = False
    for line in lines:
        is_blank = (line.strip() == '')
        if is_blank:
            if not prev_blank:
                normalized.append('')
            prev_blank = True
        else:
            normalized.append(line)
            prev_blank = False

    # å»é™¤é¦–å°¾ç©ºè¡Œ
    while normalized and normalized[0] == '':
        normalized.pop(0)
    while normalized and normalized[-1] == '':
        normalized.pop()

    return '\n'.join(normalized)


def smart_join_pages(ocr_pages):
    """æ™ºèƒ½æ‹¼æ¥å¤šé¡µ OCR ç»“æœï¼Œä¿ç•™ç¼©è¿›ä¸Šä¸‹æ–‡"""
    if not ocr_pages:
        return ''
    
    if len(ocr_pages) == 1:
        return ocr_pages[0]
    
    result = [ocr_pages[0]]
    
    for i in range(1, len(ocr_pages)):
        prev_page = ocr_pages[i-1]
        curr_page = ocr_pages[i]
        
        if not prev_page.strip() or not curr_page.strip():
            result.append(curr_page)
            continue
        
        prev_lines = prev_page.splitlines()
        curr_lines = curr_page.splitlines()
        
        # è·å–ä¸Šä¸€é¡µæœ€åä¸€ä¸ªéç©ºè¡Œçš„ç¼©è¿›
        prev_last_line = None
        for line in reversed(prev_lines):
            if line.strip():
                prev_last_line = line
                break
        
        # è·å–å½“å‰é¡µç¬¬ä¸€ä¸ªéç©ºè¡Œçš„ç¼©è¿›
        curr_first_line = None
        for line in curr_lines:
            if line.strip():
                curr_first_line = line
                break
        
        # æ£€æµ‹ç¼©è¿›å·®å¼‚å¹¶ä¿®æ­£
        if prev_last_line and curr_first_line:
            prev_indent = len(prev_last_line) - len(prev_last_line.lstrip())
            curr_indent = len(curr_first_line) - len(curr_first_line.lstrip())
            
            # å¦‚æœå½“å‰é¡µç¬¬ä¸€è¡Œç¼©è¿›å¼‚å¸¸ï¼ˆæ¯”ä¸Šä¸€é¡µå°‘å¾ˆå¤šï¼‰ï¼Œå¯èƒ½æ˜¯è·¨é¡µç¼©è¿›ä¸¢å¤±
            # è¿™é‡Œæˆ‘ä»¬ä¿å®ˆå¤„ç†ï¼šåªåœ¨æ˜æ˜¾æ˜¯ç±»/å‡½æ•°å»¶ç»­æ—¶æ‰è°ƒæ•´
            indent_diff = prev_indent - curr_indent
            
            # å¦‚æœä¸Šä¸€é¡µæœ€åä¸€è¡Œæœ‰ç¼©è¿›ï¼Œè€Œå½“å‰é¡µç¬¬ä¸€è¡Œç¼©è¿›ä¸º0ï¼Œä¸”ä¸æ˜¯æ–°çš„é¡¶å±‚å®šä¹‰
            # å¾ˆå¯èƒ½æ˜¯ç¼©è¿›ä¸¢å¤±
            if (prev_indent >= 4 and curr_indent == 0 and 
                not curr_first_line.strip().startswith(('def ', 'class ', 'import ', 'from '))):
                # å¯èƒ½éœ€è¦ç»§æ‰¿ä¸Šä¸€é¡µçš„ç¼©è¿›
                # ä½†è¿™ä¸ªå¯å‘å¼å¯èƒ½ä¸å‡†ç¡®ï¼Œæ‰€ä»¥æš‚æ—¶åªè®°å½•ï¼Œä¸å¼ºåˆ¶ä¿®æ­£
                pass
        
        result.append(curr_page)
    
    return '\n'.join(result)


def load_api_key():
    """åŠ è½½ API Key"""
    api_key = os.getenv("AIHUBMIX_API_KEY")
    if not api_key:
        # å°è¯•ä»å¤šä¸ªä½ç½®æŸ¥æ‰¾ .env æ–‡ä»¶
        script_dir = os.path.dirname(os.path.abspath(__file__))
        env_paths = [
            ".env",  # å½“å‰ç›®å½•
            os.path.join(script_dir, ".env"),  # è„šæœ¬æ‰€åœ¨ç›®å½•
            os.path.join(script_dir, "..", ".env"),  # ä¸Šçº§ç›®å½•
        ]
        
        for env_file in env_paths:
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if line.strip().startswith("AIHUBMIX_API_KEY="):
                            api_key = line.split("=", 1)[1].strip().strip('"').strip("'")
                            break
                if api_key:
                    break
    return api_key


def ocr_image(image_path, api_key, max_retries=3):
    """å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œ OCRï¼Œå¸¦é‡è¯•"""
    print(f"  [OCR] {os.path.basename(image_path)}", end='', flush=True)
    
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    data_url = f"data:image/png;base64,{b64}"
    
    client = OpenAI(api_key=api_key, base_url="https://aihubmix.com/v1")
    
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model="glm-4.6v",
                temperature=0.0,
                max_tokens=4096,
                messages=[
                    {"role": "system", "content": "You are an OCR engine for code images. Your output must preserve the exact formatting of the code."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Transcribe the code EXACTLY as shown in the image. Preserve all blank lines, indentation, and formatting EXACTLY. Do not remove empty lines. Do not modify whitespace. Output only the raw code text without any markdown formatting."},
                            {"type": "image_url", "image_url": {"url": data_url}},
                        ],
                    },
                ],
            )
            
            text = (resp.choices[0].message.content or "").strip()
            text = text.replace('<|begin_of_box|>', '').replace('<|end_of_box|>', '')
            
            if text:
                print(f" -> {len(text)} chars")
                return text
            else:
                print(f" -> empty (retry {attempt+1})", flush=True)
                if attempt < max_retries - 1:
                    time.sleep(2)
        except Exception as e:
            print(f" -> error: {e} (retry {attempt+1})", flush=True)
            if attempt < max_retries - 1:
                time.sleep(2)
    
    print(f" -> FAILED")
    return ""


def calculate_metrics(reference, hypothesis):
    """è®¡ç®— CER, WER, BLEU"""
    def lev_dist(s1, s2):
        if len(s1) < len(s2):
            return lev_dist(s2, s1)
        if len(s2) == 0:
            return len(s1)
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]
    
    # CER
    cer = lev_dist(reference, hypothesis) / len(reference) * 100
    
    # WER
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    wer = lev_dist(ref_words, hyp_words) / len(ref_words) * 100
    
    # BLEU
    from collections import Counter
    import math
    
    precisions = []
    for n in range(1, 5):
        ref_ngrams = Counter([tuple(ref_words[i:i+n]) for i in range(len(ref_words)-n+1)])
        hyp_ngrams = Counter([tuple(hyp_words[i:i+n]) for i in range(len(hyp_words)-n+1)])
        matches = sum((ref_ngrams & hyp_ngrams).values())
        total = sum(hyp_ngrams.values())
        precisions.append(matches / total if total > 0 else 0)
    
    if any(p == 0 for p in precisions):
        bleu = 0
    else:
        geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))
        bp = 1.0 if len(hyp_words) >= len(ref_words) else math.exp(1 - len(ref_words) / len(hyp_words))
        bleu = bp * geo_mean * 100
    
    return cer, wer, bleu


def load_from_cache(code_id, ratio):
    """ä» glm46v_ocr.jsonl åŠ è½½å·²æœ‰çš„ OCR ç»“æœ"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cache_path = os.path.join(script_dir, "..", "experiment_output", "glm46v_ocr.jsonl")
    
    if not os.path.exists(cache_path):
        print(f"[ERROR] Cache file not found: {cache_path}")
        return None
    
    ocr_pages = []
    with open(cache_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            item = json.loads(line)
            if item['code_id'] == code_id and item['ratio'] == int(ratio):
                text = item['text'].replace('<|begin_of_box|>', '').replace('<|end_of_box|>', '').strip()
                ocr_pages.append(text)
    
    if not ocr_pages:
        return None
    
    # ä½¿ç”¨æ™ºèƒ½æ‹¼æ¥è€Œä¸æ˜¯ç®€å•çš„ join
    return smart_join_pages(ocr_pages)


def print_diff_report(original_code, ocr_text):
    """æ‰“å°è¯¦ç»†çš„å·®å¼‚å¯¹æ¯”æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("DETAILED DIFF REPORT")
    print("=" * 80)
    
    ref_lines = original_code.splitlines()
    ocr_lines = ocr_text.splitlines()
    
    print(f"\nğŸ“Š Lines: Original={len(ref_lines)}, OCR={len(ocr_lines)}")
    
    # ä½¿ç”¨ difflib ç”Ÿæˆå·®å¼‚
    diff = list(difflib.unified_diff(
        ref_lines, 
        ocr_lines, 
        fromfile='Original', 
        tofile='OCR',
        lineterm=''
    ))
    
    if len(diff) <= 2:  # åªæœ‰å¤´éƒ¨ä¿¡æ¯ï¼Œæ²¡æœ‰å®é™…å·®å¼‚
        print("\nâœ… No differences found (line-by-line match)")
        return
    
    print(f"\nâŒ Found {len([d for d in diff if d.startswith('-') or d.startswith('+')])} diff lines")
    print("\n--- Unified Diff ---")
    for line in diff[:100]:  # é™åˆ¶è¾“å‡ºå‰100è¡Œ
        if line.startswith('-'):
            print(f"\033[91m{line}\033[0m")  # çº¢è‰²
        elif line.startswith('+'):
            print(f"\033[92m{line}\033[0m")  # ç»¿è‰²
        elif line.startswith('@'):
            print(f"\033[94m{line}\033[0m")  # è“è‰²
        else:
            print(line)
    
    if len(diff) > 100:
        print(f"\n... (truncated, {len(diff) - 100} more lines)")
    
    # é€è¡Œå¯¹æ¯”å¹¶æ ‡æ³¨å·®å¼‚
    print("\n--- Line-by-Line Comparison (first 30 mismatches) ---")
    mismatch_count = 0
    for i in range(max(len(ref_lines), len(ocr_lines))):
        if mismatch_count >= 30:
            print(f"\n... (truncated, more mismatches exist)")
            break
        
        ref = ref_lines[i] if i < len(ref_lines) else ""
        ocr = ocr_lines[i] if i < len(ocr_lines) else ""
        
        if ref.strip() != ocr.strip():
            mismatch_count += 1
            print(f"\nğŸ”´ Line {i+1}:")
            print(f"  REF: {repr(ref)}")
            print(f"  OCR: {repr(ocr)}")
            
            # å­—ç¬¦çº§å·®å¼‚
            if ref and ocr:
                s = difflib.SequenceMatcher(None, ref, ocr)
                char_diffs = []
                for tag, i1, i2, j1, j2 in s.get_opcodes():
                    if tag == 'replace':
                        char_diffs.append(f"pos {i1}-{i2}: '{ref[i1:i2]}' -> '{ocr[j1:j2]}'")
                    elif tag == 'delete':
                        char_diffs.append(f"pos {i1}-{i2}: deleted '{ref[i1:i2]}'")
                    elif tag == 'insert':
                        char_diffs.append(f"pos {i1}: inserted '{ocr[j1:j2]}'")
                if char_diffs:
                    print(f"  Diff: {'; '.join(char_diffs[:5])}")


def main_from_cache(code_id, ratio):
    """ä»ç¼“å­˜è¯»å–å¹¶å¯¹æ¯”çš„æ¨¡å¼"""
    print(f"ğŸ”„ Loading from cache: {code_id} @ ratio {ratio}x")
    
    # Load OCR from cache
    ocr_text = load_from_cache(code_id, ratio)
    if ocr_text is None:
        print(f"[ERROR] No OCR results found for {code_id} @ ratio {ratio}")
        return
    
    print(f"âœ… Loaded OCR: {len(ocr_text)} chars, {len(ocr_text.splitlines())} lines")
    
    # Load original code
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_path = os.path.join(script_dir, "..", "experiment_output", "dataset.json")
    
    if not os.path.exists(dataset_path):
        print(f"[ERROR] dataset.json not found at: {dataset_path}")
        return
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = {item['id']: item for item in json.load(f)}
    
    if code_id not in dataset:
        print(f"[ERROR] code_id not found: {code_id}")
        return
    
    original_code = dataset[code_id]['code']
    print(f"âœ… Loaded original: {len(original_code)} chars, {len(original_code.splitlines())} lines")
    
    # è§„èŒƒåŒ–ä»£ç åå†è®¡ç®—æŒ‡æ ‡ï¼ˆç”¨äºéªŒè¯ï¼štab/è¡Œå°¾ç©ºæ ¼/ç©ºè¡Œ/é¦–å°¾ç©ºè¡Œï¼‰
    print(f"\nâ³ Normalizing code (tabs/trailing spaces/blank lines/head-tail)...")
    original_normalized = normalize_code(original_code)
    ocr_normalized = normalize_code(ocr_text)
    
    print(f"After normalization:")
    print(f"  Original: {len(original_normalized)} chars, {len(original_normalized.splitlines())} lines")
    print(f"  OCR:      {len(ocr_normalized)} chars, {len(ocr_normalized.splitlines())} lines")
    
    # Calculate metrics (åŸå§‹ & è§„èŒƒåŒ–å)
    print(f"\nâ³ Calculating metrics...")
    cer_raw, wer_raw, bleu_raw = calculate_metrics(original_code, ocr_text)
    cer_norm, wer_norm, bleu_norm = calculate_metrics(original_normalized, ocr_normalized)
    
    # Output metrics
    print("\n" + "=" * 80)
    print("EVALUATION RESULTS")
    print("=" * 80)
    print(f"Code ID:                     {code_id}")
    print(f"Ratio:                       {ratio}x")
    print(f"\nğŸ“Š Raw (before normalization):")
    print(f"  CER (Character Error Rate):  {cer_raw:.2f}%")
    print(f"  WER (Word Error Rate):       {wer_raw:.2f}%")
    print(f"  BLEU Score:                  {bleu_raw:.2f}")
    print(f"\nâœ¨ Normalized (after code normalization):")
    print(f"  CER (Character Error Rate):  {cer_norm:.2f}%")
    print(f"  WER (Word Error Rate):       {wer_norm:.2f}%")
    print(f"  BLEU Score:                  {bleu_norm:.2f}")
    print(f"\nğŸ“ˆ Improvement: CER {cer_raw - cer_norm:+.2f}%, WER {wer_raw - wer_norm:+.2f}%, BLEU {bleu_norm - bleu_raw:+.2f}")
    
    # Line matching (ä½¿ç”¨è§„èŒƒåŒ–åçš„ç‰ˆæœ¬)
    ref_lines_norm = original_normalized.splitlines()
    ocr_lines_norm = ocr_normalized.splitlines()
    matches_norm = sum(1 for i in range(min(len(ref_lines_norm), len(ocr_lines_norm))) 
                  if ref_lines_norm[i].strip() == ocr_lines_norm[i].strip())
    emr_norm = matches_norm / max(len(ref_lines_norm), len(ocr_lines_norm)) * 100
    print(f"\nExact Match Rate (normalized): {emr_norm:.2f}% ({matches_norm}/{max(len(ref_lines_norm), len(ocr_lines_norm))} lines)")
    print("=" * 80)
    
    # Print detailed diff (ä½¿ç”¨è§„èŒƒåŒ–åçš„ç‰ˆæœ¬)
    print("\n[Note: Showing diff after code normalization]")
    print_diff_report(original_normalized, ocr_normalized)
    
    # Save outputs (ä¿å­˜åŸå§‹å’Œè§„èŒƒåŒ–åçš„ç‰ˆæœ¬)
    output_ref_raw = f"compare_ref_{code_id}_ratio{ratio}_raw.txt"
    output_ocr_raw = f"compare_ocr_{code_id}_ratio{ratio}_raw.txt"
    output_ref_norm = f"compare_ref_{code_id}_ratio{ratio}_normalized.txt"
    output_ocr_norm = f"compare_ocr_{code_id}_ratio{ratio}_normalized.txt"
    
    with open(output_ref_raw, 'w', encoding='utf-8') as f:
        f.write(original_code)
    with open(output_ocr_raw, 'w', encoding='utf-8') as f:
        f.write(ocr_text)
    with open(output_ref_norm, 'w', encoding='utf-8') as f:
        f.write(original_normalized)
    with open(output_ocr_norm, 'w', encoding='utf-8') as f:
        f.write(ocr_normalized)
    
    print(f"\nğŸ’¾ Saved:")
    print(f"   Reference (raw):        {output_ref_raw}")
    print(f"   OCR (raw):              {output_ocr_raw}")
    print(f"   Reference (normalized): {output_ref_norm}")
    print(f"   OCR (normalized):       {output_ocr_norm}")


def main():
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ¨¡å¼
    if len(sys.argv) >= 2 and sys.argv[1] == "--from-cache":
        if len(sys.argv) >= 4:
            code_id = sys.argv[2]
            ratio = sys.argv[3]
            main_from_cache(code_id, ratio)
            return
        else:
            print("Usage: python ocr_and_compare.py --from-cache <code_id> <ratio>")
            print("\nExample from dataset:")
            print("  python ocr_and_compare.py --from-cache astrbot_plugin_lmarena_file_bed.py 1")
            print("  python ocr_and_compare.py --from-cache astrbot_plugin_lmarena_file_bed.py 2")
            sys.exit(1)
    
    # ç¤ºä¾‹è·¯å¾„
    example_dir = r"D:\llm_projects\CodeZip\experiment_output\images\crypto-trader-bot-with-AI-algo_indicator_calculator.py\1024x1024_hl_nl"
    example_code_id = "crypto-trader-bot-with-AI-algo_indicator_calculator.py"
    
    # è·å–å‚æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œæˆ–äº¤äº’å¼è¾“å…¥ï¼‰
    if len(sys.argv) >= 3:
        image_dir = sys.argv[1]
        code_id = sys.argv[2]
    else:
        print("Usage: python ocr_and_compare.py <image_dir> <code_id>")
        print("\n" + "=" * 60)
        print("Interactive Mode")
        print("=" * 60)
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print(f"\nExample directory: {example_dir}")
        print(f"Example code_id: {example_code_id}")
        
        # è¯¢é—®æ˜¯å¦ä½¿ç”¨ç¤ºä¾‹
        use_example = input("\nUse example path? (y/n, default=y): ").strip().lower()
        
        if use_example == '' or use_example == 'y':
            image_dir = example_dir
            code_id = example_code_id
            print(f"Using: {image_dir}")
            print(f"Code ID: {code_id}")
        else:
            # äº¤äº’å¼è¾“å…¥
            image_dir = input("\nEnter image directory: ").strip()
            code_id = input("Enter code_id: ").strip()
            
            if not image_dir or not code_id:
                print("[ERROR] Both parameters are required")
                sys.exit(1)
    
    # Load API Key
    api_key = load_api_key()
    if not api_key:
        print("[ERROR] AIHUBMIX_API_KEY not found")
        sys.exit(1)
    
    # Get ratio
    print("\nEnter compression ratio (1, 2, 4, 8): ", end='')
    ratio = input().strip()
    
    # Find images (ratio=1 means original images without ratio suffix)
    if ratio == '1':
        pattern = os.path.join(image_dir, "page_*.png")
        # æ’é™¤å¸¦ ratio åç¼€çš„æ–‡ä»¶
        all_images = glob.glob(pattern)
        images = sorted([img for img in all_images if not any(f'_ratio{r}.png' in img for r in [2, 4, 8])])
    else:
        pattern = os.path.join(image_dir, f"page_*_ratio{ratio}.png")
        images = sorted(glob.glob(pattern))
    
    if not images:
        print(f"[ERROR] No images found: {pattern}")
        sys.exit(1)
    
    print(f"\nFound {len(images)} images")
    
    # OCR each image
    print(f"\nStarting OCR...")
    ocr_results = []
    for img in images:
        text = ocr_image(img, api_key, max_retries=3)
        ocr_results.append(text)
    
    # Merge (ä½¿ç”¨æ™ºèƒ½æ‹¼æ¥)
    merged_ocr = smart_join_pages(ocr_results)
    print(f"\nMerged (smart join): {len(merged_ocr)} chars, {len(merged_ocr.splitlines())} lines")
    
    # Load original code - ä½¿ç”¨ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_path = os.path.join(script_dir, "..", "experiment_output", "dataset.json")
    
    if not os.path.exists(dataset_path):
        print(f"[ERROR] dataset.json not found at: {dataset_path}")
        sys.exit(1)
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = {item['id']: item for item in json.load(f)}
    
    if code_id not in dataset:
        print(f"[ERROR] code_id not found: {code_id}")
        sys.exit(1)
    
    original_code = dataset[code_id]['code']
    print(f"Original: {len(original_code)} chars, {len(original_code.splitlines())} lines")
    
    # Calculate metrics
    print(f"\nCalculating metrics...")
    cer, wer, bleu = calculate_metrics(original_code, merged_ocr)
    
    # Output
    print("\n" + "=" * 60)
    print("EVALUATION RESULTS")
    print("=" * 60)
    print(f"CER (Character Error Rate):  {cer:.2f}%")
    print(f"WER (Word Error Rate):       {wer:.2f}%")
    print(f"BLEU Score:                  {bleu:.2f}")
    
    # Line matching
    ref_lines = original_code.splitlines()
    ocr_lines = merged_ocr.splitlines()
    matches = sum(1 for i in range(min(len(ref_lines), len(ocr_lines))) 
                  if ref_lines[i].strip() == ocr_lines[i].strip())
    emr = matches / max(len(ref_lines), len(ocr_lines)) * 100
    print(f"Exact Match Rate:            {emr:.2f}% ({matches}/{max(len(ref_lines), len(ocr_lines))} lines)")
    
    print("=" * 60)
    
    # Save
    output_file = f"ocr_merged_ratio{ratio}.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(merged_ocr)
    print(f"\nSaved: {output_file}")


if __name__ == "__main__":
    main()
