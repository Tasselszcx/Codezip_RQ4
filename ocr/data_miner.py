import json
import os
import time
import random
import ast
from datetime import datetime
from github import Github
from tqdm import tqdm

# ================= é…ç½®åŒº (æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹) =================

# âœ… å»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡è€Œä¸æ˜¯æŠŠ Token å†™è¿›ä»£ç ï¼š
# Windows PowerShell: $env:GITHUB_TOKEN="ghp_xxx" ; python data_miner.py

def _load_env_file() -> None:
    """æœ€å°ç‰ˆ dotenvï¼šä»è‹¥å¹²ä½ç½®è¯»å– .envï¼Œå¹¶å†™å…¥ os.environï¼ˆä¸è¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰ã€‚"""
    candidates = [
        os.path.join(os.getcwd(), ".env"),
        os.path.join(os.path.dirname(__file__), ".env"),
        os.path.join(os.path.dirname(os.path.dirname(__file__)), ".env"),
    ]
    for path in candidates:
        if not os.path.exists(path):
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                for raw in f:
                    line = raw.strip()
                    if not line or line.startswith("#") or "=" not in line:
                        continue
                    k, v = line.split("=", 1)
                    key = k.strip()
                    val = v.strip().strip('"').strip("'")
                    if key and (key not in os.environ):
                        os.environ[key] = val
        except Exception:
            # .env è¯»å–å¤±è´¥ä¸åº”é˜»æ–­ä¸»æµç¨‹
            pass


_load_env_file()

GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")

TARGET_DATE = "2025-08-01"  # æˆªæ­¢æ—¥æœŸ
TARGET_LANG = "python"      # ç›®æ ‡è¯­è¨€
MIN_STARS = 50              # æœ€å° Star æ•°
MAX_STARS = 400             # æœ€å¤§ Star æ•°
MIN_LINES = 50              # æœ€å°è¡Œæ•°
MAX_LINES = 120             # æœ€å¤§è¡Œæ•°
LIMIT = 100
                  # æŠ“å–æ•°é‡
OUTPUT_FILE = "dataset_fresh_2025.json"

# æ‰«æèŒƒå›´æ§åˆ¶ï¼šé€’å½’æ‰«æå“ªäº›ç›®å½•
# è¯´æ˜ï¼šåŸé€»è¾‘åªä¼šè¿›å…¥ src/lib/core/app ä¸”ä¸ä¼šç»§ç»­æ·±å…¥å­ç›®å½•ï¼Œå®¹æ˜“æ¼æ‰å¤§é‡ .pyã€‚
# è¿™é‡Œæ”¹ä¸ºï¼šåªè¦é¡¶å±‚ç›®å½•ååœ¨ç™½åå•é‡Œï¼Œå°±é€’å½’æ‰«æå…¶æ‰€æœ‰å­ç›®å½•ã€‚
SCAN_ROOT_DIRS = {
    "src", "lib", "core", "app",
    "python", "py",
    "backend", "server", "service", "services",
    "pkg", "package", "packages",
    "api", "apis",
    "project", "projects",
    "module", "modules",
    "script", "scripts",
}

# ä¿æŠ¤é˜ˆå€¼ï¼šé¿å…å•ä¸ª repo ç›®å½•è¿‡æ·±/è¿‡å¤§å¯¼è‡´ API è°ƒç”¨çˆ†ç‚¸
MAX_DIR_LISTINGS_PER_REPO = 80

# æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼ˆå­—èŠ‚ï¼‰ã€‚ä¸æ˜¯å¿…é¡»ï¼Œä½†èƒ½æ˜¾è‘—å‡å°‘â€œæå°ç¢ç‰‡æ–‡ä»¶â€å’Œâ€œè¶…å¤§æ–‡ä»¶â€å¸¦æ¥çš„å™ªå£°ä¸è€—æ—¶ã€‚
# å¦‚éœ€å…³é—­ä¸‹é™å¯è®¾ä¸º 0ï¼›å¦‚éœ€æ”¾å®½ä¸Šé™å¯è°ƒå¤§ã€‚
MIN_FILE_BYTES = 500
MAX_FILE_BYTES = 3000

# éšæœºåŒ–è®¾ç½®
ENABLE_RANDOM = True        # æ˜¯å¦å¯ç”¨éšæœºåŒ–
RANDOM_POOL_SIZE = LIMIT * 15       # ä»å‰ N ä¸ªç»“æœä¸­éšæœºæŠ½å–

# =============== æ–°å¢ï¼šä»£ç ç»“æ„è¿‡æ»¤ï¼ˆclassï¼‰===============
# éœ€æ±‚ï¼šä¼˜å…ˆé‡‡é›†â€œåŒ…å« class çš„ Python æ–‡ä»¶â€ï¼ˆè€Œä¸æ˜¯åªæœ‰é›¶æ•£ def/jsonï¼‰
REQUIRE_BIG_CLASS = True          # æ˜¯å¦å¼ºåˆ¶æ–‡ä»¶ä¸­å­˜åœ¨â€œè¾ƒå¤§çš„ classâ€
MIN_CLASS_METHODS = 3            # class å†…æœ€å°‘æ–¹æ³•æ•°ï¼ˆdef/async defï¼‰
MIN_CLASS_LINES = 25             # class æœ€å°‘è¡Œæ•°ï¼ˆåŸºäº ast çš„è¡Œå·ä¼°ç®—ï¼‰
# =========================================================
# =========================================================


def _class_span_lines(node: ast.AST) -> int:
    start = getattr(node, "lineno", None)
    end = getattr(node, "end_lineno", None)

    if start is None:
        return 0
    if end is not None:
        return max(1, end - start + 1)

    max_end = start
    for child in ast.walk(node):
        ln = getattr(child, "lineno", None)
        if isinstance(ln, int):
            max_end = max(max_end, ln)
    return max(1, max_end - start + 1)


def _has_big_class(code_text: str) -> bool:
    # å¿«é€Ÿå‰ªæ
    if "class " not in code_text:
        return False

    try:
        tree = ast.parse(code_text)
    except Exception:
        return False

    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            method_count = 0
            for b in node.body:
                if isinstance(b, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    method_count += 1

            span_lines = _class_span_lines(node)
            if method_count >= MIN_CLASS_METHODS and span_lines >= MIN_CLASS_LINES:
                return True

    return False

def fetch_fresh_code():
    # ç®€å•çš„ Token æ£€æŸ¥
    if "ghp_" not in GITHUB_TOKEN and "github_" not in GITHUB_TOKEN:
        print("âš ï¸ è­¦å‘Š: GitHub Token å¯èƒ½æœªé…ç½®ã€‚å»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡ GITHUB_TOKEN")

    print(f"ğŸš€ [Module 1] Data Miner Started...")
    print(f"ğŸ“… Filter: Created > {TARGET_DATE} | Lines: {MIN_LINES}-{MAX_LINES} | Limit: {LIMIT}")
    
    g = Github(GITHUB_TOKEN)
    
    # éšæœºåŒ–æŸ¥è¯¢å‚æ•°
    if ENABLE_RANDOM:
        # éšæœºé€‰æ‹©æ’åºæ–¹å¼å’Œé¡ºåº
        sort_options = ["stars", "forks", "updated"]
        order_options = ["desc", "asc"]
        sort_by = random.choice(sort_options)
        order_by = random.choice(order_options)
        
        # éšæœºåç§»æ˜Ÿæ˜ŸèŒƒå›´ (åœ¨ MIN_STARS~MAX_STARS åŸºç¡€ä¸Šéšæœºåç§»)
        star_offset = random.randint(0, 50)
        actual_min_stars = MIN_STARS + star_offset
        actual_max_stars = MAX_STARS + star_offset
        
        print(f"ğŸ² Random mode: sort={sort_by}, order={order_by}, stars={actual_min_stars}..{actual_max_stars}")
    else:
        sort_by = "stars"
        order_by = "desc"
        actual_min_stars = MIN_STARS
        actual_max_stars = MAX_STARS
    
    query = f"language:{TARGET_LANG} created:>{TARGET_DATE} stars:{actual_min_stars}..{actual_max_stars}"
    
    try:
        repos = g.search_repositories(query, sort=sort_by, order=order_by)
    except Exception as e:
        print(f"âŒ GitHub API Error: {e}")
        return []

    # æ”¶é›†å€™é€‰ä»“åº“ï¼ˆå…ˆæ”¶é›†ä¸€ä¸ªæ± å­ï¼Œå†éšæœºæŠ½å–ï¼‰
    candidate_repos = []
    repo_count = 0
    
    print(f"ğŸ“¦ Building candidate pool (max {RANDOM_POOL_SIZE} repos)...")
    for repo in repos:
        if repo_count >= RANDOM_POOL_SIZE:
            break
        candidate_repos.append(repo)
        repo_count += 1
        time.sleep(0.05)  # é¿å… API é™åˆ¶
    
    # éšæœºæ‰“ä¹±å€™é€‰ä»“åº“é¡ºåº
    if ENABLE_RANDOM:
        random.shuffle(candidate_repos)
        print(f"ğŸ”€ Shuffled {len(candidate_repos)} candidate repos")

    dataset = []
    pbar = tqdm(total=LIMIT, desc="Mining Code")

    for repo in candidate_repos:
        if len(dataset) >= LIMIT:
            break
        try:
            # BFS é€’å½’æ‰«æï¼šé¡¶å±‚ç›®å½•åœ¨ç™½åå•å†…åˆ™ç»§ç»­æ·±å…¥ï¼ˆæ”¯æŒ src/**ã€backend/** ç­‰ï¼‰ã€‚
            contents = repo.get_contents("")
            dir_calls = 1
            files_to_check = []
            while contents:
                file_content = contents.pop(0)
                if file_content.type == "dir":
                    top = (file_content.path.split("/", 1)[0] or "").lower()
                    if top in SCAN_ROOT_DIRS and dir_calls < MAX_DIR_LISTINGS_PER_REPO:
                        try:
                            contents.extend(repo.get_contents(file_content.path))
                            dir_calls += 1
                        except:
                            pass
                elif file_content.path.endswith(".py"):
                    path_lower = file_content.path.lower()
                    if "test" not in path_lower and "__init__" not in path_lower:
                        files_to_check.append(file_content)
            
            for file_node in files_to_check:
                if MIN_FILE_BYTES < file_node.size < MAX_FILE_BYTES:
                    code_text = file_node.decoded_content.decode('utf-8', errors='replace')
                    lines = code_text.splitlines()
                    if MIN_LINES <= len(lines) <= MAX_LINES:
                        if REQUIRE_BIG_CLASS and (not _has_big_class(code_text)):
                            continue
                        dataset.append({
                            "id": f"{repo.name}_{file_node.path}".replace("/", "_"), # æ‰å¹³åŒ–IDæ–¹ä¾¿åšæ–‡ä»¶å
                            "repo": repo.full_name,
                            "url": file_node.html_url,
                            "code": code_text,
                            "line_count": len(lines)
                        })
                        pbar.update(1)
                        break 
        except:
            continue
        time.sleep(0.1)

    pbar.close()
    
    print(f"âœ… [Module 1] Completed. Saved {len(dataset)} items to {OUTPUT_FILE}")
    return dataset

if __name__ == "__main__":
    fetch_fresh_code()