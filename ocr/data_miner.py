import json
import time
from datetime import datetime
from github import Github
from tqdm import tqdm

# ================= é…ç½®åŒº (æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹) =================
# âš ï¸ è¯·åŠ¡å¿…åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„ GitHub Token
GITHUB_TOKEN = "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" 

TARGET_DATE = "2025-08-01"  # æˆªæ­¢æ—¥æœŸ
TARGET_LANG = "python"      # ç›®æ ‡è¯­è¨€
MIN_STARS = 10              # æœ€å° Star æ•°
MAX_STARS = 200             # æœ€å¤§ Star æ•°
MIN_LINES = 50              # æœ€å°è¡Œæ•°
MAX_LINES = 120             # æœ€å¤§è¡Œæ•°
LIMIT = 2                   # æŠ“å–æ•°é‡ (æµ‹è¯•ç”¨)
OUTPUT_FILE = "dataset_fresh_2025.json"
# =========================================================

def fetch_fresh_code():
    # ç®€å•çš„ Token æ£€æŸ¥
    if "ghp_" not in GITHUB_TOKEN and "github_" not in GITHUB_TOKEN:
        print("âš ï¸ è­¦å‘Š: GitHub Token å¯èƒ½æœªé…ç½®ï¼Œè¯·æ£€æŸ¥ data_miner.py")

    print(f"ðŸš€ [Module 1] Data Miner Started...")
    print(f"ðŸ“… Filter: Created > {TARGET_DATE} | Lines: {MIN_LINES}-{MAX_LINES} | Limit: {LIMIT}")
    
    g = Github(GITHUB_TOKEN)
    query = f"language:{TARGET_LANG} created:>{TARGET_DATE} stars:{MIN_STARS}..{MAX_STARS}"
    
    try:
        repos = g.search_repositories(query, sort="stars", order="desc")
    except Exception as e:
        print(f"âŒ GitHub API Error: {e}")
        return []

    dataset = []
    pbar = tqdm(total=LIMIT, desc="Mining Code")

    for repo in repos:
        if len(dataset) >= LIMIT:
            break
        try:
            contents = repo.get_contents("")
            files_to_check = []
            while contents:
                file_content = contents.pop(0)
                if file_content.type == "dir":
                    if file_content.path in ['src', 'lib', 'core', 'app']:
                        try:
                            contents.extend(repo.get_contents(file_content.path))
                        except: pass
                elif file_content.path.endswith(".py"):
                    if "test" not in file_content.path and "__init__" not in file_content.path:
                        files_to_check.append(file_content)
            
            for file_node in files_to_check:
                if 1000 < file_node.size < 20000:
                    code_text = file_node.decoded_content.decode('utf-8')
                    lines = code_text.splitlines()
                    if MIN_LINES <= len(lines) <= MAX_LINES:
                        dataset.append({
                            "id": f"{repo.name}_{file_node.path}".replace("/", "_"), # æ‰å¹³åŒ–IDæ–¹ä¾¿åšæ–‡ä»¶å
                            "repo": repo.full_name,
                            "url": file_node.html_url,
                            "code": code_text,
                            "line_count": len(lines)
                        })
                        pbar.update(1)
                        break 
        except:
            continue
        time.sleep(0.1)

    pbar.close()
    
    # ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)
        
    print(f"âœ… [Module 1] Completed. Saved {len(dataset)} items to {OUTPUT_FILE}")
    return dataset

if __name__ == "__main__":
    fetch_fresh_code()