import os
import json
import shutil
import time
import base64
from collections import Counter
from PIL import Image  # ç”¨äºæ‰‹åŠ¨å®ç°è§†è§‰å‹ç¼©
from data_miner import fetch_fresh_code 
import text_to_image_compact 

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ================= é…ç½®åŒº =================
OUTPUT_DIR = "./experiment_output"
IMAGES_DIR = os.path.join(OUTPUT_DIR, "images")
TARGET_RATIOS = [1, 2, 4, 8]  # æˆ‘ä»¬çš„å‹ç¼©ç›®æ ‡

# ================= æ¨¡å—ä¸‰é…ç½®ï¼ˆInference Engineï¼‰=================
# åªè·‘ GLM-4.6Vï¼ˆé€šè¿‡ aihubmix OpenAI-compat æ¥å£ï¼‰
RUN_MODULE_3 = True
AIHUBMIX_BASE_URL = "https://aihubmix.com/v1"
GLM_MODEL_NAME = "glm-4.6v"
OCR_SYSTEM_PROMPT = "You are an OCR engine for code images."
OCR_USER_PROMPT = (
    "Transcribe the code in this image exactly.\n"
    "- Output plain text only (no Markdown, no code fences).\n"
    "- Preserve all whitespace, indentation, and newlines.\n"
    "- Do not add, remove, or rename anything.\n"
)
OCR_MAX_TOKENS = 4096
OCR_TEMPERATURE = 0.0
OCR_SLEEP_SECONDS = 0.2
OCR_MAX_RETRIES = 5
# =========================================

# ================= æ¨¡å—å››é…ç½®ï¼ˆAuto-Judgeï¼‰=================
RUN_MODULE_4 = True  # æ˜¯å¦è¿è¡Œè¯„ä¼°æ¨¡å—
JUDGE_LLM_MODEL = "gpt-5-mini"  # ç”¨äºsoft taxonomyåˆ†ç±»çš„æ¨¡å‹

# é”™è¯¯åˆ†ç±»ä½“ç³» (8ç±»)
ERROR_TAXONOMY = [
    "Visual_Typo",          # è§†è§‰ç›¸ä¼¼å­—ç¬¦æ›¿æ¢ï¼Œå¦‚ O/0, l/1
    "Symbol_Loss",          # æ ‡ç‚¹ç¬¦å·ä¸¢å¤±ï¼Œå¦‚ç¼ºå°‘æ‹¬å·ã€å†’å·
    "Indentation_Error",    # ç¼©è¿›é”™è¯¯
    "Line_Skipped",         # è·³è¡Œæ¼è¯»
    "Variable_Hallucination", # å˜é‡åå¹»è§‰ï¼Œå¦‚å°† 'data' è¯»æˆ 'date'
    "Code_Invention",       # å‡­ç©ºæé€ ä¸å­˜åœ¨çš„ä»£ç 
    "Repetition",           # é‡å¤è¾“å‡ºæŸäº›è¡Œ
    "Comment_Loss"          # æ³¨é‡Šå†…å®¹ä¸¢å¤±
]
# =========================================


def _mask_api_key(key: str) -> str:
    if not key:
        return ""
    if len(key) <= 12:
        return key[:2] + "..." + key[-2:]
    return key[:6] + "..." + key[-6:]


def _try_load_api_key_from_env_files() -> str:
    """ä» .env æ–‡ä»¶ä¸­å°è¯•è¯»å– AIHUBMIX_API_KEYã€‚

    ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ï¼ˆè°ƒç”¨æ–¹å¤„ç†ï¼‰> å·¥ä½œåŒºæ ¹ç›®å½• .env > ocr/.env
    """
    script_dir = os.path.dirname(__file__)
    repo_dir = os.path.abspath(os.path.join(script_dir, os.pardir))

    candidates = [
        os.path.join(os.getcwd(), ".env"),   # å–å†³äºä½ ä»å“ªé‡Œå¯åŠ¨
        os.path.join(repo_dir, ".env"),      # ä»“åº“æ ¹ç›®å½•ï¼ˆæ›´ç¨³ï¼‰
        os.path.join(script_dir, ".env"),    # ocr/.env
    ]

    for path in candidates:
        if not os.path.exists(path):
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                for raw_line in f:
                    line = raw_line.strip()
                    if not line or line.startswith("#"):
                        continue
                    if "=" not in line:
                        continue
                    k, v = line.split("=", 1)
                    if k.strip() != "AIHUBMIX_API_KEY":
                        continue
                    value = v.strip().strip('"').strip("'")
                    if value:
                        return value
        except Exception:
            continue

    return ""


def _iter_image_files(root_dir: str):
    for dirpath, _, filenames in os.walk(root_dir):
        for fn in filenames:
            if fn.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
                yield os.path.join(dirpath, fn)


def _load_done_set(jsonl_path: str) -> set:
    done = set()
    if not os.path.exists(jsonl_path):
        return done
    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    if obj.get("image_path"):
                        done.add(obj["image_path"])
                except Exception:
                    continue
    except Exception:
        return done
    return done


def _encode_image_to_data_url(image_path: str) -> str:
    ext = os.path.splitext(image_path)[1].lower()
    mime = "image/png"
    if ext in (".jpg", ".jpeg"):
        mime = "image/jpeg"
    elif ext == ".webp":
        mime = "image/webp"

    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def _parse_ratio_from_filename(image_path: str) -> int:
    # e.g. page_001_ratio2.png -> 2 ; page_001.png -> 1
    stem = os.path.splitext(os.path.basename(image_path))[0]
    marker = "_ratio"
    if marker in stem:
        try:
            return int(stem.split(marker, 1)[1])
        except Exception:
            return 1
    return 1


def run_module_3_glm46v(images_dir: str, output_dir: str):
    print("\n" + "=" * 40)
    print("ğŸš€ Running Module 3: Inference Engine (GLM-4.6V)")
    print("=" * 40)

    if OpenAI is None:
        print("âŒ Missing dependency: openai. Run: pip install openai")
        return

    api_key = os.getenv("AIHUBMIX_API_KEY")
    api_key_source = "env:AIHUBMIX_API_KEY" if api_key else ""
    if not api_key:
        api_key = _try_load_api_key_from_env_files()
        if api_key:
            api_key_source = "file:.env"
    if not api_key:
        searched = [
            os.path.join(os.getcwd(), ".env"),
            os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, ".env")),
            os.path.join(os.path.dirname(__file__), ".env"),
        ]
        print("âŒ Missing AIHUBMIX_API_KEY.")
        print("   - æ–¹å¼1ï¼šPowerShell ä¸´æ—¶è®¾ç½®ï¼š$env:AIHUBMIX_API_KEY=\"sk-...\"")
        print("   - æ–¹å¼2ï¼šå†™å…¥ .env æ–‡ä»¶ï¼ˆä¸è¦æäº¤åˆ°ä»“åº“ï¼‰")
        print("     æ ¼å¼ï¼šAIHUBMIX_API_KEY=sk-...")
        print("     æŸ¥æ‰¾è·¯å¾„ï¼š")
        for p in searched:
            print(f"       - {p}")
        return

    print(f"ğŸ”‘ AIHUBMIX_API_KEY loaded ({api_key_source}): {_mask_api_key(api_key)}")

    os.makedirs(output_dir, exist_ok=True)
    out_jsonl = os.path.join(output_dir, "glm46v_ocr.jsonl")
    done = _load_done_set(out_jsonl)

    client = OpenAI(api_key=api_key, base_url=AIHUBMIX_BASE_URL)

    total = 0
    skipped = 0
    errors = 0

    image_paths = list(_iter_image_files(images_dir))
    print(f"ğŸ–¼ï¸  Total images to OCR: {len(image_paths)}")

    for i, image_path in enumerate(image_paths, start=1):
        print(f"[{i}/{len(image_paths)}] OCR: {os.path.basename(image_path)}")
        if image_path in done:
            skipped += 1
            continue

        # è·¯å¾„ç»“æ„: images/{code_id}/{variant_folder}/page_xxx.png
        # éœ€è¦å–ä¸Šä¸¤å±‚ç›®å½•æ‰æ˜¯ code_id
        parent_dir = os.path.dirname(image_path)            # .../1024x1500_hl_nl
        code_id_dir = os.path.dirname(parent_dir)           # .../{code_id}
        code_id = os.path.basename(code_id_dir)
        ratio = _parse_ratio_from_filename(image_path)

        data_url = _encode_image_to_data_url(image_path)

        last_err = None
        text = ""

        for attempt in range(1, OCR_MAX_RETRIES + 1):
            try:
                resp = client.chat.completions.create(
                    model=GLM_MODEL_NAME,
                    temperature=OCR_TEMPERATURE,
                    max_tokens=OCR_MAX_TOKENS,
                    messages=[
                        {"role": "system", "content": OCR_SYSTEM_PROMPT},
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": OCR_USER_PROMPT},
                                {"type": "image_url", "image_url": {"url": data_url}},
                            ],
                        },
                    ],
                )
                text = (resp.choices[0].message.content or "").rstrip()
                last_err = None
                break
            except Exception as e:
                last_err = str(e)
                # exponential backoff: 1,2,4,8,... capped at 30s
                backoff = min(30.0, float(2 ** (attempt - 1)))
                time.sleep(backoff)

        rec = {
            "code_id": code_id,
            "ratio": ratio,
            "image_path": image_path,
            "model": GLM_MODEL_NAME,
        }

        if last_err is None:
            rec["text"] = text
            total += 1
        else:
            rec["error"] = last_err
            errors += 1

        with open(out_jsonl, "a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

        time.sleep(OCR_SLEEP_SECONDS)

    print(f"âœ… Module 3 finished. ok={total}, skipped={skipped}, error={errors}")
    print(f"ğŸ“„ Output: {os.path.abspath(out_jsonl)}")


# ============================================================
# ğŸŸ  æ¨¡å—å››: Auto-Judge (è¯„ä¼°å™¨)
# ============================================================

def normalize_code(text: str) -> str:
    """
    ä»£ç è§„èŒƒåŒ–ï¼šå‹ç¼©ç©ºè¡Œ + å»é™¤è¡Œå°¾ç©ºæ ¼ + tabâ†’4ç©ºæ ¼
    ç”¨äºè®¡ç®— CER/WER/BLEU ç­‰æŒ‡æ ‡æ—¶å‡å°‘æ ¼å¼å™ªå£°
    """
    lines = text.splitlines()
    
    # 1. Tab â†’ 4 spaces
    lines = [line.replace('\t', '    ') for line in lines]
    
    # 2. å»é™¤è¡Œå°¾ç©ºæ ¼ï¼ˆtrailing spacesï¼‰
    lines = [line.rstrip() for line in lines]
    
    # 3. å‹ç¼©è¿ç»­ç©ºè¡Œä¸ºå•ä¸ªç©ºè¡Œ
    normalized = []
    prev_blank = False
    for line in lines:
        is_blank = (line.strip() == '')
        if is_blank:
            if not prev_blank:  # åªä¿ç•™ç¬¬ä¸€ä¸ªç©ºè¡Œ
                normalized.append('')
            prev_blank = True
        else:
            normalized.append(line)
            prev_blank = False
    
    # 4. å»é™¤é¦–å°¾ç©ºè¡Œ
    while normalized and normalized[0] == '':
        normalized.pop(0)
    while normalized and normalized[-1] == '':
        normalized.pop()
    
    return '\n'.join(normalized)


def _compute_cer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate)
    CER = (S + D + I) / N
    ä½¿ç”¨ Levenshtein ç¼–è¾‘è·ç¦»
    """
    ref = list(reference)
    hyp = list(hypothesis)
    n = len(ref)
    m = len(hyp)

    if n == 0:
        return 1.0 if m > 0 else 0.0

    # DP çŸ©é˜µ
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if ref[i - 1] == hyp[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,      # deletion
                dp[i][j - 1] + 1,      # insertion
                dp[i - 1][j - 1] + cost  # substitution
            )

    edit_distance = dp[n][m]
    return edit_distance / n


def _check_ast_parsable(code: str) -> bool:
    """æ£€æŸ¥ä»£ç æ˜¯å¦å¯è¢« Python AST è§£æ"""
    import ast
    try:
        ast.parse(code)
        return True
    except SyntaxError:
        return False


def _compute_token_bleu(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—åŸºäº token çš„ç®€åŒ– BLEU (CodeBLEU æ ¸å¿ƒ)
    ä½¿ç”¨ 1-gram åˆ° 4-gram çš„å‡ ä½•å¹³å‡
    """
    import re
    from collections import Counter
    import math

    def tokenize(text):
        # ç®€å•çš„ä»£ç åˆ†è¯ï¼šæŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²
        return re.findall(r'\w+|[^\w\s]', text)

    ref_tokens = tokenize(reference)
    hyp_tokens = tokenize(hypothesis)

    if len(hyp_tokens) == 0:
        return 0.0

    # è®¡ç®— n-gram precision
    def ngram_precision(ref_toks, hyp_toks, n):
        if len(hyp_toks) < n:
            return 0.0
        ref_ngrams = Counter(tuple(ref_toks[i:i+n]) for i in range(len(ref_toks) - n + 1))
        hyp_ngrams = Counter(tuple(hyp_toks[i:i+n]) for i in range(len(hyp_toks) - n + 1))

        match = 0
        total = 0
        for ng, cnt in hyp_ngrams.items():
            match += min(cnt, ref_ngrams.get(ng, 0))
            total += cnt
        return match / total if total > 0 else 0.0

    # è®¡ç®— 1-gram åˆ° 4-gram çš„ precision
    precisions = []
    for n in range(1, 5):
        p = ngram_precision(ref_tokens, hyp_tokens, n)
        precisions.append(p)

    # è¿‡æ»¤æ‰ 0 å€¼ï¼ˆé¿å… log(0)ï¼‰
    non_zero = [p for p in precisions if p > 0]
    if not non_zero:
        return 0.0

    # å‡ ä½•å¹³å‡
    log_avg = sum(math.log(p) for p in non_zero) / len(non_zero)
    bleu = math.exp(log_avg)

    # Brevity penalty
    bp = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0

    return bp * bleu


def _compute_wer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—è¯é”™è¯¯ç‡ (Word Error Rate)
    WER = (S + D + I) / Nï¼Œä»¥è¯ä¸ºå•ä½
    """
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    n = len(ref_words)
    m = len(hyp_words)

    if n == 0:
        return 1.0 if m > 0 else 0.0

    # DP çŸ©é˜µ
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,      # deletion
                dp[i][j - 1] + 1,      # insertion
                dp[i - 1][j - 1] + cost  # substitution
            )

    edit_distance = dp[n][m]
    return edit_distance / n


def _compute_exact_match_rate(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ (Exact Match Rate)
    å®Œå…¨åŒ¹é…çš„è¡Œæ•° / æ€»è¡Œæ•°
    """
    ref_lines = reference.split('\n')
    hyp_lines = hypothesis.split('\n')
    
    if len(ref_lines) == 0:
        return 1.0 if len(hyp_lines) == 0 else 0.0
    
    # å¯¹é½æ¯”è¾ƒï¼šå–ä¸¤è€…ä¸­è¾ƒçŸ­çš„é•¿åº¦
    matched = 0
    for i, ref_line in enumerate(ref_lines):
        if i < len(hyp_lines) and ref_line == hyp_lines[i]:
            matched += 1
    
    return matched / len(ref_lines)


# ============================================================
# ğŸ·ï¸ å…«å¤§é”™è¯¯è°±ç³»æ£€æµ‹å‡½æ•°ï¼ˆåŸºäºè§„åˆ™ï¼Œä¸ä¾èµ– LLMï¼‰
# ============================================================

def _detect_visual_typo(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹å½¢è¿‘å­—æ··æ·† (1 vs l, 0 vs O, etc.)
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    # å½¢è¿‘å­—å¯¹ï¼ˆåŒå‘æ£€æµ‹ï¼‰
    confusable_pairs = [
        ('1', 'l'), ('1', 'I'), ('l', 'I'),  # 1/l/I
        ('0', 'O'), ('0', 'o'), ('O', 'o'),  # 0/O/o
        ('5', 'S'), ('5', 's'),              # 5/S
        ('8', 'B'),                          # 8/B
        ('2', 'Z'), ('2', 'z'),              # 2/Z
        ('6', 'G'),                          # 6/G
        ('rn', 'm'),                         # rn/m (è¿å­—)
        ("'", '`'), ('"', "''"),             # å¼•å·æ··æ·†
    ]
    
    ref_lower = reference.lower()
    hyp_lower = hypothesis.lower()
    
    for a, b in confusable_pairs:
        # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿäº†æ›¿æ¢ï¼šref ä¸­æœ‰ aï¼Œhyp ä¸­å¯¹åº”ä½ç½®å˜æˆäº† b
        ref_count_a = reference.count(a)
        hyp_count_a = hypothesis.count(a)
        ref_count_b = reference.count(b)
        hyp_count_b = hypothesis.count(b)
        
        # å¦‚æœ a åœ¨ ref ä¸­æ›´å¤šï¼Œä½† b åœ¨ hyp ä¸­æ›´å¤šï¼Œå¯èƒ½å‘ç”Ÿäº†æ··æ·†
        if ref_count_a > hyp_count_a and hyp_count_b > ref_count_b:
            return 1
        if ref_count_b > hyp_count_b and hyp_count_a > ref_count_a:
            return 1
    
    return 0


def _detect_symbol_loss(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ç¬¦å·ä¸¢å¤± (_, :, ;, æ‹¬å·ç­‰)
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    critical_symbols = ['_', ':', ';', '(', ')', '[', ']', '{', '}', ',', '.', '=', '+', '-', '*', '/']
    
    for sym in critical_symbols:
        ref_count = reference.count(sym)
        hyp_count = hypothesis.count(sym)
        
        # å¦‚æœ ref ä¸­çš„ç¬¦å·æ¯” hyp ä¸­å¤š 20% ä»¥ä¸Šï¼Œè®¤ä¸ºå‘ç”Ÿäº†ä¸¢å¤±
        if ref_count > 0 and hyp_count < ref_count * 0.8:
            return 1
    
    return 0


def _detect_indentation_error(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ç¼©è¿›é”™è¯¯
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    ref_lines = reference.split('\n')
    hyp_lines = hypothesis.split('\n')
    
    # è®¡ç®—æ¯è¡Œå¼€å¤´çš„ç©ºæ ¼æ•°
    def get_indent(line):
        return len(line) - len(line.lstrip(' \t'))
    
    # å–ä¸¤è€…è¾ƒçŸ­çš„é•¿åº¦è¿›è¡Œæ¯”è¾ƒ
    min_len = min(len(ref_lines), len(hyp_lines))
    
    indent_errors = 0
    for i in range(min_len):
        ref_indent = get_indent(ref_lines[i])
        hyp_indent = get_indent(hyp_lines[i])
        
        # å¦‚æœç¼©è¿›å·®å¼‚è¶…è¿‡ 2 ä¸ªç©ºæ ¼ï¼Œè®¤ä¸ºæ˜¯é”™è¯¯
        if abs(ref_indent - hyp_indent) >= 2:
            indent_errors += 1
    
    # å¦‚æœè¶…è¿‡ 5% çš„è¡Œæœ‰ç¼©è¿›é”™è¯¯ï¼Œæ ‡è®°ä¸º 1
    if min_len > 0 and indent_errors / min_len > 0.05:
        return 1
    
    return 0


def _detect_line_skipped(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹æ•´è¡Œä¸¢å¤±
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    ref_lines = [line.strip() for line in reference.split('\n') if line.strip()]
    hyp_lines = [line.strip() for line in hypothesis.split('\n') if line.strip()]
    
    # å¦‚æœ hyp çš„è¡Œæ•°å°‘äº ref çš„ 90%ï¼Œè®¤ä¸ºå‘ç”Ÿäº†è¡Œä¸¢å¤±
    if len(hyp_lines) < len(ref_lines) * 0.9:
        return 1
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ ref ä¸­çš„è¡Œå®Œå…¨ä¸åœ¨ hyp ä¸­
    hyp_set = set(hyp_lines)
    missing_lines = 0
    for line in ref_lines:
        if len(line) > 10 and line not in hyp_set:  # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„è¡Œ
            missing_lines += 1
    
    # å¦‚æœè¶…è¿‡ 10% çš„æœ‰æ„ä¹‰è¡Œä¸¢å¤±ï¼Œæ ‡è®°ä¸º 1
    if len(ref_lines) > 0 and missing_lines / len(ref_lines) > 0.1:
        return 1
    
    return 0


def _detect_variable_hallucination(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹å˜é‡åå¹»è§‰ï¼ˆOCR ä¸­å‡ºç°äº† reference ä¸­ä¸å­˜åœ¨çš„æ ‡è¯†ç¬¦ï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    import re
    
    # æå–æ ‡è¯†ç¬¦ï¼ˆå˜é‡åã€å‡½æ•°åç­‰ï¼‰
    def extract_identifiers(code):
        # åŒ¹é… Python æ ‡è¯†ç¬¦
        identifiers = set(re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', code))
        # æ’é™¤ Python å…³é”®å­—å’Œå¸¸è§å†…ç½®å‡½æ•°
        keywords = {'def', 'class', 'if', 'else', 'elif', 'for', 'while', 'return', 
                   'import', 'from', 'as', 'try', 'except', 'finally', 'with', 
                   'True', 'False', 'None', 'and', 'or', 'not', 'in', 'is',
                   'print', 'len', 'range', 'str', 'int', 'float', 'list', 'dict',
                   'self', 'cls', 'args', 'kwargs'}
        return identifiers - keywords
    
    ref_ids = extract_identifiers(reference)
    hyp_ids = extract_identifiers(hypothesis)
    
    # æ‰¾å‡º hyp ä¸­æœ‰ä½† ref ä¸­æ²¡æœ‰çš„æ ‡è¯†ç¬¦ï¼ˆå¹»è§‰ï¼‰
    hallucinated = hyp_ids - ref_ids
    
    # è¿‡æ»¤æ‰é•¿åº¦å°äº 3 çš„ï¼ˆå¯èƒ½æ˜¯è¯¯åˆ¤ï¼‰
    hallucinated = {h for h in hallucinated if len(h) >= 3}
    
    # å¦‚æœæœ‰è¶…è¿‡ 3 ä¸ªå¹»è§‰æ ‡è¯†ç¬¦ï¼Œæ ‡è®°ä¸º 1
    if len(hallucinated) >= 3:
        return 1
    
    return 0


def _detect_code_invention(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ä»£ç æé€ ï¼ˆOCR ä¸­å‡ºç°äº†å®Œå…¨ä¸å­˜åœ¨çš„ä»£ç æ®µï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    hyp_lines = [line.strip() for line in hypothesis.split('\n') if line.strip()]
    
    invented_lines = 0
    for line in hyp_lines:
        # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„è¡Œï¼ˆé•¿åº¦ > 15ï¼‰
        if len(line) > 15:
            # å¦‚æœè¿™è¡Œåœ¨ reference ä¸­å®Œå…¨æ‰¾ä¸åˆ°ä»»ä½•ç›¸ä¼¼ç‰‡æ®µ
            if line not in reference and line[:20] not in reference:
                invented_lines += 1
    
    # å¦‚æœæœ‰è¶…è¿‡ 5% çš„è¡Œæ˜¯æé€ çš„ï¼Œæ ‡è®°ä¸º 1
    if len(hyp_lines) > 0 and invented_lines / len(hyp_lines) > 0.05:
        return 1
    
    return 0


def _detect_repetition(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹é‡å¤è¾“å‡ºï¼ˆå¤è¯»æœºç°è±¡ï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    hyp_lines = [line for line in hypothesis.split('\n') if line.strip()]
    
    if len(hyp_lines) < 3:
        return 0
    
    # æ£€æµ‹è¿ç»­é‡å¤çš„è¡Œ
    consecutive_repeats = 0
    for i in range(1, len(hyp_lines)):
        if hyp_lines[i] == hyp_lines[i-1] and len(hyp_lines[i].strip()) > 5:
            consecutive_repeats += 1
    
    # å¦‚æœæœ‰ 2 è¡Œä»¥ä¸Šè¿ç»­é‡å¤ï¼Œæ ‡è®°ä¸º 1
    if consecutive_repeats >= 2:
        return 1
    
    # æ£€æµ‹éè¿ç»­çš„å¤§é‡é‡å¤
    from collections import Counter
    line_counts = Counter(line for line in hyp_lines if len(line.strip()) > 10)
    
    # å¦‚æœæœ‰ä»»ä½•è¡Œé‡å¤è¶…è¿‡ 3 æ¬¡ï¼Œæ ‡è®°ä¸º 1
    for line, count in line_counts.items():
        if count >= 3:
            return 1
    
    return 0


def _detect_comment_loss(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹æ³¨é‡Šä¸¢å¤±æˆ–ä¹±ç 
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    # æå–æ³¨é‡Šè¡Œ
    def extract_comments(code):
        comments = []
        for line in code.split('\n'):
            stripped = line.strip()
            if stripped.startswith('#'):
                comments.append(stripped)
            # ä¹Ÿæ£€æµ‹è¡Œå†…æ³¨é‡Š
            if '#' in line:
                comment_part = line.split('#', 1)[1].strip()
                if comment_part:
                    comments.append('#' + comment_part)
        return comments
    
    ref_comments = extract_comments(reference)
    hyp_comments = extract_comments(hypothesis)
    
    if not ref_comments:
        return 0  # åŸä»£ç æ²¡æœ‰æ³¨é‡Š
    
    # æ£€æŸ¥æ³¨é‡Šæ•°é‡æ˜¯å¦å¤§å¹…å‡å°‘
    if len(hyp_comments) < len(ref_comments) * 0.7:
        return 1
    
    # æ£€æŸ¥æ³¨é‡Šå†…å®¹æ˜¯å¦ä¸¥é‡å˜å½¢
    matched = 0
    for ref_c in ref_comments:
        for hyp_c in hyp_comments:
            # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æµ‹
            if ref_c in hyp_c or hyp_c in ref_c:
                matched += 1
                break
            # æˆ–è€…è¶…è¿‡ 70% çš„å­—ç¬¦åŒ¹é…
            common_chars = sum(1 for c in ref_c if c in hyp_c)
            if len(ref_c) > 0 and common_chars / len(ref_c) > 0.7:
                matched += 1
                break
    
    # å¦‚æœå°‘äº 70% çš„æ³¨é‡Šè¢«æ­£ç¡®ä¿ç•™ï¼Œæ ‡è®°ä¸º 1
    if len(ref_comments) > 0 and matched / len(ref_comments) < 0.7:
        return 1
    
    return 0


def _detect_all_taxonomy_errors(reference: str, hypothesis: str) -> dict:
    """
    æ£€æµ‹æ‰€æœ‰å…«å¤§é”™è¯¯è°±ç³»ï¼Œè¿”å› 0/1 æ ‡ç­¾å­—å…¸
    """
    return {
        "Visual_Typo": _detect_visual_typo(reference, hypothesis),
        "Symbol_Loss": _detect_symbol_loss(reference, hypothesis),
        "Indentation_Error": _detect_indentation_error(reference, hypothesis),
        "Line_Skipped": _detect_line_skipped(reference, hypothesis),
        "Variable_Hallucination": _detect_variable_hallucination(reference, hypothesis),
        "Code_Invention": _detect_code_invention(reference, hypothesis),
        "Repetition": _detect_repetition(reference, hypothesis),
        "Comment_Loss": _detect_comment_loss(reference, hypothesis),
    }


def _call_llm_for_taxonomy(client, reference: str, hypothesis: str) -> list:
    """
    è°ƒç”¨ LLM è¿›è¡Œé”™è¯¯åˆ†ç±»
    è¿”å›æ£€æµ‹åˆ°çš„é”™è¯¯ç±»å‹åˆ—è¡¨
    """
    prompt = f"""You are an expert code quality evaluator. 
Compare the reference code with the OCR output and identify error types.

Reference code:
```
{reference[:3000]}
```

OCR output:
```
{hypothesis[:3000]}
```

Analyze the differences and return ONLY a JSON array of error types from this list:
{ERROR_TAXONOMY}

Rules:
- Return an empty array [] if the output matches the reference perfectly
- Only include error types that are clearly present
- Return ONLY the JSON array, no other text

Example response: ["Visual_Typo", "Symbol_Loss"]
"""

    try:
        resp = client.chat.completions.create(
            model=JUDGE_LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=256,
            temperature=0.0,
        )
        content = resp.choices[0].message.content.strip()
        # è§£æ JSON
        import re
        match = re.search(r'\[.*?\]', content, re.DOTALL)
        if match:
            return json.loads(match.group())
        return []
    except Exception as e:
        print(f"   âš ï¸ LLM taxonomy call failed: {e}")
        return []


def run_module_4_judge(output_dir: str):
    """
    æ¨¡å—4: Auto-Judge è¯„ä¼°å™¨
    è¯»å– OCR ç»“æœï¼Œä¸åŸå§‹ä»£ç å¯¹æ¯”ï¼Œè¾“å‡ºè¯„ä¼°æŒ‡æ ‡
    """
    print("\n" + "=" * 40)
    print("ğŸš€ Running Module 4: Auto-Judge")
    print("=" * 40)

    # è¯»å– dataset.json (åŸå§‹ä»£ç )
    dataset_path = os.path.join(output_dir, "dataset.json")
    if not os.path.exists(dataset_path):
        print("âŒ dataset.json not found, skipping Module 4")
        return

    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    # å»ºç«‹ code_id -> code çš„æ˜ å°„
    code_map = {item["id"]: item["code"] for item in dataset}

    # è¯»å– OCR ç»“æœ
    ocr_path = os.path.join(output_dir, "glm46v_ocr.jsonl")
    if not os.path.exists(ocr_path):
        print("âŒ glm46v_ocr.jsonl not found, skipping Module 4")
        return

    ocr_results = []
    with open(ocr_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                ocr_results.append(json.loads(line))

    # ğŸŒŸ æ ¸å¿ƒä¿®å¤ï¼šæŒ‰ (code_id, ratio) åˆ†ç»„ï¼Œåˆå¹¶å¤šé¡µç»“æœ
    from collections import defaultdict
    grouped = defaultdict(list)  # (code_id, ratio) -> [{"text": ..., "image_path": ...}, ...]
    
    for rec in ocr_results:
        # æå– code_id
        img_path = rec.get("image_path", "")
        parts = img_path.replace("\\", "/").split("/")
        code_id = parts[-3] if len(parts) >= 3 else rec.get("code_id", "")
        
        ratio = rec.get("ratio", 1)
        ocr_text = rec.get("text", "")
        
        # è·³è¿‡é”™è¯¯ç»“æœ
        if not ocr_text or "error" in rec:
            continue
        
        # æ¸…ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå…³é”®ï¼ï¼‰
        ocr_text = ocr_text.replace('<|begin_of_box|>', '').replace('<|end_of_box|>', '').strip()
        
        # æŒ‰ (code_id, ratio) åˆ†ç»„
        grouped[(code_id, ratio)].append({
            "text": ocr_text,
            "image_path": img_path,
        })

    # è¯„ä¼°ç»“æœ
    detail_path = os.path.join(output_dir, "judge_results_detail.jsonl")
    summary_path = os.path.join(output_dir, "judge_summary.json")

    # æŒ‰ ratio åˆ†ç»„ç»Ÿè®¡
    stats_by_ratio = {r: {
        "cer_sum": 0, "wer_sum": 0, "bleu_sum": 0, 
        "exact_match_sum": 0,
        "ast_pass": 0, "count": 0, 
        "errors": Counter(), "taxonomy_sums": Counter()
    } for r in TARGET_RATIOS}

    # æ¸…ç©º detail æ–‡ä»¶
    open(detail_path, "w").close()

    total = len(grouped)
    evaluated = 0
    
    # ğŸŒŸ ç°åœ¨æŒ‰ (code_id, ratio) ç»„åˆè¿›è¡Œè¯„ä¼°
    for idx, ((code_id, ratio), pages) in enumerate(grouped.items(), 1):
        reference = code_map.get(code_id, "")
        if not reference:
            print(f"[{idx}/{total}] âš ï¸ No ground truth for {code_id}, skipping")
            continue

        # ğŸŒŸ åˆå¹¶å¤šé¡µ OCR ç»“æœï¼ˆæŒ‰æ–‡ä»¶åæ’åºç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
        pages.sort(key=lambda x: x["image_path"])
        merged_ocr = '\n'.join([p["text"] for p in pages])
        
        evaluated += 1
        num_pages = len(pages)
        print(f"[{idx}/{total}] Evaluating: {code_id} @ ratio {ratio}x ({num_pages} pages)")

        # ğŸŒŸ è§„èŒƒåŒ–å¤„ç†ï¼ˆç”¨äº hard metricsï¼‰
        ref_normalized = normalize_code(reference)
        ocr_normalized = normalize_code(merged_ocr)

        # 1. Hard metricsï¼ˆä½¿ç”¨è§„èŒƒåŒ–åçš„æ–‡æœ¬ï¼‰
        cer = _compute_cer(ref_normalized, ocr_normalized)
        wer = _compute_wer(ref_normalized, ocr_normalized)
        bleu = _compute_token_bleu(ref_normalized, ocr_normalized)
        exact_match = _compute_exact_match_rate(ref_normalized, ocr_normalized)
        ast_ok = _check_ast_parsable(merged_ocr)  # AST ç”¨åŸå§‹æ–‡æœ¬

        # 2. Soft taxonomyï¼ˆä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼Œä¿ç•™ç¼©è¿›/ç¬¦å·/ç©ºè¡Œä¿¡æ¯ï¼‰
        taxonomy_labels = _detect_all_taxonomy_errors(reference, merged_ocr)
        detected_error_types = [k for k, v in taxonomy_labels.items() if v == 1]

        # è®°å½•è¯¦æƒ… 
        detail_rec = {
            "code_id": code_id,
            "ratio": ratio,
            "num_pages": num_pages,
            "cer": round(cer, 4),
            "wer": round(wer, 4),
            "token_bleu": round(bleu, 4),
            "exact_match_rate": round(exact_match, 4),
            "ast_parsable": ast_ok,
            "taxonomy_labels": taxonomy_labels,
            "detected_errors": detected_error_types,
        }

        with open(detail_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(detail_rec, ensure_ascii=False) + "\n")

        # æ›´æ–°ç»Ÿè®¡
        if ratio in stats_by_ratio:
            stats_by_ratio[ratio]["cer_sum"] += cer
            stats_by_ratio[ratio]["wer_sum"] += wer
            stats_by_ratio[ratio]["bleu_sum"] += bleu
            stats_by_ratio[ratio]["exact_match_sum"] += exact_match
            stats_by_ratio[ratio]["ast_pass"] += (1 if ast_ok else 0)
            stats_by_ratio[ratio]["count"] += 1
            
            # è®°å½•è¯¥æ ·æœ¬ä¸­å‡ºç°çš„é”™è¯¯ï¼ˆtaxonomyï¼‰
            for err_type, val in taxonomy_labels.items():
                if val == 1:
                    stats_by_ratio[ratio]["errors"][err_type] += 1
                stats_by_ratio[ratio]["taxonomy_sums"][err_type] += val

    # ç”Ÿæˆæ±‡æ€»
    summary = {}
    for ratio, s in stats_by_ratio.items():
        if s["count"] == 0:
            continue
        # è®¡ç®—æ¯ç§é”™è¯¯ç±»å‹çš„æ£€å‡ºç‡
        error_rates = {et: round(cnt / s["count"], 4) 
                      for et, cnt in s["errors"].items()}
        summary[f"ratio_{ratio}x"] = {
            "count": s["count"],
            "avg_cer": round(s["cer_sum"] / s["count"], 4),
            "avg_wer": round(s["wer_sum"] / s["count"], 4),
            "avg_token_bleu": round(s["bleu_sum"] / s["count"], 4),
            "avg_exact_match_rate": round(s["exact_match_sum"] / s["count"], 4),
            "ast_pass_rate": round(s["ast_pass"] / s["count"], 4),
            "error_counts": dict(s["errors"]),  # åŸå§‹è®¡æ•°
            "error_rates": error_rates,  # æ£€å‡ºç‡
        }

    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Module 4 finished.")
    print(f"ğŸ“„ Detail: {os.path.abspath(detail_path)}")
    print(f"ğŸ“Š Summary: {os.path.abspath(summary_path)}")

    # æ‰“å°ç®€è¦æ±‡æ€»
    print("\nğŸ“ˆ Quick Summary:")
    for ratio_key, data in summary.items():
        print(f"   {ratio_key}:")
        print(f"      â”œâ”€ CER={data['avg_cer']:.2%}, WER={data['avg_wer']:.2%}")
        print(f"      â”œâ”€ BLEU={data['avg_token_bleu']:.4f}, Exact Match={data['avg_exact_match_rate']:.2%}")
        print(f"      â”œâ”€ AST Pass={data['ast_pass_rate']:.0%}")
        if data['error_counts']:
            err_str = ", ".join([f"{k}:{v}" for k, v in data['error_counts'].items()])
            print(f"      â””â”€ Errors: {err_str}")
        else:
            print(f"      â””â”€ Errors: (none detected)")


def apply_visual_corruption(image_path, ratio):
    """
    æ‰‹åŠ¨å®ç°è§†è§‰å¹²æ‰°å™¨ï¼šè¯»å–åŸå›¾ï¼Œå…ˆæŒ‰æ¯”ä¾‹ç¼©å°å†æ”¾å¤§å›åŸå°ºå¯¸ï¼ˆä¿æŒå°ºå¯¸ä¸€è‡´ï¼‰
    """
    if ratio == 1:
        return image_path
    
    try:
        with Image.open(image_path) as img:
            # è®¡ç®—æ–°å°ºå¯¸
            original_w, original_h = img.size
            new_w = max(1, int(original_w / ratio))
            new_h = max(1, int(original_h / ratio))
            
            # æ‰§è¡Œå‹ç¼© (Downsampling) -> å† Upsampling å›åŸå°ºå¯¸
            # è¿™æ ·å¯ä»¥ä¿æŒå°ºå¯¸ä¸€è‡´ï¼ŒåŒæ—¶é€šè¿‡ä¿¡æ¯ä¸¢å¤±åˆ¶é€ â€œå˜ç³Šâ€æ•ˆæœ
            small_img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)
            resized_img = small_img.resize((original_w, original_h), Image.Resampling.BILINEAR)
            
            # æ„é€ æ–°æ–‡ä»¶å: page_001.png -> page_001_ratio2.png
            dir_name = os.path.dirname(image_path)
            base_name = os.path.basename(image_path)
            name_part, ext = os.path.splitext(base_name)
            new_filename = f"{name_part}_ratio{ratio}{ext}"
            new_path = os.path.join(dir_name, new_filename)
            
            resized_img.save(new_path)
            return new_path
    except Exception as e:
        print(f"   âš ï¸ Compression failed for ratio {ratio}: {e}")
        return None

def run_full_process():
    # 0. æ¸…ç†ç¯å¢ƒ
    if os.path.exists(OUTPUT_DIR):
        try:
            shutil.rmtree(OUTPUT_DIR)
        except:
            pass # æœ‰æ—¶å€™æ–‡ä»¶å ç”¨åˆ ä¸æ‰ï¼Œå¿½ç•¥
    os.makedirs(IMAGES_DIR, exist_ok=True)

    # -------------------------------------------------
    # ğŸŸ¢ æ¨¡å—ä¸€: æ•°æ®æŒ–æ˜ (Data Miner)
    # -------------------------------------------------
    print("\n" + "="*40)
    print("ğŸš€ Running Module 1: Data Miner")
    print("="*40)
    
    dataset = fetch_fresh_code()
    
    if not dataset:
        print("âŒ No data found.")
        return

    dataset_path = os.path.join(OUTPUT_DIR, "dataset.json")
    with open(dataset_path, "w", encoding="utf-8") as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)

    # -------------------------------------------------
    # ğŸ”µ æ¨¡å—äºŒ: è§†è§‰å¹²æ‰°å™¨ (Visual Corruptor)
    # -------------------------------------------------
    print("\n" + "="*40)
    print("ğŸš€ Running Module 2: Visual Corruptor")
    print(f"ğŸ¯ Target Ratios: {TARGET_RATIOS}")
    print("="*40)

    total_images_generated = 0
    
    for idx, item in enumerate(dataset):
        code_id = item['id']
        source_code = item['code']
        
        print(f"[{idx+1}/{len(dataset)}] Processing: {code_id} ...")
        
        item_output_dir = os.path.join(IMAGES_DIR, code_id)
        os.makedirs(item_output_dir, exist_ok=True)
        
        temp_file_path = os.path.join(item_output_dir, "temp_source.py")
        with open(temp_file_path, "w", encoding="utf-8") as f:
            f.write(source_code)
            
        try:
            # 1. ç”ŸæˆåŸºå‡†é«˜æ¸…å›¾ (1x)
            # ä½¿ç”¨æ‚¨æä¾›çš„å‡½æ•°å®šä¹‰ï¼Œæ³¨æ„å‚æ•°åå˜åŒ–
            generated_paths = text_to_image_compact.generate_images_for_file(
                filename=temp_file_path,
                source_code=source_code,
                base_output_dir=item_output_dir,
                width=1024,
                height=1024,  # æ­£æ–¹å½¢
                font_size=18,  # ç¨å¾®å°ä¸€ç‚¹é€‚åº”æ­£æ–¹å½¢
                line_height=1.2,
                dpi=100,
                # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šæ”¹ä¸º Trueï¼Œä¿æŒä»£ç åŸæ ·æ¢è¡Œ ğŸŒŸ
                preserve_newlines=True,  
                enable_syntax_highlight=True,
                unique_id="base"
            )
            
            if not generated_paths:
                print("   âŒ No base image generated.")
                continue

            # 2. æ‰§è¡Œè§†è§‰å‹ç¼©å¾ªç¯ (1x, 2x, 4x, 8x)
            # æ—¢ç„¶ generate_images_for_file åªèƒ½ç”Ÿæˆä¸€ç§ï¼Œæˆ‘ä»¬åœ¨å¤–é¢æ‰‹åŠ¨å‹ç¼©
            for original_path in generated_paths:
                for ratio in TARGET_RATIOS:
                    if ratio == 1:
                        # 1x å°±æ˜¯åŸå›¾ï¼Œä¸ç”¨åŠ¨ï¼Œæˆ–è€…é‡å‘½åä¸€ä¸‹æ–¹ä¾¿ç»Ÿä¸€
                        total_images_generated += 1
                        continue
                    
                    # ç”Ÿæˆå˜ç³Šçš„å›¾
                    new_path = apply_visual_corruption(original_path, ratio)
                    if new_path:
                        # print(f"      -> Generated {ratio}x compressed: {os.path.basename(new_path)}")
                        total_images_generated += 1

        except Exception as e:
            print(f"   âŒ Error processing {code_id}: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    print("\n" + "="*40)
    print("ğŸ‰ Pipeline Stage 1 & 2 Completed!")
    print(f"ğŸ“Š Summary:")
    print(f"   - Data Mined: {len(dataset)}")
    print(f"   - Total Variants Generated: {total_images_generated}")
    print(f"   - Output Location: {os.path.abspath(OUTPUT_DIR)}")
    print("="*40)

    # -------------------------------------------------
    # ğŸŸ£ æ¨¡å—ä¸‰: æ¨ç†å¼•æ“ (Inference Engine) - GLM-4.6V
    # -------------------------------------------------
    if RUN_MODULE_3:
        run_module_3_glm46v(IMAGES_DIR, OUTPUT_DIR)

    # -------------------------------------------------
    # ğŸŸ  æ¨¡å—å››: è‡ªåŠ¨è¯„ä¼°å™¨ (Auto-Judge)
    # -------------------------------------------------
    if RUN_MODULE_4:
        run_module_4_judge(OUTPUT_DIR)

if __name__ == "__main__":
    run_full_process()