import os
import json
import shutil
import time
import base64
import re
import difflib
from collections import Counter
from PIL import Image  # ç”¨äºæ‰‹åŠ¨å®ç°è§†è§‰å‹ç¼©
import text_to_image_compact 

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ================= é…ç½®åŒº =================
OUTPUT_DIR = "./experiment_output"
IMAGES_DIR_DEFAULT = os.path.join(OUTPUT_DIR, "images_gemini")  # ğŸŒŸ Gemini ä¸“ç”¨ç›®å½•
# æ˜¯å¦ä½¿ç”¨â€œå·²æœ‰å›¾ç‰‡é›†â€ç›´æ¥ OCR + judgeï¼ˆç”¨äºè·¨æ¨¡å‹å…¬å¹³å¯¹æ¯”ï¼‰ã€‚
# - USE_EXISTING_IMAGES=1ï¼šè·³è¿‡æ¨¡å—1/2ï¼Œä¸æ¸…ç† imagesï¼›ç›´æ¥ç”¨ EXISTING_IMAGES_DIRï¼ˆæˆ–é»˜è®¤ IMAGES_DIR_DEFAULTï¼‰é‡Œçš„å›¾ç‰‡ã€‚
# - DATASET_FILENAMEï¼šæŒ‡å®šåŒä¸€ä»½ GT æ•°æ®é›†æ–‡ä»¶åï¼ˆæ”¾åœ¨ OUTPUT_DIR ä¸‹ï¼‰ï¼Œä¸¤ç§æ¨¡å‹è·‘åŒä¸€å¼ è¡¨å³å¯å¯¹æ¯”ã€‚
USE_EXISTING_IMAGES = os.getenv("USE_EXISTING_IMAGES", "0").strip().lower() in ("1", "true", "yes", "y")
EXISTING_IMAGES_DIR = os.getenv("EXISTING_IMAGES_DIR", "").strip()
IMAGES_DIR = EXISTING_IMAGES_DIR or IMAGES_DIR_DEFAULT
DEFAULT_DATASET_FILENAME = "dataset_gemini.json"
DATASET_FILENAME = os.getenv("DATASET_FILENAME", DEFAULT_DATASET_FILENAME).strip() or DEFAULT_DATASET_FILENAME
TARGET_RATIOS = [1, 2, 4, 6, 8]  # æˆ‘ä»¬çš„å‹ç¼©ç›®æ ‡


def _env_bool(name: str, default: bool) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return raw.strip().lower() in ("1", "true", "yes", "y", "on")

# ================= æ¨¡å—ä¸‰é…ç½®ï¼ˆInference Engineï¼‰=================
# ä½¿ç”¨ Geminiï¼ˆé€šè¿‡ aihubmix OpenAI-compat æ¥å£ï¼‰
RUN_MODULE_3 = _env_bool("RUN_MODULE_3", True)
AIHUBMIX_BASE_URL = "https://aihubmix.com/v1"
GEMINI_MODEL_NAME = "gemini-3-pro-preview"  # ğŸŒŸ ä¿®æ”¹ä¸º Gemini æ¨¡å‹
OCR_SYSTEM_PROMPT = "You are an OCR engine for code images."
OCR_USER_PROMPT = (
    "Transcribe the code in these images exactly.\n"
    "- These images are consecutive pages of the SAME code file, in order.\n"
    "- The page may start mid-block (e.g., indented lines without a visible 'def' header). Keep the indentation exactly as shown.\n"
    "- Do NOT invent missing context. Do NOT add wrapper code such as 'def', 'class', imports, or any extra lines.\n"
    "- Output plain text only (no Markdown, no code fences).\n"
    "- Preserve all whitespace, indentation, and newlines.\n"
    "- Do not add, remove, or rename anything.\n"
)

# Gemini Safety Settingsï¼ˆé»˜è®¤å…³é—­ï¼Œä»¥é¿å…æ”¹å˜åŸæœ‰è¡Œä¸ºï¼›éœ€è¦æ—¶é€šè¿‡ç¯å¢ƒå˜é‡å¼€å¯ï¼‰
# è¯´æ˜ï¼šä¸åŒ OpenAI-compat ä¸­è½¬å¯¹è¯¥å­—æ®µæ”¯æŒä¸ä¸€ï¼Œå¼€å¯åè‹¥æŠ¥å‚æ•°é”™è¯¯ï¼Œå¯å…³é—­è¯¥å¼€å…³ã€‚
GEMINI_ENABLE_SAFETY_SETTINGS = _env_bool("GEMINI_ENABLE_SAFETY_SETTINGS", False)
GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# Prompt å¯é€‰å¢å¼º/è¦†ç›–ï¼ˆé»˜è®¤ä¸å¯ç”¨ï¼Œä»¥é¿å…æ”¹å˜åŸæœ‰è¡Œä¸ºï¼‰
OCR_PROMPT_PERSONAL_OFFLINE = _env_bool("OCR_PROMPT_PERSONAL_OFFLINE", False)
OCR_USER_PROMPT_OVERRIDE = os.getenv("OCR_USER_PROMPT_OVERRIDE", "").strip()


def _get_ocr_user_prompt() -> str:
    """è·å– OCR user promptã€‚

    ä¼˜å…ˆçº§ï¼š
    1) OCR_USER_PROMPT_OVERRIDEï¼ˆå®Œå…¨è¦†ç›–ï¼‰
    2) OCR_PROMPT_PERSONAL_OFFLINE=1ï¼ˆåœ¨ä¸æ”¹å˜çº¦æŸçš„æƒ…å†µä¸‹ï¼Œå¢åŠ ç”¨é€”è¯´æ˜ï¼‰
    3) é»˜è®¤ OCR_USER_PROMPT
    """
    if OCR_USER_PROMPT_OVERRIDE:
        return OCR_USER_PROMPT_OVERRIDE
    if OCR_PROMPT_PERSONAL_OFFLINE:
        return (
            "Transcribe the code in these images exactly as it appears. "
            "This is for a personal offline syntax check project.\n"
            "- These images are consecutive pages of the SAME code file, in order.\n"
            "- The page may start mid-block (e.g., indented lines without a visible 'def' header). Keep the indentation exactly as shown.\n"
            "- Do NOT invent missing context. Do NOT add wrapper code such as 'def', 'class', imports, or any extra lines.\n"
            "- Output plain text only (no Markdown, no code fences).\n"
            "- Preserve all whitespace, indentation, and newlines.\n"
            "- Do not add, remove, or rename anything.\n"
        )
    return OCR_USER_PROMPT
OCR_MAX_TOKENS = 16384  # Gemini æ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡ï¼Œè¿™é‡Œè®¾ç½®ä¸ºè¾ƒå¤§å€¼
OCR_TEMPERATURE = 0.0
OCR_SLEEP_SECONDS = 0.2
OCR_MAX_RETRIES = 5
# OCR å¹¶è¡Œé…ç½®ï¼šé»˜è®¤ä¸æ”¹å˜è¡Œä¸ºï¼ˆ=1 ä¸²è¡Œï¼‰ã€‚
# è®¾ç½®ç¯å¢ƒå˜é‡ OCR_CONCURRENCY=4 å¯æ˜¾è‘—æé€Ÿï¼›å¦‚é‡åˆ°æœåŠ¡é™æµï¼Œå¯è®¾ç½® OCR_PARALLEL_MIN_INTERVAL_SECONDS åšå…¨å±€èŠ‚æµã€‚
OCR_CONCURRENCY = int(os.getenv("OCR_CONCURRENCY", "4"))
OCR_PARALLEL_MIN_INTERVAL_SECONDS = float(os.getenv("OCR_PARALLEL_MIN_INTERVAL_SECONDS", "0"))
# =========================================

# ================= æ¨¡å—å››é…ç½®ï¼ˆAuto-Judgeï¼‰=================
RUN_MODULE_4 = _env_bool("RUN_MODULE_4", True)  # æ˜¯å¦è¿è¡Œè¯„ä¼°æ¨¡å—
JUDGE_LLM_MODEL = "gpt-5-mini"  # ç”¨äºsoft taxonomyåˆ†ç±»çš„æ¨¡å‹

# é”™è¯¯åˆ†ç±»ä½“ç³» (8ç±»)
ERROR_TAXONOMY = [
    "Visual_Typo",          # è§†è§‰ç›¸ä¼¼å­—ç¬¦æ›¿æ¢ï¼Œå¦‚ O/0, l/1
    "Symbol_Loss",          # æ ‡ç‚¹ç¬¦å·ä¸¢å¤±ï¼Œå¦‚ç¼ºå°‘æ‹¬å·ã€å†’å·
    "Indentation_Error",    # ç¼©è¿›é”™è¯¯
    "Line_Skipped",         # è·³è¡Œæ¼è¯»
    "Variable_Hallucination", # å˜é‡åå¹»è§‰ï¼Œå¦‚å°† 'data' è¯»æˆ 'date'
    "Code_Invention",       # å‡­ç©ºæé€ ä¸å­˜åœ¨çš„ä»£ç 
    "Repetition",           # é‡å¤è¾“å‡ºæŸäº›è¡Œ
    "Comment_Loss"          # æ³¨é‡Šå†…å®¹ä¸¢å¤±
]
# =========================================


def _mask_api_key(key: str) -> str:
    if not key:
        return ""
    if len(key) <= 12:
        return key[:2] + "..." + key[-2:]
    return key[:6] + "..." + key[-6:]


def _try_load_api_key_from_env_files() -> str:
    """ä» .env æ–‡ä»¶ä¸­å°è¯•è¯»å– AIHUBMIX_API_KEYã€‚

    ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ï¼ˆè°ƒç”¨æ–¹å¤„ç†ï¼‰> å·¥ä½œåŒºæ ¹ç›®å½• .env > ocr/.env
    """
    script_dir = os.path.dirname(__file__)
    repo_dir = os.path.abspath(os.path.join(script_dir, os.pardir))

    candidates = [
        os.path.join(os.getcwd(), ".env"),   # å–å†³äºä½ ä»å“ªé‡Œå¯åŠ¨
        os.path.join(repo_dir, ".env"),      # ä»“åº“æ ¹ç›®å½•ï¼ˆæ›´ç¨³ï¼‰
        os.path.join(script_dir, ".env"),    # ocr/.env
    ]

    for path in candidates:
        if not os.path.exists(path):
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                for raw_line in f:
                    line = raw_line.strip()
                    if not line or line.startswith("#"):
                        continue
                    if "=" not in line:
                        continue
                    k, v = line.split("=", 1)
                    if k.strip() != "AIHUBMIX_API_KEY":
                        continue
                    value = v.strip().strip('"').strip("'")
                    if value:
                        return value
        except Exception:
            continue

    return ""


def _safe_filename_component(text: str) -> str:
    """å°†æ¨¡å‹åç­‰å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯ç”¨äºæ–‡ä»¶åçš„å®‰å…¨ç‰‡æ®µã€‚"""
    value = (text or "").strip()
    if not value:
        return "model"
    value = re.sub(r"[^a-zA-Z0-9._-]+", "_", value)
    return value[:80]


def _remove_file_if_exists(path: str) -> bool:
    try:
        if path and os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        return False
    return False


def _dataset_filename_for_model(model_name: str) -> str:
    """ä¸ºä¸åŒå¤§æ¨¡å‹ç”Ÿæˆéš”ç¦»çš„æ•°æ®é›†æ–‡ä»¶åï¼Œé¿å…äº’ç›¸è¦†ç›–ã€‚"""
    model_tag = _safe_filename_component(model_name)
    return f"dataset_{model_tag}.json"


def _iter_image_files(root_dir: str):
    for dirpath, _, filenames in os.walk(root_dir):
        for fn in filenames:
            if fn.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
                yield os.path.join(dirpath, fn)


def _load_done_set(jsonl_path: str) -> set:
    done = set()
    if not os.path.exists(jsonl_path):
        return done
    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    if obj.get("image_path"):
                        done.add(obj["image_path"])
                    if obj.get("code_id") and ("ratio" in obj):
                        done.add(f"{obj.get('code_id')}|{obj.get('ratio')}")
                except Exception:
                    continue
    except Exception:
        return done
    return done


def _encode_image_to_data_url(image_path: str) -> str:
    ext = os.path.splitext(image_path)[1].lower()
    mime = "image/png"
    if ext in (".jpg", ".jpeg"):
        mime = "image/jpeg"
    elif ext == ".webp":
        mime = "image/webp"

    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def _clean_ocr_text(text: str) -> str:
    if not text:
        return ""
    cleaned = text
    # ä¸Šæ¸¸ï¼ˆå« GLM/éƒ¨åˆ†ä¸­è½¬ï¼‰å¯èƒ½å¸¦çš„åŒ…å›´æ ‡è®°
    cleaned = cleaned.replace("<|begin_of_box|>", "").replace("<|end_of_box|>", "")
    return cleaned.strip("\n")


def _extract_response_diagnostics(resp) -> dict:
    """ä» OpenAI-compat å“åº”å¯¹è±¡ä¸­å°½é‡æå–å¯ç”¨äºæ’éšœçš„å­—æ®µã€‚

    æ³¨æ„ï¼šä¸åŒä¸­è½¬/SDK ç‰ˆæœ¬å­—æ®µå½¢çŠ¶å¯èƒ½ä¸åŒï¼›è¿™é‡Œå°½é‡å®¹é”™ï¼Œä¸å½±å“åŸæœ‰æµç¨‹ã€‚
    """
    diag: dict = {}
    try:
        resp_id = getattr(resp, "id", None)
        if resp_id:
            diag["response_id"] = resp_id
    except Exception:
        pass

    try:
        model = getattr(resp, "model", None)
        if model:
            diag["response_model"] = model
    except Exception:
        pass

    finish_reason = None
    try:
        if getattr(resp, "choices", None) and len(resp.choices) > 0:
            finish_reason = getattr(resp.choices[0], "finish_reason", None)
    except Exception:
        finish_reason = None
    if finish_reason is not None:
        diag["finish_reason"] = finish_reason

    try:
        usage = getattr(resp, "usage", None)
        if usage is not None:
            # usage å¯èƒ½æ˜¯å¯¹è±¡æˆ– dict
            if hasattr(usage, "model_dump"):
                diag["usage"] = usage.model_dump()
            elif isinstance(usage, dict):
                diag["usage"] = usage
    except Exception:
        pass

    # æŸäº›å®ç°å¯èƒ½æä¾› refusal / safety ä¿¡æ¯ï¼ˆå°½é‡æŠ“å–ï¼Œä¸åšå¼ºä¾èµ–ï¼‰
    try:
        if getattr(resp, "choices", None) and len(resp.choices) > 0:
            msg = getattr(resp.choices[0], "message", None)
            refusal = getattr(msg, "refusal", None) if msg is not None else None
            if refusal:
                diag["refusal"] = refusal
    except Exception:
        pass

    return diag


def _parse_ratio_from_filename(image_path: str) -> int:
    # e.g. page_001_ratio2.png -> 2 ; page_001.png -> 1
    stem = os.path.splitext(os.path.basename(image_path))[0]
    marker = "_ratio"
    if marker in stem:
        try:
            return int(stem.split(marker, 1)[1])
        except Exception:
            return 1
    return 1


def _extract_page_num_from_filename(image_path: str) -> int:
    """page_001_ratio2.png -> 1ï¼›æå–ä¸åˆ°åˆ™è¿”å› 0ã€‚"""
    stem = os.path.splitext(os.path.basename(image_path))[0]
    m = re.search(r"page_(\d+)", stem)
    if not m:
        return 0
    try:
        return int(m.group(1))
    except Exception:
        return 0


def run_module_3_gemini(images_dir: str, output_dir: str):
    print("\n" + "=" * 40)
    print(f"ğŸš€ Running Module 3: Inference Engine ({GEMINI_MODEL_NAME})")
    print("=" * 40)

    if OpenAI is None:
        print("âŒ Missing dependency: openai. Run: pip install openai")
        return

    api_key = os.getenv("AIHUBMIX_API_KEY")
    api_key_source = "env:AIHUBMIX_API_KEY" if api_key else ""
    if not api_key:
        api_key = _try_load_api_key_from_env_files()
        if api_key:
            api_key_source = "file:.env"
    if not api_key:
        searched = [
            os.path.join(os.getcwd(), ".env"),
            os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, ".env")),
            os.path.join(os.path.dirname(__file__), ".env"),
        ]
        print("âŒ Missing AIHUBMIX_API_KEY.")
        print("   - æ–¹å¼1ï¼šPowerShell ä¸´æ—¶è®¾ç½®ï¼š$env:AIHUBMIX_API_KEY=\"sk-...\"")
        print("   - æ–¹å¼2ï¼šå†™å…¥ .env æ–‡ä»¶ï¼ˆä¸è¦æäº¤åˆ°ä»“åº“ï¼‰")
        print("     æ ¼å¼ï¼šAIHUBMIX_API_KEY=sk-...")
        print("     æŸ¥æ‰¾è·¯å¾„ï¼š")
        for p in searched:
            print(f"       - {p}")
        return

    print(f"ğŸ”‘ AIHUBMIX_API_KEY loaded ({api_key_source}): {_mask_api_key(api_key)}")

    os.makedirs(output_dir, exist_ok=True)
    out_jsonl = os.path.join(output_dir, "gemini_ocr.jsonl")  # ğŸŒŸ ä¿®æ”¹è¾“å‡ºæ–‡ä»¶å
    done = _load_done_set(out_jsonl)

    client = OpenAI(api_key=api_key, base_url=AIHUBMIX_BASE_URL)

    total = 0
    skipped = 0
    errors = 0

    # single-turnï¼šæŒ‰ (code_id, ratio) åˆ†ç»„ï¼ŒæŠŠåŒä¸€ä¸ªæ ·æœ¬çš„æ‰€æœ‰ pages åœ¨åŒä¸€æ¬¡è¯·æ±‚ä¸­å‘é€
    image_paths = list(_iter_image_files(images_dir))
    from collections import defaultdict

    grouped_images = defaultdict(list)  # (code_id, ratio) -> [image_path...]
    for image_path in image_paths:
        parent_dir = os.path.dirname(image_path)
        code_id_dir = os.path.dirname(parent_dir)
        code_id = os.path.basename(code_id_dir)
        ratio = _parse_ratio_from_filename(image_path)
        grouped_images[(code_id, ratio)].append(image_path)

    cases = []  # [(code_id, ratio, [paths...])]
    for (code_id, ratio), paths in grouped_images.items():
        paths.sort(key=lambda p: (_extract_page_num_from_filename(p), os.path.basename(p)))
        cases.append((code_id, ratio, paths))
    cases.sort(key=lambda x: (x[0], x[1]))

    print(f"ğŸ§© Total cases to OCR (single-turn): {len(cases)}")

    if OCR_CONCURRENCY <= 1:
        for i, (code_id, ratio, page_paths) in enumerate(cases, start=1):
            case_key = f"{code_id}|{ratio}"
            if case_key in done:
                skipped += 1
                continue
            print(
                f"[{i}/{len(cases)}] OCR(single-turn): {code_id} @ ratio {ratio}x ({len(page_paths)} pages)"
            )

            content = [{"type": "text", "text": _get_ocr_user_prompt()}]
            for p in page_paths:
                data_url = _encode_image_to_data_url(p)
                content.append({"type": "image_url", "image_url": {"url": data_url}})

            last_err = None
            text = ""
            diagnostics = {}

            for attempt in range(1, OCR_MAX_RETRIES + 1):
                try:
                    extra_body = {"safety_settings": GEMINI_SAFETY_SETTINGS} if GEMINI_ENABLE_SAFETY_SETTINGS else None
                    resp = client.chat.completions.create(
                        model=GEMINI_MODEL_NAME,  # ğŸŒŸ ä½¿ç”¨ Gemini æ¨¡å‹
                        temperature=OCR_TEMPERATURE,
                        max_tokens=OCR_MAX_TOKENS,
                        messages=[
                            {"role": "system", "content": OCR_SYSTEM_PROMPT},
                            {
                                "role": "user",
                                "content": content,
                            },
                        ],
                        extra_body=extra_body,
                    )
                    text = _clean_ocr_text(resp.choices[0].message.content or "")
                    diagnostics = _extract_response_diagnostics(resp)
                    last_err = None
                    break
                except Exception as e:
                    last_err = str(e)
                    # exponential backoff: 1,2,4,8,... capped at 30s
                    backoff = min(30.0, float(2 ** (attempt - 1)))
                    time.sleep(backoff)

            rec = {
                "code_id": code_id,
                "ratio": ratio,
                "num_pages": len(page_paths),
                "image_paths": page_paths,
                "image_path": page_paths[0] if page_paths else "",
                "model": GEMINI_MODEL_NAME,  # ğŸŒŸ è®°å½•æ¨¡å‹åç§°
            }

            if diagnostics:
                rec.update(diagnostics)

            if last_err is None:
                rec["text"] = text
                rec["text_len"] = len(text)
                if rec.get("finish_reason") in ("content_filter", "safety"):
                    rec["blocked_by_safety"] = True
                total += 1
            else:
                rec["error"] = last_err
                errors += 1

            with open(out_jsonl, "a", encoding="utf-8") as f:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")

            time.sleep(OCR_SLEEP_SECONDS)
    else:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading

        pending_cases = [(code_id, ratio, page_paths) for (code_id, ratio, page_paths) in cases if f"{code_id}|{ratio}" not in done]
        skipped = len(cases) - len(pending_cases)

        print(
            f"âš¡ Parallel OCR enabled: workers={OCR_CONCURRENCY}, "
            f"global_min_interval={OCR_PARALLEL_MIN_INTERVAL_SECONDS}s"
        )

        client_local = threading.local()
        write_lock = threading.Lock()
        rate_lock = threading.Lock()
        next_allowed_time = 0.0

        def _get_client():
            c = getattr(client_local, "client", None)
            if c is None:
                c = OpenAI(api_key=api_key, base_url=AIHUBMIX_BASE_URL)
                client_local.client = c
            return c

        def _rate_limit_wait():
            nonlocal next_allowed_time
            interval = float(OCR_PARALLEL_MIN_INTERVAL_SECONDS)
            if interval <= 0:
                return
            with rate_lock:
                now = time.monotonic()
                if now < next_allowed_time:
                    wait_s = next_allowed_time - now
                    next_allowed_time = next_allowed_time + interval
                else:
                    wait_s = 0.0
                    next_allowed_time = now + interval
            if wait_s > 0:
                time.sleep(wait_s)

        def _ocr_one_case(code_id: str, ratio: int, page_paths: list[str]):
            content = [{"type": "text", "text": _get_ocr_user_prompt()}]
            for p in page_paths:
                data_url = _encode_image_to_data_url(p)
                content.append({"type": "image_url", "image_url": {"url": data_url}})

            last_err = None
            text = ""
            diagnostics = {}

            for attempt in range(1, OCR_MAX_RETRIES + 1):
                try:
                    _rate_limit_wait()
                    extra_body = {"safety_settings": GEMINI_SAFETY_SETTINGS} if GEMINI_ENABLE_SAFETY_SETTINGS else None
                    resp = _get_client().chat.completions.create(
                        model=GEMINI_MODEL_NAME,
                        temperature=OCR_TEMPERATURE,
                        max_tokens=OCR_MAX_TOKENS,
                        messages=[
                            {"role": "system", "content": OCR_SYSTEM_PROMPT},
                            {
                                "role": "user",
                                "content": content,
                            },
                        ],
                        extra_body=extra_body,
                    )
                    text = _clean_ocr_text(resp.choices[0].message.content or "")
                    diagnostics = _extract_response_diagnostics(resp)
                    last_err = None
                    break
                except Exception as e:
                    last_err = str(e)
                    backoff = min(30.0, float(2 ** (attempt - 1)))
                    time.sleep(backoff)

            rec = {
                "code_id": code_id,
                "ratio": ratio,
                "num_pages": len(page_paths),
                "image_paths": page_paths,
                "image_path": page_paths[0] if page_paths else "",
                "model": GEMINI_MODEL_NAME,
            }
            if diagnostics:
                rec.update(diagnostics)
            if last_err is None:
                rec["text"] = text
                rec["text_len"] = len(text)
                if rec.get("finish_reason") in ("content_filter", "safety"):
                    rec["blocked_by_safety"] = True
                return rec, True
            rec["error"] = last_err
            return rec, False

        completed = 0
        total_jobs = len(pending_cases)

        with ThreadPoolExecutor(max_workers=OCR_CONCURRENCY) as ex:
            futures = {ex.submit(_ocr_one_case, code_id, ratio, page_paths): (code_id, ratio, page_paths) for (code_id, ratio, page_paths) in pending_cases}
            for fut in as_completed(futures):
                code_id, ratio, page_paths = futures[fut]
                try:
                    rec, ok = fut.result()
                except Exception as e:
                    rec = {
                        "code_id": code_id,
                        "ratio": ratio,
                        "num_pages": len(page_paths),
                        "image_paths": page_paths,
                        "image_path": page_paths[0] if page_paths else "",
                        "model": GEMINI_MODEL_NAME,
                        "error": f"worker_exception: {e}",
                    }
                    ok = False

                with write_lock:
                    with open(out_jsonl, "a", encoding="utf-8") as f:
                        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

                completed += 1
                if ok:
                    total += 1
                else:
                    errors += 1
                print(
                    f"[{completed}/{total_jobs}] OCR done: {code_id} @ ratio {ratio}x "
                    f"({'ok' if ok else 'error'})"
                )

    print(f"âœ… Module 3 finished. ok={total}, skipped={skipped}, error={errors}")
    print(f"ğŸ“„ Output: {os.path.abspath(out_jsonl)}")


# ============================================================
# ğŸŸ  æ¨¡å—å››: Auto-Judge (è¯„ä¼°å™¨)
# ============================================================

def normalize_code(text: str) -> str:
    """
    ä»£ç è§„èŒƒåŒ–ï¼šå‹ç¼©ç©ºè¡Œ + å»é™¤è¡Œå°¾ç©ºæ ¼ + tabâ†’4ç©ºæ ¼
    ç”¨äºè®¡ç®— CER/WER/BLEU ç­‰æŒ‡æ ‡æ—¶å‡å°‘æ ¼å¼å™ªå£°
    """
    lines = text.splitlines()
    
    # 1. Tab â†’ 4 spaces
    lines = [line.replace('\t', '    ') for line in lines]
    
    # 2. å»é™¤è¡Œå°¾ç©ºæ ¼ï¼ˆtrailing spacesï¼‰
    lines = [line.rstrip() for line in lines]
    
    # 3. å‹ç¼©è¿ç»­ç©ºè¡Œä¸ºå•ä¸ªç©ºè¡Œ
    normalized = []
    prev_blank = False
    for line in lines:
        is_blank = (line.strip() == '')
        if is_blank:
            if not prev_blank:  # åªä¿ç•™ç¬¬ä¸€ä¸ªç©ºè¡Œ
                normalized.append('')
            prev_blank = True
        else:
            normalized.append(line)
            prev_blank = False
    
    # 4. å»é™¤é¦–å°¾ç©ºè¡Œ
    while normalized and normalized[0] == '':
        normalized.pop(0)
    while normalized and normalized[-1] == '':
        normalized.pop()
    
    return '\n'.join(normalized)


def _split_nonblank_lines_for_diff(text: str) -> list[str]:
    """ç”¨äº codediff çš„é¢„å¤„ç†ï¼štab->4ç©ºæ ¼ã€å»è¡Œå°¾ç©ºæ ¼ã€åˆ é™¤æ‰€æœ‰ç©ºè¡Œï¼ˆä¸åŠ¨è¡Œé¦–ç¼©è¿›ï¼‰ã€‚"""
    lines = text.splitlines()
    lines = [line.replace('\t', '    ').rstrip() for line in lines]
    return [line for line in lines if line.strip() != ""]


def _compute_codediff_metrics_no_blank(reference: str, hypothesis: str) -> dict:
    ref_lines = _split_nonblank_lines_for_diff(reference)
    hyp_lines = _split_nonblank_lines_for_diff(hypothesis)

    sm = difflib.SequenceMatcher(a=ref_lines, b=hyp_lines, autojunk=False)
    opcodes = sm.get_opcodes()

    added = 0
    removed = 0
    replaced = 0
    hunks = 0

    for tag, i1, i2, j1, j2 in opcodes:
        if tag != "equal":
            hunks += 1
        if tag == "insert":
            added += (j2 - j1)
        elif tag == "delete":
            removed += (i2 - i1)
        elif tag == "replace":
            replaced += max(i2 - i1, j2 - j1)

    ref_line_count = len(ref_lines)
    change = added + removed + replaced

    return {
        "line_similarity": round(sm.ratio(), 4),
        "added_lines": added,
        "removed_lines": removed,
        "replaced_lines": replaced,
        "diff_hunks": hunks,
        "change_rate": round(change / max(1, ref_line_count), 4),
        "ref_nonblank_lines": ref_line_count,
        "hyp_nonblank_lines": len(hyp_lines),
    }


def _compute_cer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate)
    CER = (S + D + I) / N
    ä½¿ç”¨ Levenshtein ç¼–è¾‘è·ç¦»
    """
    ref = list(reference)
    hyp = list(hypothesis)
    n = len(ref)
    m = len(hyp)

    if n == 0:
        return 1.0 if m > 0 else 0.0

    # DP çŸ©é˜µ
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if ref[i - 1] == hyp[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,      # deletion
                dp[i][j - 1] + 1,      # insertion
                dp[i - 1][j - 1] + cost  # substitution
            )

    edit_distance = dp[n][m]
    return edit_distance / n


def _check_ast_parsable(code: str) -> bool:
    """æ£€æŸ¥ä»£ç æ˜¯å¦å¯è¢« Python AST è§£æ"""
    import ast
    try:
        ast.parse(code)
        return True
    except SyntaxError:
        return False


def _compute_keyword_f1(reference: str, hypothesis: str) -> float:
    """è®¡ç®—è¯­è¨€å…³é”®å­—çš„ F1ï¼ˆCodeBLEU çš„ keyword-match å­é¡¹ï¼Œè½»é‡å®ç°ï¼‰ã€‚"""
    import keyword

    def tokenize(text: str) -> list[str]:
        return re.findall(r"\w+|[^\w\s]", text)

    keywords = set(keyword.kwlist)
    ref_kw = [t for t in tokenize(reference) if t in keywords]
    hyp_kw = [t for t in tokenize(hypothesis) if t in keywords]

    if not ref_kw and not hyp_kw:
        return 1.0
    if not ref_kw or not hyp_kw:
        return 0.0

    from collections import Counter

    ref_c = Counter(ref_kw)
    hyp_c = Counter(hyp_kw)
    overlap = sum(min(ref_c[k], hyp_c.get(k, 0)) for k in ref_c)

    precision = overlap / max(1, sum(hyp_c.values()))
    recall = overlap / max(1, sum(ref_c.values()))
    if precision + recall == 0:
        return 0.0
    return 2 * precision * recall / (precision + recall)


def _compute_codebleu(reference: str, hypothesis: str) -> float:
    """è½»é‡ç‰ˆ CodeBLEUï¼šngram-match(token BLEU) + keyword-match(F1)ã€‚

    è¯´æ˜ï¼šå®Œæ•´ CodeBLEU è¿˜åŒ…å« syntax-match / dataflow-matchï¼Œé€šå¸¸éœ€è¦ tree-sitter ä¸æ•°æ®æµæå–ã€‚
    è¿™é‡Œå…ˆå®ç°æ— é¢å¤–ä¾èµ–çš„ç‰ˆæœ¬ï¼Œä¾¿äºå·¥ç¨‹ç¨³å®šè½åœ°ã€‚
    """
    ngram = _compute_token_bleu(reference, hypothesis)
    kw_f1 = _compute_keyword_f1(reference, hypothesis)
    return 0.8 * ngram + 0.2 * kw_f1


def _compute_token_bleu(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—åŸºäº token çš„ç®€åŒ– BLEU (CodeBLEU æ ¸å¿ƒ)
    ä½¿ç”¨ 1-gram åˆ° 4-gram çš„å‡ ä½•å¹³å‡
    """
    import re
    from collections import Counter
    import math

    def tokenize(text):
        # ç®€å•çš„ä»£ç åˆ†è¯ï¼šæŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²
        return re.findall(r'\w+|[^\w\s]', text)

    ref_tokens = tokenize(reference)
    hyp_tokens = tokenize(hypothesis)

    if len(hyp_tokens) == 0:
        return 0.0

    # è®¡ç®— n-gram precision
    def ngram_precision(ref_toks, hyp_toks, n):
        if len(hyp_toks) < n:
            return 0.0
        ref_ngrams = Counter(tuple(ref_toks[i:i+n]) for i in range(len(ref_toks) - n + 1))
        hyp_ngrams = Counter(tuple(hyp_toks[i:i+n]) for i in range(len(hyp_toks) - n + 1))

        match = 0
        total = 0
        for ng, cnt in hyp_ngrams.items():
            match += min(cnt, ref_ngrams.get(ng, 0))
            total += cnt
        return match / total if total > 0 else 0.0

    # è®¡ç®— 1-gram åˆ° 4-gram çš„ precision
    precisions = []
    for n in range(1, 5):
        p = ngram_precision(ref_tokens, hyp_tokens, n)
        precisions.append(p)

    # è¿‡æ»¤æ‰ 0 å€¼ï¼ˆé¿å… log(0)ï¼‰
    non_zero = [p for p in precisions if p > 0]
    if not non_zero:
        return 0.0

    # å‡ ä½•å¹³å‡
    log_avg = sum(math.log(p) for p in non_zero) / len(non_zero)
    bleu = math.exp(log_avg)

    # Brevity penalty
    bp = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0

    return bp * bleu


def _compute_wer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—è¯é”™è¯¯ç‡ (Word Error Rate)
    WER = (S + D + I) / Nï¼Œä»¥è¯ä¸ºå•ä½
    """
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    n = len(ref_words)
    m = len(hyp_words)

    if n == 0:
        return 1.0 if m > 0 else 0.0

    # DP çŸ©é˜µ
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,      # deletion
                dp[i][j - 1] + 1,      # insertion
                dp[i - 1][j - 1] + cost  # substitution
            )

    edit_distance = dp[n][m]
    return edit_distance / n


def _compute_exact_match_rate(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ (Exact Match Rate)
    å®Œå…¨åŒ¹é…çš„è¡Œæ•° / æ€»è¡Œæ•°
    """
    ref_lines = reference.split('\n')
    hyp_lines = hypothesis.split('\n')
    
    if len(ref_lines) == 0:
        return 1.0 if len(hyp_lines) == 0 else 0.0
    
    # å¯¹é½æ¯”è¾ƒï¼šå–ä¸¤è€…ä¸­è¾ƒçŸ­çš„é•¿åº¦
    matched = 0
    for i, ref_line in enumerate(ref_lines):
        if i < len(hyp_lines) and ref_line == hyp_lines[i]:
            matched += 1
    
    return matched / len(ref_lines)


# ============================================================
# ğŸ·ï¸ å…«å¤§é”™è¯¯è°±ç³»æ£€æµ‹å‡½æ•°ï¼ˆåŸºäºè§„åˆ™ï¼Œä¸ä¾èµ– LLMï¼‰
# ============================================================

def _detect_visual_typo(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹å½¢è¿‘å­—æ··æ·† (1 vs l, 0 vs O, etc.)
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    # å½¢è¿‘å­—å¯¹ï¼ˆåŒå‘æ£€æµ‹ï¼‰
    confusable_pairs = [
        ('1', 'l'), ('1', 'I'), ('l', 'I'),  # 1/l/I
        ('0', 'O'), ('0', 'o'), ('O', 'o'),  # 0/O/o
        ('5', 'S'), ('5', 's'),              # 5/S
        ('8', 'B'),                          # 8/B
        ('2', 'Z'), ('2', 'z'),              # 2/Z
        ('6', 'G'),                          # 6/G
        ('rn', 'm'),                         # rn/m (è¿å­—)
        ("'", '`'), ('"', "''"),             # å¼•å·æ··æ·†
    ]
    
    ref_lower = reference.lower()
    hyp_lower = hypothesis.lower()
    
    for a, b in confusable_pairs:
        # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿäº†æ›¿æ¢ï¼šref ä¸­æœ‰ aï¼Œhyp ä¸­å¯¹åº”ä½ç½®å˜æˆäº† b
        ref_count_a = reference.count(a)
        hyp_count_a = hypothesis.count(a)
        ref_count_b = reference.count(b)
        hyp_count_b = hypothesis.count(b)
        
        # å¦‚æœ a åœ¨ ref ä¸­æ›´å¤šï¼Œä½† b åœ¨ hyp ä¸­æ›´å¤šï¼Œå¯èƒ½å‘ç”Ÿäº†æ··æ·†
        if ref_count_a > hyp_count_a and hyp_count_b > ref_count_b:
            return 1
        if ref_count_b > hyp_count_b and hyp_count_a > ref_count_a:
            return 1
    
    return 0


def _detect_symbol_loss(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ç¬¦å·ä¸¢å¤± (_, :, ;, æ‹¬å·ç­‰)
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    critical_symbols = ['_', ':', ';', '(', ')', '[', ']', '{', '}', ',', '.', '=', '+', '-', '*', '/']
    
    for sym in critical_symbols:
        ref_count = reference.count(sym)
        hyp_count = hypothesis.count(sym)
        
        # å¦‚æœ ref ä¸­çš„ç¬¦å·æ¯” hyp ä¸­å¤š 20% ä»¥ä¸Šï¼Œè®¤ä¸ºå‘ç”Ÿäº†ä¸¢å¤±
        if ref_count > 0 and hyp_count < ref_count * 0.8:
            return 1
    
    return 0


def _detect_indentation_error(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ç¼©è¿›é”™è¯¯
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    ref_lines = reference.split('\n')
    hyp_lines = hypothesis.split('\n')
    
    # è®¡ç®—æ¯è¡Œå¼€å¤´çš„ç©ºæ ¼æ•°
    def get_indent(line):
        return len(line) - len(line.lstrip(' \t'))
    
    # å–ä¸¤è€…è¾ƒçŸ­çš„é•¿åº¦è¿›è¡Œæ¯”è¾ƒ
    min_len = min(len(ref_lines), len(hyp_lines))
    
    indent_errors = 0
    for i in range(min_len):
        ref_indent = get_indent(ref_lines[i])
        hyp_indent = get_indent(hyp_lines[i])
        
        # å¦‚æœç¼©è¿›å·®å¼‚è¶…è¿‡ 2 ä¸ªç©ºæ ¼ï¼Œè®¤ä¸ºæ˜¯é”™è¯¯
        if abs(ref_indent - hyp_indent) >= 2:
            indent_errors += 1
    
    # å¦‚æœè¶…è¿‡ 5% çš„è¡Œæœ‰ç¼©è¿›é”™è¯¯ï¼Œæ ‡è®°ä¸º 1
    if min_len > 0 and indent_errors / min_len > 0.05:
        return 1
    
    return 0


def _detect_line_skipped(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹æ•´è¡Œä¸¢å¤±
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    ref_lines = [line.strip() for line in reference.split('\n') if line.strip()]
    hyp_lines = [line.strip() for line in hypothesis.split('\n') if line.strip()]
    
    # å¦‚æœ hyp çš„è¡Œæ•°å°‘äº ref çš„ 90%ï¼Œè®¤ä¸ºå‘ç”Ÿäº†è¡Œä¸¢å¤±
    if len(hyp_lines) < len(ref_lines) * 0.9:
        return 1
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ ref ä¸­çš„è¡Œå®Œå…¨ä¸åœ¨ hyp ä¸­
    hyp_set = set(hyp_lines)
    missing_lines = 0
    for line in ref_lines:
        if len(line) > 10 and line not in hyp_set:  # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„è¡Œ
            missing_lines += 1
    
    # å¦‚æœè¶…è¿‡ 10% çš„æœ‰æ„ä¹‰è¡Œä¸¢å¤±ï¼Œæ ‡è®°ä¸º 1
    if len(ref_lines) > 0 and missing_lines / len(ref_lines) > 0.1:
        return 1
    
    return 0


def _detect_variable_hallucination(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹å˜é‡åå¹»è§‰ï¼ˆOCR ä¸­å‡ºç°äº† reference ä¸­ä¸å­˜åœ¨çš„æ ‡è¯†ç¬¦ï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    import re
    
    # æå–æ ‡è¯†ç¬¦ï¼ˆå˜é‡åã€å‡½æ•°åç­‰ï¼‰
    def extract_identifiers(code):
        # åŒ¹é… Python æ ‡è¯†ç¬¦
        identifiers = set(re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', code))
        # æ’é™¤ Python å…³é”®å­—å’Œå¸¸è§å†…ç½®å‡½æ•°
        keywords = {'def', 'class', 'if', 'else', 'elif', 'for', 'while', 'return', 
                   'import', 'from', 'as', 'try', 'except', 'finally', 'with', 
                   'True', 'False', 'None', 'and', 'or', 'not', 'in', 'is',
                   'print', 'len', 'range', 'str', 'int', 'float', 'list', 'dict',
                   'self', 'cls', 'args', 'kwargs'}
        return identifiers - keywords
    
    ref_ids = extract_identifiers(reference)
    hyp_ids = extract_identifiers(hypothesis)
    
    # æ‰¾å‡º hyp ä¸­æœ‰ä½† ref ä¸­æ²¡æœ‰çš„æ ‡è¯†ç¬¦ï¼ˆå¹»è§‰ï¼‰
    hallucinated = hyp_ids - ref_ids
    
    # è¿‡æ»¤æ‰é•¿åº¦å°äº 3 çš„ï¼ˆå¯èƒ½æ˜¯è¯¯åˆ¤ï¼‰
    hallucinated = {h for h in hallucinated if len(h) >= 3}
    
    # å¦‚æœæœ‰è¶…è¿‡ 3 ä¸ªå¹»è§‰æ ‡è¯†ç¬¦ï¼Œæ ‡è®°ä¸º 1
    if len(hallucinated) >= 3:
        return 1
    
    return 0


def _detect_code_invention(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹ä»£ç æé€ ï¼ˆOCR ä¸­å‡ºç°äº†å®Œå…¨ä¸å­˜åœ¨çš„ä»£ç æ®µï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    hyp_lines = [line.strip() for line in hypothesis.split('\n') if line.strip()]
    
    invented_lines = 0
    for line in hyp_lines:
        # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„è¡Œï¼ˆé•¿åº¦ > 15ï¼‰
        if len(line) > 15:
            # å¦‚æœè¿™è¡Œåœ¨ reference ä¸­å®Œå…¨æ‰¾ä¸åˆ°ä»»ä½•ç›¸ä¼¼ç‰‡æ®µ
            if line not in reference and line[:20] not in reference:
                invented_lines += 1
    
    # å¦‚æœæœ‰è¶…è¿‡ 5% çš„è¡Œæ˜¯æé€ çš„ï¼Œæ ‡è®°ä¸º 1
    if len(hyp_lines) > 0 and invented_lines / len(hyp_lines) > 0.05:
        return 1
    
    return 0


def _detect_repetition(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹é‡å¤è¾“å‡ºï¼ˆå¤è¯»æœºç°è±¡ï¼‰
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    hyp_lines = [line for line in hypothesis.split('\n') if line.strip()]
    
    if len(hyp_lines) < 3:
        return 0
    
    # æ£€æµ‹è¿ç»­é‡å¤çš„è¡Œ
    consecutive_repeats = 0
    for i in range(1, len(hyp_lines)):
        if hyp_lines[i] == hyp_lines[i-1] and len(hyp_lines[i].strip()) > 5:
            consecutive_repeats += 1
    
    # å¦‚æœæœ‰ 2 è¡Œä»¥ä¸Šè¿ç»­é‡å¤ï¼Œæ ‡è®°ä¸º 1
    if consecutive_repeats >= 2:
        return 1
    
    # æ£€æµ‹éè¿ç»­çš„å¤§é‡é‡å¤
    from collections import Counter
    line_counts = Counter(line for line in hyp_lines if len(line.strip()) > 10)
    
    # å¦‚æœæœ‰ä»»ä½•è¡Œé‡å¤è¶…è¿‡ 3 æ¬¡ï¼Œæ ‡è®°ä¸º 1
    for line, count in line_counts.items():
        if count >= 3:
            return 1
    
    return 0


def _detect_comment_loss(reference: str, hypothesis: str) -> int:
    """
    æ£€æµ‹æ³¨é‡Šä¸¢å¤±æˆ–ä¹±ç 
    è¿”å›: 1 = æ£€æµ‹åˆ°, 0 = æœªæ£€æµ‹åˆ°
    """
    # æå–æ³¨é‡Šè¡Œ
    def extract_comments(code):
        comments = []
        for line in code.split('\n'):
            stripped = line.strip()
            if stripped.startswith('#'):
                comments.append(stripped)
            # ä¹Ÿæ£€æµ‹è¡Œå†…æ³¨é‡Š
            if '#' in line:
                comment_part = line.split('#', 1)[1].strip()
                if comment_part:
                    comments.append('#' + comment_part)
        return comments
    
    ref_comments = extract_comments(reference)
    hyp_comments = extract_comments(hypothesis)
    
    if not ref_comments:
        return 0  # åŸä»£ç æ²¡æœ‰æ³¨é‡Š
    
    # æ£€æŸ¥æ³¨é‡Šæ•°é‡æ˜¯å¦å¤§å¹…å‡å°‘
    if len(hyp_comments) < len(ref_comments) * 0.7:
        return 1
    
    # æ£€æŸ¥æ³¨é‡Šå†…å®¹æ˜¯å¦ä¸¥é‡å˜å½¢
    matched = 0
    for ref_c in ref_comments:
        for hyp_c in hyp_comments:
            # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æµ‹
            if ref_c in hyp_c or hyp_c in ref_c:
                matched += 1
                break
            # æˆ–è€…è¶…è¿‡ 70% çš„å­—ç¬¦åŒ¹é…
            common_chars = sum(1 for c in ref_c if c in hyp_c)
            if len(ref_c) > 0 and common_chars / len(ref_c) > 0.7:
                matched += 1
                break
    
    # å¦‚æœå°‘äº 70% çš„æ³¨é‡Šè¢«æ­£ç¡®ä¿ç•™ï¼Œæ ‡è®°ä¸º 1
    if len(ref_comments) > 0 and matched / len(ref_comments) < 0.7:
        return 1
    
    return 0


def _detect_all_taxonomy_errors(reference: str, hypothesis: str) -> dict:
    """
    æ£€æµ‹æ‰€æœ‰å…«å¤§é”™è¯¯è°±ç³»ï¼Œè¿”å› 0/1 æ ‡ç­¾å­—å…¸
    """
    return {
        "Visual_Typo": _detect_visual_typo(reference, hypothesis),
        "Symbol_Loss": _detect_symbol_loss(reference, hypothesis),
        "Indentation_Error": _detect_indentation_error(reference, hypothesis),
        "Line_Skipped": _detect_line_skipped(reference, hypothesis),
        "Variable_Hallucination": _detect_variable_hallucination(reference, hypothesis),
        "Code_Invention": _detect_code_invention(reference, hypothesis),
        "Repetition": _detect_repetition(reference, hypothesis),
        "Comment_Loss": _detect_comment_loss(reference, hypothesis),
    }


def _call_llm_for_taxonomy(client, reference: str, hypothesis: str) -> list:
    """
    è°ƒç”¨ LLM è¿›è¡Œé”™è¯¯åˆ†ç±»
    è¿”å›æ£€æµ‹åˆ°çš„é”™è¯¯ç±»å‹åˆ—è¡¨
    """
    prompt = f"""You are an expert code quality evaluator. 
Compare the reference code with the OCR output and identify error types.

Reference code:
```
{reference[:3000]}
```

OCR output:
```
{hypothesis[:3000]}
```

Analyze the differences and return ONLY a JSON array of error types from this list:
{ERROR_TAXONOMY}

Rules:
- Return an empty array [] if the output matches the reference perfectly
- Only include error types that are clearly present
- Return ONLY the JSON array, no other text

Example response: ["Visual_Typo", "Symbol_Loss"]
"""

    try:
        resp = client.chat.completions.create(
            model=JUDGE_LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=256,
            temperature=0.0,
        )
        content = resp.choices[0].message.content.strip()
        # è§£æ JSON
        import re
        match = re.search(r'\[.*?\]', content, re.DOTALL)
        if match:
            return json.loads(match.group())
        return []
    except Exception as e:
        print(f"   âš ï¸ LLM taxonomy call failed: {e}")
        return []


def run_module_4_judge(
    output_dir: str,
    ocr_jsonl_filename: str,
    ocr_model_name: str,
    dataset_json_filename: str | None = None,
):
    """
    æ¨¡å—4: Auto-Judge è¯„ä¼°å™¨
    è¯»å– OCR ç»“æœï¼Œä¸åŸå§‹ä»£ç å¯¹æ¯”ï¼Œè¾“å‡ºè¯„ä¼°æŒ‡æ ‡
    """
    model_tag = _safe_filename_component(ocr_model_name)

    print("\n" + "=" * 40)
    print(f"ğŸš€ Running Module 4: Auto-Judge ({ocr_model_name})")
    print("=" * 40)

    # è¯»å– dataset (æŒ‰æ¨¡å‹éš”ç¦»ï¼Œé¿å…è·¨è„šæœ¬è¦†ç›–)
    preferred_dataset = dataset_json_filename or _dataset_filename_for_model(ocr_model_name)
    dataset_path = os.path.join(output_dir, preferred_dataset)
    if not os.path.exists(dataset_path):
        legacy_path = os.path.join(output_dir, "dataset.json")
        if os.path.exists(legacy_path):
            dataset_path = legacy_path
        else:
            print(f"âŒ {preferred_dataset} not found (and dataset.json not found), skipping Module 4")
            return

    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    # å»ºç«‹ code_id -> code çš„æ˜ å°„
    code_map = {item["id"]: item["code"] for item in dataset}

    # è¯»å– OCR ç»“æœ
    ocr_path = os.path.join(output_dir, ocr_jsonl_filename)
    if not os.path.exists(ocr_path):
        print(f"âŒ {ocr_jsonl_filename} not found, skipping Module 4")
        return

    ocr_results = []
    with open(ocr_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                ocr_results.append(json.loads(line))

    # ğŸŒŸ æŒ‰ (code_id, ratio) åˆ†ç»„ï¼š
    # - æ—§æ ¼å¼ï¼šæ¯é¡µä¸€æ¡è®°å½•ï¼ˆimage_path/textï¼‰ï¼Œéœ€è¦åˆå¹¶å¤šé¡µ
    # - æ–°æ ¼å¼ï¼ˆsingle-turnï¼‰ï¼šæ¯æ ·æœ¬ä¸€æ¡è®°å½•ï¼ˆimage_paths/num_pages/textï¼‰ï¼Œæ— éœ€å†æ‹¼é¡µ
    from collections import defaultdict
    grouped = defaultdict(list)  # (code_id, ratio) -> [{"text": ..., "image_path": ..., "num_pages": ...?}, ...]

    for rec in ocr_results:
        ratio = rec.get("ratio", 1)
        ocr_text = rec.get("text", "")

        # è·³è¿‡é”™è¯¯ç»“æœ
        if not ocr_text or "error" in rec:
            continue

        # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼ code_idï¼›å¦åˆ™ä» image_path å›æ¨
        code_id = rec.get("code_id", "")
        img_path = rec.get("image_path", "")
        if not code_id and img_path:
            parts = img_path.replace("\\", "/").split("/")
            code_id = parts[-3] if len(parts) >= 3 else ""

        if not code_id:
            continue

        # æ¸…ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå…³é”®ï¼ï¼‰
        ocr_text = ocr_text.replace('<|begin_of_box|>', '').replace('<|end_of_box|>', '').strip()

        if rec.get("image_paths"):
            grouped[(code_id, ratio)].append({
                "text": ocr_text,
                "image_path": img_path,
                "num_pages": int(rec.get("num_pages") or len(rec.get("image_paths") or [])),
            })
        else:
            grouped[(code_id, ratio)].append({
                "text": ocr_text,
                "image_path": img_path,
            })

    # è¯„ä¼°ç»“æœ
    detail_path = os.path.join(output_dir, f"judge_results_detail_{model_tag}.jsonl")
    summary_path = os.path.join(output_dir, f"judge_summary_{model_tag}.json")

    # æŒ‰ ratio åˆ†ç»„ç»Ÿè®¡
    stats_by_ratio = {r: {
        "cer_sum": 0, "wer_sum": 0, "bleu_sum": 0, "codebleu_sum": 0,
        "exact_match_sum": 0,
        "codediff_line_sim_sum": 0.0,
        "codediff_change_rate_sum": 0.0,
        "codediff_added_sum": 0,
        "codediff_removed_sum": 0,
        "codediff_replaced_sum": 0,
        "codediff_hunks_sum": 0,
        "count": 0,
        "errors": Counter(), "taxonomy_sums": Counter()
    } for r in TARGET_RATIOS}
    # æ¸…ç©º detail æ–‡ä»¶
    open(detail_path, "w").close()

    total = len(grouped)
    evaluated = 0
    
    # ğŸŒŸ ç°åœ¨æŒ‰ (code_id, ratio) ç»„åˆè¿›è¡Œè¯„ä¼°
    for idx, ((code_id, ratio), pages) in enumerate(grouped.items(), 1):
        reference = code_map.get(code_id, "")
        if not reference:
            print(f"[{idx}/{total}] âš ï¸ No ground truth for {code_id}, skipping")
            continue

        # ğŸŒŸ åˆå¹¶å¤šé¡µ OCR ç»“æœï¼š
        # - single-turnï¼špages åªæœ‰ä¸€æ¡ï¼ˆæ•´æ®µæ–‡æœ¬ï¼‰ï¼Œä¸å†æ‹¼é¡µ
        # - legacyï¼šæŒ‰ image_path æ’åºåæ‹¼æ¥
        pages.sort(key=lambda x: x.get("image_path", ""))
        if len(pages) == 1 and ("num_pages" in pages[0]):
            merged_ocr = pages[0]["text"]
            num_pages = int(pages[0].get("num_pages") or 1)
        else:
            merged_ocr = '\n'.join([p["text"] for p in pages])
            num_pages = len(pages)

        evaluated += 1
        print(f"[{idx}/{total}] Evaluating: {code_id} @ ratio {ratio}x ({num_pages} pages)")

        # ğŸŒŸ è§„èŒƒåŒ–å¤„ç†ï¼ˆç”¨äº hard metricsï¼‰
        ref_normalized = normalize_code(reference)
        ocr_normalized = normalize_code(merged_ocr)

        # 1. Hard metricsï¼ˆä½¿ç”¨è§„èŒƒåŒ–åçš„æ–‡æœ¬ï¼‰
        cer = _compute_cer(ref_normalized, ocr_normalized)
        wer = _compute_wer(ref_normalized, ocr_normalized)
        bleu = _compute_token_bleu(ref_normalized, ocr_normalized)
        codebleu = _compute_codebleu(ref_normalized, ocr_normalized)
        exact_match = _compute_exact_match_rate(ref_normalized, ocr_normalized)

        # 2. Soft taxonomyï¼ˆä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼Œä¿ç•™ç¼©è¿›/ç¬¦å·/ç©ºè¡Œä¿¡æ¯ï¼‰
        taxonomy_labels = _detect_all_taxonomy_errors(reference, merged_ocr)
        detected_error_types = [k for k, v in taxonomy_labels.items() if v == 1]

        # è®°å½•è¯¦æƒ… 
        detail_rec = {
            "code_id": code_id,
            "ratio": ratio,
            "num_pages": num_pages,
            "cer": round(cer, 4),
            "wer": round(wer, 4),
            "token_bleu": round(bleu, 4),
            "codebleu": round(codebleu, 4),
            "exact_match_rate": round(exact_match, 4),
            "taxonomy_labels": taxonomy_labels,
            "detected_errors": detected_error_types,
        }

        codediff_no_blank = _compute_codediff_metrics_no_blank(reference, merged_ocr)
        detail_rec["codediff_no_blank"] = codediff_no_blank

        with open(detail_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(detail_rec, ensure_ascii=False) + "\n")

        # æ›´æ–°ç»Ÿè®¡
        if ratio in stats_by_ratio:
            stats_by_ratio[ratio]["cer_sum"] += cer
            stats_by_ratio[ratio]["wer_sum"] += wer
            stats_by_ratio[ratio]["bleu_sum"] += bleu
            stats_by_ratio[ratio]["codebleu_sum"] += codebleu
            stats_by_ratio[ratio]["exact_match_sum"] += exact_match
            stats_by_ratio[ratio]["codediff_line_sim_sum"] += float(codediff_no_blank.get("line_similarity", 0.0))
            stats_by_ratio[ratio]["codediff_change_rate_sum"] += float(codediff_no_blank.get("change_rate", 0.0))
            stats_by_ratio[ratio]["codediff_added_sum"] += int(codediff_no_blank.get("added_lines", 0))
            stats_by_ratio[ratio]["codediff_removed_sum"] += int(codediff_no_blank.get("removed_lines", 0))
            stats_by_ratio[ratio]["codediff_replaced_sum"] += int(codediff_no_blank.get("replaced_lines", 0))
            stats_by_ratio[ratio]["codediff_hunks_sum"] += int(codediff_no_blank.get("diff_hunks", 0))
            stats_by_ratio[ratio]["count"] += 1
            
            # è®°å½•è¯¥æ ·æœ¬ä¸­å‡ºç°çš„é”™è¯¯ï¼ˆtaxonomyï¼‰
            for err_type, val in taxonomy_labels.items():
                if val == 1:
                    stats_by_ratio[ratio]["errors"][err_type] += 1
                stats_by_ratio[ratio]["taxonomy_sums"][err_type] += val

    # ç”Ÿæˆæ±‡æ€»
    summary = {}
    for ratio, s in stats_by_ratio.items():
        if s["count"] == 0:
            continue
        # è®¡ç®—æ¯ç§é”™è¯¯ç±»å‹çš„æ£€å‡ºç‡
        error_rates = {et: round(cnt / s["count"], 4) 
                      for et, cnt in s["errors"].items()}

        # 8ç±»é”™è¯¯ -> 3å¤§ç±»èšåˆç»Ÿè®¡
        error_groups = {
            "Recognition": ["Visual_Typo", "Symbol_Loss", "Comment_Loss"],
            "Structure": ["Indentation_Error", "Line_Skipped", "Repetition"],
            "Hallucination": ["Variable_Hallucination", "Code_Invention"],
        }
        error_group_counts = {
            group: int(sum(s["errors"].get(k, 0) for k in members))
            for group, members in error_groups.items()
        }
        error_group_rates = {
            group: round(cnt / s["count"], 4)
            for group, cnt in error_group_counts.items()
        }
        summary[f"ratio_{ratio}x"] = {
            "count": s["count"],
            "avg_cer": round(s["cer_sum"] / s["count"], 4),
            "avg_wer": round(s["wer_sum"] / s["count"], 4),
            "avg_token_bleu": round(s["bleu_sum"] / s["count"], 4),
            "avg_codebleu": round(s["codebleu_sum"] / s["count"], 4),
            "avg_exact_match_rate": round(s["exact_match_sum"] / s["count"], 4),
            "avg_codediff_line_similarity_no_blank": round(s["codediff_line_sim_sum"] / s["count"], 4),
            "avg_codediff_change_rate_no_blank": round(s["codediff_change_rate_sum"] / s["count"], 4),
            "avg_codediff_added_lines_no_blank": round(s["codediff_added_sum"] / s["count"], 4),
            "avg_codediff_removed_lines_no_blank": round(s["codediff_removed_sum"] / s["count"], 4),
            "avg_codediff_replaced_lines_no_blank": round(s["codediff_replaced_sum"] / s["count"], 4),
            "avg_codediff_diff_hunks_no_blank": round(s["codediff_hunks_sum"] / s["count"], 4),
            "error_counts": dict(s["errors"]),  # åŸå§‹è®¡æ•°
            "error_rates": error_rates,  # æ£€å‡ºç‡
            "error_group_counts": error_group_counts,
            "error_group_rates": error_group_rates,
        }

    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Module 4 finished.")
    print(f"ğŸ“„ Detail: {os.path.abspath(detail_path)}")
    print(f"ğŸ“Š Summary: {os.path.abspath(summary_path)}")

    # æ‰“å°ç®€è¦æ±‡æ€»
    print("\nğŸ“ˆ Quick Summary:")
    for ratio_key, data in summary.items():
        print(f"   {ratio_key}:")
        print(f"      â”œâ”€ CER={data['avg_cer']:.2%}, WER={data['avg_wer']:.2%}")
        print(
            f"      â”œâ”€ CodeBLEU={data.get('avg_codebleu', 0.0):.4f}, "
            f"BLEU={data['avg_token_bleu']:.4f}, Exact Match={data['avg_exact_match_rate']:.2%}"
        )
        if "avg_codediff_line_similarity_no_blank" in data:
            print(
                "      â”œâ”€ CodeDiff(no-blank): "
                f"line_sim={data.get('avg_codediff_line_similarity_no_blank', 0.0):.4f}, "
                f"change_rate={data.get('avg_codediff_change_rate_no_blank', 0.0):.4f}, "
                f"hunks={data.get('avg_codediff_diff_hunks_no_blank', 0.0):.2f}, "
                f"replaced={data.get('avg_codediff_replaced_lines_no_blank', 0.0):.2f}"
            )
        if "error_group_counts" in data:
            egc = data.get("error_group_counts") or {}
            print(
                "      â”œâ”€ ErrorGroups(count): "
                f"Recognition={egc.get('Recognition', 0)}, "
                f"Structure={egc.get('Structure', 0)}, "
                f"Hallucination={egc.get('Hallucination', 0)}"
            )
        if "error_group_rates" in data:
            egr = data.get("error_group_rates") or {}
            print(
                "      â”œâ”€ ErrorGroups(rate): "
                f"Recognition={float(egr.get('Recognition', 0.0)):.2%}, "
                f"Structure={float(egr.get('Structure', 0.0)):.2%}, "
                f"Hallucination={float(egr.get('Hallucination', 0.0)):.2%}"
            )
        if data['error_counts']:
            err_str = ", ".join([f"{k}:{v}" for k, v in data['error_counts'].items()])
            print(f"      â””â”€ Errors: {err_str}")
        else:
            print(f"      â””â”€ Errors: (none detected)")


def apply_visual_corruption(image_path, ratio):
    """
    æ‰‹åŠ¨å®ç°è§†è§‰å¹²æ‰°å™¨ï¼šè¯»å–åŸå›¾ï¼Œå…ˆæŒ‰æ¯”ä¾‹ç¼©å°å†æ”¾å¤§å›åŸå°ºå¯¸ï¼ˆä¿æŒå°ºå¯¸ä¸€è‡´ï¼‰
    çº¦å®šï¼šæ— è®º ratio æ˜¯ 1/2/4/6/8ï¼Œéƒ½ç”Ÿæˆä¸€ä¸ªå¸¦ _ratio{ratio} åç¼€çš„æ–°æ–‡ä»¶ã€‚
    """
    try:
        with Image.open(image_path) as img:
            # æ„é€ æ–°æ–‡ä»¶å: page_001.png -> page_001_ratio1.png
            dir_name = os.path.dirname(image_path)
            base_name = os.path.basename(image_path)
            name_part, ext = os.path.splitext(base_name)
            new_filename = f"{name_part}_ratio{ratio}{ext}"
            new_path = os.path.join(dir_name, new_filename)
            
            if ratio == 1:
                # ratio=1: ç›´æ¥ä¿å­˜åŸå›¾ï¼ˆä¸å‹ç¼©ï¼‰ï¼Œä½†é‡å‘½åä¸º _ratio1
                img.save(new_path)
                return new_path
            
            # ratio>1: æ‰§è¡Œå‹ç¼©å¤„ç†
            original_w, original_h = img.size
            new_w = max(1, int(original_w / ratio))
            new_h = max(1, int(original_h / ratio))
            
            # æ‰§è¡Œå‹ç¼© (Downsampling) -> å† Upsampling å›åŸå°ºå¯¸
            small_img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)
            resized_img = small_img.resize((original_w, original_h), Image.Resampling.BILINEAR)
            resized_img.save(new_path)
            return new_path
            
    except Exception as e:
        print(f"   âš ï¸ Compression failed for ratio {ratio}: {e}")
        return None

def run_full_process():
    os.makedirs(OUTPUT_DIR, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

    # 0. å›¾ç‰‡è¾“å…¥æ¨¡å¼
    if USE_EXISTING_IMAGES:
        if RUN_MODULE_3 and (not os.path.exists(IMAGES_DIR)):
            print("âŒ USE_EXISTING_IMAGES=1 but images directory not found:")
            print(f"   - {os.path.abspath(IMAGES_DIR)}")
            print("   ä½ å½“å‰å¯ç”¨äº† RUN_MODULE_3ï¼ˆéœ€è¦å›¾ç‰‡åš OCRï¼‰ã€‚")
            print("   è¯·è®¾ç½® $env:EXISTING_IMAGES_DIR=\"...\" æŒ‡å‘å·²æœ‰å›¾ç‰‡ç›®å½•ï¼Œæˆ–å…ˆè·‘ä¸€æ¬¡æ¨¡å—1/2ç”Ÿæˆå›¾ç‰‡ã€‚")
            return
        print("ğŸ§© Using existing images (skip Module 1 & 2)")
        print(f"   - images_dir: {os.path.abspath(IMAGES_DIR)}")
        print(f"   - dataset: {os.path.abspath(os.path.join(OUTPUT_DIR, DATASET_FILENAME))}")
    else:
        # 0. æ¸…ç†ç¯å¢ƒï¼ˆåªåˆ é™¤å½“å‰æ¨¡å‹çš„ images ç›®å½•ï¼‰
        if os.path.exists(IMAGES_DIR):
            try:
                shutil.rmtree(IMAGES_DIR)
                print(f"ğŸ§¹ Cleaned up: {IMAGES_DIR}")
            except Exception as e:
                print(f"âš ï¸ Failed to clean {IMAGES_DIR}: {e}")

    # ğŸ§¹ æ¸…ç†å½“å‰æ¨¡å‹ä¸Šæ¬¡è¿è¡Œæ®‹ç•™çš„è¾“å‡ºæ–‡ä»¶ï¼ˆé¿å… done-set è·³è¿‡ + è¯„ä¼°ç»“æœæ··æ·†ï¼‰
    gemini_ocr_jsonl = os.path.join(OUTPUT_DIR, "gemini_ocr.jsonl")
    gemini_model_tag = _safe_filename_component(GEMINI_MODEL_NAME)
    gemini_dataset_json = os.path.join(OUTPUT_DIR, DEFAULT_DATASET_FILENAME)
    legacy_dataset_json = os.path.join(OUTPUT_DIR, "dataset.json")
    gemini_judge_detail = os.path.join(OUTPUT_DIR, f"judge_results_detail_{gemini_model_tag}.jsonl")
    gemini_judge_summary = os.path.join(OUTPUT_DIR, f"judge_summary_{gemini_model_tag}.json")
    removed = []
    # ä½¿ç”¨å·²æœ‰å›¾ç‰‡é›†æ—¶ï¼šä¸è¦åˆ é™¤ datasetï¼ˆå¦åˆ™ judge æ²¡æœ‰ GTï¼‰ã€‚
    # èµ°å…¨æµç¨‹æ—¶ï¼šä¼šé‡å»º datasetï¼Œå› æ­¤å¯å®‰å…¨æ¸…ç†æ‰æ—§çš„ dataset åŠ legacy dataset.jsonã€‚
    to_remove = [gemini_judge_detail, gemini_judge_summary]
    # åªæœ‰åœ¨è¦é‡æ–°è·‘ OCR æ—¶æ‰åˆ é™¤ ocr.jsonlï¼›åªè·‘ Module 4 æ—¶ä¿ç•™ç°æœ‰ OCR ç»“æœã€‚
    if RUN_MODULE_3:
        to_remove.insert(0, gemini_ocr_jsonl)
    if not USE_EXISTING_IMAGES:
        to_remove.extend([gemini_dataset_json, legacy_dataset_json])

    for p in to_remove:
        if _remove_file_if_exists(p):
            removed.append(os.path.basename(p))
    if removed:
        print("ğŸ§¹ Removed model artifacts: " + ", ".join(removed))

    if not USE_EXISTING_IMAGES:
        os.makedirs(IMAGES_DIR, exist_ok=True)

    dataset_filename = DATASET_FILENAME
    dataset = None
    if not USE_EXISTING_IMAGES:
        # -------------------------------------------------
        # ğŸŸ¢ æ¨¡å—ä¸€: æ•°æ®æŒ–æ˜ (Data Miner)
        # -------------------------------------------------
        print("\n" + "="*40)
        print("ğŸš€ Running Module 1: Data Miner")
        print("="*40)

        try:
            from data_miner import fetch_fresh_code
        except Exception as e:
            print("âŒ Failed to import data_miner.fetch_fresh_code.")
            print(f"   - error: {e}")
            print("   ä½ å½“å‰å¦‚æœåªæ˜¯æƒ³ç”¨å·²æœ‰å›¾ç‰‡é›†è¯„æµ‹ï¼Œè¯·è®¾ç½® USE_EXISTING_IMAGES=1ã€‚")
            print("   å¦‚æœè¦èµ°å…¨æµç¨‹ï¼ˆæ‹‰å– GitHub ä»£ç ï¼‰ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–ï¼špip install PyGithub")
            return
        
        dataset = fetch_fresh_code()
        
        if not dataset:
            print("âŒ No data found.")
            return

        dataset_path = os.path.join(OUTPUT_DIR, dataset_filename)
        with open(dataset_path, "w", encoding="utf-8") as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)

        # -------------------------------------------------
        # ğŸ”µ æ¨¡å—äºŒ: è§†è§‰å¹²æ‰°å™¨ (Visual Corruptor)
        # -------------------------------------------------
        print("\n" + "="*40)
        print("ğŸš€ Running Module 2: Visual Corruptor")
        print(f"ğŸ¯ Target Ratios: {TARGET_RATIOS}")
        print("="*40)

        total_images_generated = 0
        
        for idx, item in enumerate(dataset):
            code_id = item['id']
            source_code = item['code']
            
            print(f"[{idx+1}/{len(dataset)}] Processing: {code_id} ...")
            
            item_output_dir = os.path.join(IMAGES_DIR, code_id)
            os.makedirs(item_output_dir, exist_ok=True)
            
            temp_file_path = os.path.join(item_output_dir, "temp_source.py")
            with open(temp_file_path, "w", encoding="utf-8") as f:
                f.write(source_code)
                
            try:
                # 1. ç”ŸæˆåŸºå‡†é«˜æ¸…å›¾ (1x)
                generated_paths = text_to_image_compact.generate_images_for_file(
                    filename=temp_file_path,
                    source_code=source_code,
                    base_output_dir=item_output_dir,
                    width=1024,
                    height=1024,  # æ­£æ–¹å½¢
                    font_size=18,  # ç¨å¾®å°ä¸€ç‚¹é€‚åº”æ­£æ–¹å½¢
                    line_height=1.2,
                    dpi=100,
                    preserve_newlines=True,
                    enable_syntax_highlight=True,
                    unique_id="base"
                )
                
                if not generated_paths:
                    print("   âŒ No base image generated.")
                    continue

                # 2. æ‰§è¡Œè§†è§‰å‹ç¼©å¾ªç¯ (1x, 2x, 4x, 8x)
                for original_path in generated_paths:
                    for ratio in TARGET_RATIOS:
                        new_path = apply_visual_corruption(original_path, ratio)
                        if new_path:
                            total_images_generated += 1
                    
                    # ğŸ—‘ï¸ åˆ é™¤åŸå§‹å›¾ç‰‡ï¼ˆå·²ç”Ÿæˆæ‰€æœ‰ ratio ç‰ˆæœ¬ï¼‰
                    try:
                        if os.path.exists(original_path):
                            os.remove(original_path)
                    except Exception as e:
                        print(f"      âš ï¸ Failed to remove original: {e}")

            except Exception as e:
                print(f"   âŒ Error processing {code_id}: {e}")
                import traceback
                traceback.print_exc()
            finally:
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)

        print("\n" + "="*40)
        print("ğŸ‰ Pipeline Stage 1 & 2 Completed!")
        print(f"ğŸ“Š Summary:")
        print(f"   - Data Mined: {len(dataset)}")
        print(f"   - Total Variants Generated: {total_images_generated}")
        print(f"   - Output Location: {os.path.abspath(OUTPUT_DIR)}")
        print("="*40)

    # -------------------------------------------------
    # ğŸŸ£ æ¨¡å—ä¸‰: æ¨ç†å¼•æ“ (Inference Engine)
    # -------------------------------------------------
    if RUN_MODULE_3:
        run_module_3_gemini(IMAGES_DIR, OUTPUT_DIR)

    # -------------------------------------------------
    # ğŸŸ  æ¨¡å—å››: è‡ªåŠ¨è¯„ä¼°å™¨ (Auto-Judge)
    # -------------------------------------------------
    if RUN_MODULE_4:
        run_module_4_judge(OUTPUT_DIR, "gemini_ocr.jsonl", GEMINI_MODEL_NAME, dataset_filename)

if __name__ == "__main__":
    run_full_process()
