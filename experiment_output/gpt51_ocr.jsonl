{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\n\nfrom .logger import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = No\nne):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(\"running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(1, width - 1)\n            y = self.rng.randint(1, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")"}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"Tools for generating spatial coordinates for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Dict, Any, List\n\nfrom .logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial coordinates for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profile file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.output_path = output_path\n        self.seed = seed if self.seed_config[\"seed\", seed]\n        self.rng = random.Random(seed)\n        logger.info(\"Initializing SpaceGenerator.\")\n\n    def _load_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n            for idx, item in enumerate(data):\n                agent = item\n                agent[\"id\"] = idx\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Assigns random positions in the map to agents.\"\"\"\n        agents = self._load_agents()\n        width, height = self.space_config\n\n        for agent in agents:\n            x = self.rng.randint(0, width - 1)\n            y = self.rng.randint(0, height - 1)\n            agent[\"position\"] = (x, y)\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            agents_info = [\n                {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                for agent in agents\n            ]\n            json.dump(agents_info, f, ensure_ascii=False, indent=4)\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")"}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom .logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(\"Running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(0, width - 1)\n            y = self.rng.randint(0, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = [\n                    {\n                        \"id\": agent[\"id\"],\n                        \"name\": agent[\"name\"],\n                        \"position\": agent[\"position\"],\n                    }\n                ]\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")"}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\n\nfrom ..logger import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(\"running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(1, width - 1)\n            y = self.rng.randint(1, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")"}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom TRM: Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\n\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self._stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self._stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n        k = min(5, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self._stable_count += 1\n            if self._stable_count >= self.patience:\n                return True\n        else:\n            self._stable_count = 0\n\n        return False"}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\n   From TRM: Q-learning inspired halting - stop when imrpovement is minimal.\n\"\"\"\n\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\n\nclass ConfidenceEstimator:\n    \"\"\" \n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stoping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self._stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self._stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n        k = min(5, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self._stable_count += 1\n            if self._stable_count >= self.patience:\n                return True\n        else:\n            self._stable_count = 0\n\n        return False"}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\" \nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom TDS-Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\n\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.0):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self._stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self._stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n\n        k = min(len(prev_order), len(curr_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        if curr_scores.mean() - prev_scores.mean() < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self._stable_count += 1\n            if self._stable_count >= self.patience:\n                return True\n        else:\n            self._stable_count = 0\n\n        return False"}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Tools for generating agent arguments for agents.\"\"\"\n\nimport os\nimport json\nimport shutil\nfrom typing import Dict, Any, List\n\nTMP_AGENT_DATA_DIR = \"tmp\"\n\nlogger = get_logger(__name__)\n\n\nclass AgentMetadata:\n    \"\"\"Class for generating agent arguments for agents.\"\"\"\n\n    def __init__(self, agent_config_path: str, agent_data: Dict[str, Any], output_path: str) -> None:\n        \"\"\"\n        Initializes the AgentMetadata.\n\n        Args:\n            agent_config_path (str): The path to the agent config file.\n            agent_data (Dict[str, Any]): The agent configuration.\n            output_path (str): The path to the output file for agent specs info\n                and credentials. The output path.\n\n        \"\"\"\n        self.agent_config_path = agent_config_path\n        self.agent_data = agent_data\n        self.output_path = output_path\n        self.credentials = self.agent_data.get(\"credentials\", {})\n        self.agent_name = agent_data[\"agent_name\"]\n        self.tmp_agent_data_dir = TMP_AGENT_DATA_DIR\n\n    def generate_agent_args(self) -> Dict[str, Any]:\n        \"\"\"\n        Load and check the agent data from the path to path.\n\n        Returns:\n            Dict[str, Any]: The dict of agent data.\n\n        \"\"\"\n        agent = self.agent_data.copy()\n        agent_name = agent[\"agent_name\"]\n        agent[\"output_path\"] = os.path.join(self.output_path, agent_name + \".tsv\")\n        agent[\"agent_id\"] = agent[\"agent_name\"]\n        agent[\"queue_name\"] = self.agent_data.get(\"queue_name\", \"default\")\n        self.credentials[agent_name] = {}\n        for key in self.credentials.keys():\n            self.credentials[agent_name][key] = agent.get(key)\n        agent[\"credentials_path\"] = self._dump_credentials(agent_name)\n        agent[\"input_file_path\"] = self.agent_config_path\n\n        agent = self._update_agent_with_tmp_path(agent)\n\n        logger.info(\"Generated agent arguments for agent %s\", agent_name)\n\n        return agent\n\n    def _update_agent_with_tmp_path(self, agent: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Update the given agent with tmp data dir path.\n\n        Args:\n            agent (Dict[str, Any]): The dict of agent data.\n\n        Returns:\n            Dict[str, Any]: The updated agent dict.\n\n        \"\"\"\n        agent[\"tmp_dir_path\"] = os.path.join(self.tmp_agent_data_dir, agent[\"agent_name\"])\n        return agent\n\n    def _dump_credentials(self, agent_name: str) -> str:\n        \"\"\"\n        Dump credentials to a json file.\n\n        Args:\n            agent_name (str): The agent name.\n\n        Returns:\n            str: The path to the json file.\n\n        \"\"\"\n        os.makedirs(self.tmp_agent_data_dir, exist_ok=True)\n        credentials_path = os.path.join(self.tmp_agent_data_dir, f\"{agent_name}_credentials.json\")\n        with open(credentials_path, \"w\") as f:\n            json.dump(self.credentials, f)\n        return credentials_path\n\n\ndef copy_agent_data_dir(agent_args: List[Dict[str, Any]], output_path: str) -> str:\n    \"\"\"\n    Copies agent data dir for the list of agents to path.\n\n    Args:\n        agent_args (List[Dict[str, Any]]): The list of agent data.\n\n    Returns:\n        str: The output_path.\n\n    \"\"\"\n    tmp_dir = os.path.join(output_path, TMP_AGENT_DATA_DIR)\n    if os.path.exists(tmp_dir):\n        shutil.rmtree(tmp_dir)\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    for agent in agent_args:\n        tmp_agent_dir = os.path.join(tmp_dir, agent[\"agent_name\"])\n        os.makedirs(tmp_agent_dir, exist_ok=True)\n        agent[\"tmp_dir_path\"] = tmp_agent_dir\n        for key, value in agent.items():\n            if key.endswith(\"_path\") and os.path.exists(value):\n                shutil.copy2(value, tmp_agent_dir)\n\n    logger.info(\"Copied agent data directory to path %s\", output_path)\n\n    return output_path"}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"                                                                     \nRefineAddressState - decreases softmax to enable early stopping.       \n\nFrom TAO @baimingze/misty - stop when top-2 agreement is small.        \n\nMimic rules so far.                                                    \n\nfrom scripts.retrieval.retrieval_state import RefineAddressState       \n\n\nclass RefineAddressState:                                              \n                                                                        \n    \"\"\"Decrease softmax to enable early stopping.                       \n                                                                        \n    Save previous to avoid stopping on every step(w/o improvements).   \n    \"\"\"                                                                \n                                                                        \n    def __init__(self, address: int = 1, min_agreement: float = 0.95): \n        self.address = address                                         \n        self.min_agreement = min_agreement                             \n        self._streak_count = 0                                         \n                                                                        \n    def reset(self):                                                   \n        \"\"\"Reset state for a new query.\"\"\"                             \n        self._streak_count = 0                                         \n                                                                        \n    def should_reject_state(self, refine_address: int) -> bool:        \n        \"\"\"Check if we should stop refining based on score stability.  \n                                                                        \n        Returns early (binary).                                        \n        \"\"\"                                                             \n        state_scores = state.scores.tolist()                            \n        prev_scores = state.scores                                     \n                                                                        \n        prev_index = np.argmax(prev_scores)                             \n        probs = state.probs[prev_index]                                \n                                                                        \n        is_stable = False                                              \n        # Softmax stability:                                           \n        # We compare the difference between top-2 scores               \n        # to the min_agreement threshold.                              \n        if len(probs) > 1:                                             \n            sorted_idx = np.argsort(probs)[::-1]                        \n            top1, top2 = probs[sorted_idx[:2]]                          \n            # Agreement between top positions                          \n            agreement = top1 / (top1 + top2 + 1e-8)                     \n                                                                        \n            if agreement < self.min_agreement:                          \n                self._streak_count = 0                                 \n            else:                                                       \n                self._streak_count += 1                                \n                if self._streak_count >= self.address:                 \n                    return True                                         \n        else:                                                           \n            self._streak_count += 1                                    \n                                                                        \n        return False"}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "####################################################################################################\n# DataNormalization: Normalize and balance the dataset for ML training\n#\n# Task 3/4: Performing required testing - Implement experiment 1 on dataset\n####################################################################################################\n\nimport random as rnd\n\nfrom projects.binary_classifier.model import MLPClassification\n\nclass DataNormalization:\n\n    \"\"\"Normalize and balance the dataset for ML training.\n\n    This function is useful evaluate the various requirements characteristics\n    \"\"\"\n\n    def __init__(self, tolerance: int = 2, num_experiments: float = 2.0):\n        self.tolerance = tolerance\n        self.num_experiments = num_experiments\n        self.num_experiments = num_experiments\n        self.results_count = {}\n\n    def experiment1():\n        \"\"\"\" Select events for a new sample \"\"\n        self.results_count = {}\n\n    def making_experiment(self, model: MLPClassification) -> bool:\n        \"\"\" Perform a ML model initial analysis based on events number for \n            the following events/fetures \"\"\"\n        ok = False\n        events_list = model.events_target_varieties()\n\n        events = model.events_target_varieties()\n        for event in events:\n            events_records = model.events_dataset(event)\n            not_detected = model.base_dataset[~model.base_dataset.isin(event_records)]\n            if len(event_records) > len(not_detected):\n                self.results_count[event] = len(not_detected)\n            else:\n                if len(event_records) < self.tolderance:\n                    self.results_count = 0\n                else:\n                    self.results_count[event] = len(event_records)\n\n        total_events = list(self.results_count.keys())\n        max_events = max(list(self.results_count.values()))\n\n        sample_events = rnd.randit(0, len(total_events), size=num_events)\n        correct_events = []\n        for event_index in sample_events:\n            number_of_events = self.results_count[total_events[event_index]]\n            if number_of_events <= max_events:\n                correct_events.append(number_of_events)\n            else:\n                correct_events.append(max_events - 1)\n        sample_events = [total_events[index] for index in set(correct_events)]\n\n        ok = True if len(sample_events) > self.tolerance else False\n\n        if ok:\n            self.results_count = self._get_values\n            return True\n        else:\n            self.results_count = self.tolerance\n\n        return False"}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "import logging\nfrom typing import Dict, Optional, Any\n\n\nlogger = logging.getLogger(__name__)\n\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cost:.5f})\"\n            msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]"}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cos\nt:.5f})\"\n\n        msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tok\nens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]"}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, \"CostTracker\"] = {}\n\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g. MxL generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.llm_tokens = 0\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"MxL generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM ${self.llm_cost:.5f}, Embedding ${self.embedding_cost:.5f})\" \\\n                  f\" | Total tokens: {total_tokens} (LLM {self.llm_tokens}, Embedding {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tokens}, \"\n                    f\"LLM={self.llm_tokens}, Embedding={self.embedding_tokens}\")\n\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]"}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, \"CostTracker\"] = {}\n\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g. RAG generation)\n    Separates LLM and embedding costs.\n    \"\"\"\n\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.llm_tokens_in = 0\n        self.llm_tokens_out = 0\n        self.embedding_tokens = 0\n        self.llm_cost_usd = 0.0\n        self.embedding_cost_usd = 0.0\n\n    def add_embedding_tokens(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost_usd += cost\n\n    def add_llm_request(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens_in += prompt_tokens\n        self.llm_tokens_out += completion_tokens\n        self.llm_cost_usd += cost\n\n    def get_total_cost_usd(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.llm_cost_usd + self.embedding_cost_usd\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_usd = self.get_total_cost_usd()\n        total_tokens = self.llm_tokens_in + self.llm_tokens_out + self.embedding_tokens\n\n        msg = \"RAG generation completed successfully!\"\n\n        if total_tokens > 0:\n            msg += f\" Total tokens: {total_tokens}, LLM (in/out): {self.llm_tokens_in}/{self.llm_tokens_out}, Embedding: {self.embedding_tokens}.\"\n            msg += f\" Total token-based cost: ${total_usd:.6f} USD (LLM: ${self.llm_cost_usd:.6f}, Embedding: ${self.embedding_cost_usd:.6f})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens_in + self.llm_tokens_out + self.embedding_tokens\n        logger.info(\n            f\"[CostTracker] Task {self.task_id} - Total tokens: {total_tokens}, \"\n            f\"LLM (in/out): {self.llm_tokens_in}/{self.llm_tokens_out}, Embedding: {self.embedding_tokens}, \"\n            f\"Total cost: ${self.get_total_cost_usd():.6f} USD (LLM: ${self.llm_cost_usd:.6f}, Embedding: ${self.embedding_cost_usd:.6f})\"\n        )\n\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]"}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "import logging\nfrom typing import Any, Dict, Optional, Set\n\nlogger = logging.getLogger(__name__)\n\n# Number of tokens to show per chunk in errors\nMAX_ERROR_TOKENS_PREVIEW = 5\n\n\nclass Embeddings:\n\n    \"\"\"Base class for a specific task (e.g., RAG generation)\n    based on LLM embeddings index.\n\n    \"\"\"\n\n    def __init__(self, task: str) -> None:\n        self._task_name = task\n        self._embeddings_index = {}\n        self._indexing_error = None\n        self._lock = False\n\n    def has_indexing_error(self) -> bool:\n        \"\"\"Has indexing step met error.\"\"\"\n        return self._indexing_error is not None\n\n    def indexing_error(self) -> Optional[BaseException]:\n        \"\"\"The last error met.\"\"\"\n        return self._indexing_error\n\n    def is_locked(self) -> bool:\n        \"\"\"Has this task been locked.\"\"\"\n        return self._lock\n\n    def set_lock(self) -> None:\n        \"\"\"Lock this task.\"\"\"\n        self._lock = True\n\n    def unset_lock(self) -> None:\n        self._lock = False\n\n    def set_indexed_chunks(self, chunks: Dict[int, Any]) -> None:\n        \"\"\"Set the chunk indexed from the user\n\n        This method is only called by the system package\n        when a LLM backend returns the index.\n\n        \"\"\"\n        if not self._lock:\n            logger.error(\"Can't set index for unlocked task\")\n        self._embeddings_index = chunks\n\n    def get_indexed_chunks(self) -> Dict[int, Any]:\n        \"\"\"Get the current indexed chunks\"\"\"\n        if not self._lock:\n            logger.error(\"Can't get index on unlocked task\")\n        return self._embeddings_index\n\n    def get_indexing_error(self) -> str:\n        \"\"\"Get a summary of the error.\"\"\"\n\n        # There was an error indexing the text\n        error_msg = \"Error indexing the text:\"\n        if self._indexing_error is not None:\n            text_preview = str(self._indexing_error)\n            tokens = text_preview[:MAX_ERROR_TOKENS_PREVIEW]\n            error_msg = (\n                f\"{error_msg} Error raised: {type(self._indexing_error)} \"\n                f\"from text preview: {tokens}\"\n            )\n            return error_msg\n        return error_msg\n\n    def summarize(self) -> str:\n        \"\"\"Get a summary of the index.\"\"\"\n        total_tokens = sum(len(chunk) for chunk in self._embeddings_index)\n        return (\n            f\"Total tokens: {total_tokens} Total chunks: \"\n            f\"{len(self._embeddings_index)}\"\n        )\n\n    def get_embeddings(self) -> Dict[int, Any]:\n        \"\"\"Get a summary of the index.\"\"\"\n        return self._embeddings_index\n\n    def get_safe_chunks_ids(self) -> Set[int]:\n        \"\"\"Get a chunks id not failed to load\n\n        \"\"\"\n        # This is a pure example\n        not_safe_chunks_id = set()\n        for index, chunk in self._embeddings_index:\n            if \"error\" in chunk.metadata:\n                not_safe_chunks_id.add(index)\n        return not_safe_chunks_id"}
{"code_id": "HGM_tree.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgm_utils\nfrom hgm_utils import eval_agent, sample_child\n\n\ndef get_num_total_evals():\n    return hgm_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None:  #\n            self.id = len(hgm_utils.nodes)\n        else:\n            self.id = id\n        hgm_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_decendant_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_decendant_evals(self, num_pseudo=10):\n        decendant_evals = self.get_pseudo_decendant_evals(num_pseudo)\n        for decendant in self.get_sub_tree()[1:]:\n            decendant_evals += decendant.utility_measures\n\n        return decendant_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return np.inf\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }"}
{"code_id": "HGM_tree.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgm_utils\nfrom hgm_utils import eval_agent, sample_child\n\ndef get_num_total_evals():\n\treturn hgm_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\nclass Node:\n\tdef __init__(\n\t\tself,\n\t\tcommit_id,\n\t\tutility_measures=None,\n\t\tparent_id=None,\n\t\tid=None,\n\t):\n\t\tself.commit_id = commit_id\n\t\tself.children = []\n\t\tif utility_measures:\n\t\t\tself.utility_measures = utility_measures\n\t\telse:\n\t\t\tself.utility_measures = []\n\t\tself.parent_id = parent_id\n\t\tif id is None:\n\t\t\tself.id = len(hgm_utils.nodes)\n\t\telse:\n\t\t\tself.id = id\n\t\thgm_utils.nodes[self.id] = self\n\n\tdef get_sub_tree(self, fn=lambda self: self):\n\t\tif len(self.children) == 0:\n\t\t\treturn [fn(self)]\n\t\telse:\n\t\t\tnodes_list = [fn(self)]\n\t\t\tfor child in self.children:\n\t\t\t\tnodes_list.extend(child.get_sub_tree(fn))\n\t\t\treturn nodes_list\n\n\tdef get_pseudo_decendant_evals(self, num_pseudo):\n\t\treturn self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n\tdef get_decendant_evals(self, num_pseudo=10):\n\t\tdecendant_evals = self.get_pseudo_decendant_evals(num_pseudo)\n\t\tfor decendant in self.get_sub_tree()[1:]:\n\t\t\tdecendant_evals += decendant.utility_measures\n\n\t\treturn decendant_evals\n\n\t@property\n\tdef num_evals(self):\n\t\treturn len(self.utility_measures)\n\n\t@property\n\tdef mean_utility(self):\n\t\tif self.num_evals == 0:\n\t\t\treturn np.inf\n\t\treturn np.sum(self.utility_measures) / self.num_evals\n\n\tdef add_child(self, child):\n\t\tself.children.append(child)\n\n\tdef save_as_dict(self):\n\t\treturn {\n\t\t\t\"commit_id\": self.commit_id,\n\t\t\t\"id\": self.id,\n\t\t\t\"parent_id\": self.parent_id,\n\t\t\t\"mean_utility\": self.mean_utility,\n\t\t\t\"num_evals\": self.num_evals,\n\t\t}"}
{"code_id": "HGM_tree.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgn_utils\nfrom hgn_utils import eval_agent, sample_child\n\ndef get_num_total_evals():\n    return hgn_utils.nodes[0].get_sub_tree(lambda node: node.num_evals)\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None:\n            self.id = len(hgn_utils.nodes)\n        else:\n            self.id = id\n        hgn_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_descendent_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_descendent_evals(self, num_pseudo=0):\n        descendent_evals = self.get_pseudo_descendent_evals(num_pseudo)\n        for descendent in self.get_sub_tree()[1:]:\n            descendent_evals += descendent.utility_measures\n\n        return descendent_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return 0\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }"}
{"code_id": "HGM_tree.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "import os\nimport pickle\nfrom statistics import median\n\nimport numpy as np\n\nimport sqlite3 as sq\n\nimport log_utils\nfrom log_utils import logerr and_logger, setup_logs\n\ndef get_log_level_args():\n    return log_utils.setup().get_cfg()['node node run_args']\n\nclass Node:\n    def __init__(\n        self,\n        node_id,\n        parent_id,\n        all_cliq_measures=None,\n        num_child_procs,\n        cliques\n    ):\n        self.node_id = node_id\n        self.parent_id = parent_id\n        if parent_id is None: #\n            self.cliq_measures = all_cliq_measures\n        else:\n            self.cliq_measures = []\n        self.parent_id = parent_id\n        if id in cliques:\n            self.clique = cliques[node_id]\n        else:\n            self.clique = []\n        logs_utils.node_logger_id = node_id\n\n    def get_sub_tree(self, include_self, cur_root):\n        \"\"\"\n        Return subtree of self\n        \"\"\"\n        nodes = [cur_root]\n\n        nodes_to_explore = [cur_root]\n        cur_root_gen_ind = get_sub_tree(self.nodes)\n        while nodes_to_explore:\n            cur_node.gen_id = []\n            for child in self.children:\n                nodes_to_explore.append(child)\n                nodes.append(child)\n\n            cur_node = nodes_to_explore.pop(0)\n\n        return nodes\n\n    def get_node_descendent_cliq_measures(self, max_period):\n        if cliq_measures is self.cliq_measures:\n            return self.cliq_measures\n        max_period = max(max_period or self.max_period, max_period)\n\n        for descendent in self.get_subtree():\n            for desc in descendent:\n                if desc.cliq_measures is None or len(desc.cliq_measures) < max_period:\n                    desc.cliq_measures = [median(desc.cliq_measure) for _ in xrange(max_period)]\n\n        return descendent.cliq_measures\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_descendent_scores(self, max_period):\n        descendent_scores = self.get_node_descendent_cliq_measures(max_period)\n        for descendent in self.get_subtree():\n            self.descendent = self.cliq_measures\n            descendent_scores = descendent.cliq_measures\n\n        return descendent_scores\n\n    @property\n    def num_cliques(self):\n        return len(self.cliq_measures)\n\n    @property\n    def max_period(self):\n        if self.sub_tree is None:\n            return 0\n        return np.max([len(cliq_measures) for cliq_measures in self.sub_tree])\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def as_dict(self):\n        return {\n            \"node_id\": self.node_id,\n            \"cli\": self.clique,\n            \"parent_id\": self.parent_id,\n            \"num_cliques\": self.num_cliques,\n            \"max_period\": self.max_period,\n        }"}
{"code_id": "HGM_tree.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "import os\nimport shutil\nfrom datetime import datetime\n\nROOT_VIDEO_DIR = \"\"\n\nLABEL_MAP = {\n    \"gop\": \"GOP\",\n}\nLABEL_MAP_REV = {v: k for k, v in LABEL_MAP.items()}\n\ndef get_video_date(path):\n    return datetime.fromtimestamp(os.path.getmtime(path)).date().isoformat()\n\nclass Node:\n    def __init__(\n        self,\n        name,\n        label = None,\n        children=None,\n        parent=None,\n        params=None,\n    ):\n        self.name = name\n        self.params = params or {}\n        self.children = children or []\n        self.parent = parent\n\n        self.path = os.path.join(\n            parent.path if parent else ROOT_VIDEO_DIR,\n            name,\n        )\n\n        if \"video_date\" not in self.params:\n            self.params[\"video_date\"] = get_video_date(self.path)\n\n    def get_children(self, include_self=True):\n        if include_self:\n            yield self\n        for child in self.children:\n            yield from child.get_children(include_self=False)\n\n    def get_video_description(self, long=False):\n        parts = [self.name, *(c.name for c in self.children)]\n        if len(parts) > 1:\n            parts[-1] = f\"& {parts[-1]}\"\n        video_list = \", \".join(parts) if long else self.name\n        return f\"{video_list} ({self.params['video_date']})\"\n\n    def get_youtube_metadata(self, args):\n        prefix = args.prefix\n        suffix = args.suffix or \"\"\n        title = args.title or self.get_video_description()\n        if prefix:\n            title = f\"{prefix} - {title}\"\n        if suffix:\n            title = f\"{title} - {suffix}\"\n        if self.params.get(\"series\"):\n            hashtags = self.params[\"series\"].split(\"-\") + [\"vlog\"]\n        else:\n            hashtags = [\"vlog\"]\n        description = (\n            self.params.get(\"description\", \"\") + \"\\n\\n\"\n            \" \".join(f\"#{h}\" for h in hashtags)\n        )\n        labels = [self.params.get(\"label\") or LABEL_MAP_REV[\"GOP\"]]\n        return LocalsDict(\n            {\n                \"title\": title,\n                \"description\": description,\n                \"labels\": labels,\n                \"privacyStatus\": \"unlisted\",\n                \"categoryId\": 22,\n            }\n        )\n\n    def move_completed_videos(self, dry_run=False):\n        source_dir = self.path\n        target_dir = os.path.join(ROOT_VIDEO_DIR, \"completed\", self.name)\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n        for filename in os.listdir(source_dir):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n            if dry_run:\n                print(f\"Would move {source_path} to {target_path}\")\n            else:\n                shutil.move(source_path, target_path)\n\nclass LocalsDict(dict):\n    pass\n\ndef main():\n    root = Node(\"gop\")\n    root.move_completed_videos()\n\nif __name__ == \"__main__\":\n    main()"}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# --------------------------------------------------------\n# Heads for downstream tasks\n# --------------------------------------------------------\n\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = [8, 3, 12, 4, 24, 8][croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                croconet.enc_embed_dim\n                if hook < croconet.enc_depth\n                else croconet.dec_embed_dim\n            )\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out"}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# ====================================================================\n# Heads for downstream tasks\n# ====================================================================\n\n\n'''\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n'''\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = (8 - 3, 12, 4, 24: 8)[croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            croconet.enc_embed_dim\n            if hook < croconet.enc_depth\n            else croconet.dec_embed_dim\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out"}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# Copyright (c) 2022-present Recur Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# ------------------------------------------------------------------------\n# Heads for downstream tasks\n# ------------------------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(crossent) takes a CrossEnt and set all layers according to the head and crossent attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys \"width\" and \"height\"\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskH5NdPT(nn.Module):\n    \"\"\"DPT module for CrdDt.\n    by default, hooks_idn will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder-decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        hooks_idn=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskH5NdPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idn = hooks_idn\n        self.layer_dims = layer_dims\n\n    def setup(self, crossent):\n        dpt_args = dict(\n            output_width_ratio=self.output_width_ratio,\n            num_channels=self.num_channels,\n        )\n\n        if self.hooks_idn is None:\n            if hasattr(crossent, \"dec_blocks\"):  # encoder + decoder\n                step = (8 - 1) // 4  # (crossent.dec_depth)\n                hooks_idn = [\n                    crossent.enc_depth + crossent.dec_depth - 1 - i * step\n                    for i in range(2, -1, -1)\n                ]\n            else:  # encoder only\n                step = crossent.enc_depth // 4\n                hooks_idn = [\n                    crossent.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idn = hooks_idn\n            print(\n                f\"PixelwiseTaskH5NdPT: automatically setting hook_idn={self.hooks_idn}\"\n            )\n\n        dpt_args[\"hooks\"] = self.hooks_idn\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            crossent.enc_embed_dim\n            if hook < crossent.enc_depth\n            else crossent.dec_embed_dim\n            for hook in self.hooks_idn\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, img_prior=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out"}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# Copyright (c) 2023-present NAVER Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only)\n\n\n# Hook for SwinUnet hooks\n# ----------------------------------------------------------------------\n\n\n\n\"\"\"\nA hook is a module where the __init__ defines only the hook hyperparameters\nA method output_channel takes a list/tuple and an int: layers according to the hook and channel attribution.\nThe forward takes the features as well as a dictionary img_info enclosing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom abc import ABCMeta\nfrom ..utils import PSPUpsampler\n\n\nclass FeatureHookWithGPT(nn.Module):\n    \"\"\"\n    GPT module for hooks\n    By default, hooks_attr is to be equal to:\n    * for encoder-only: 4 equally spaced layers\n    * for encoder-decoder: last encoder + 3 equally spaced layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        out,\n        hooks_attr=None,\n        num_classes=20,  # COCO, VOC, City, IDD, ACDC\n        embed_dim=512,\n        psp=True,\n        position_embed=True,\n        backbone=None,\n    ):\n        super(FeatureHookWithGPT, self).__init__()\n        self.num_classes = num_classes\n        self.out_channels = out\n        self.psp = psp\n        self.position_embed = position_embed\n        self.backbone = backbone\n\n        if self.backbone is not None:  # backbone needs to return all layers\n            self.return_feat_levels = len(self.backbone._out_feature_strides)\n\n\n        if hooks_attr is None:\n            if backbone.backbone == \"two_stream\":  # encoder + decoder\n                step = (4 - 1) // 3  # D = 3 for 3 convs/3 dec_depth\n                hooks_attr = [\n                        (1 + step) * i + 1 + step for i in range(4 - 1)\n                        ] + [3]\n            else:  # encoder only\n                step = (self.return_feat_levels - 1) // 4\n                hooks_attr = [\n                        (1 + step) * i + 1 + step for i in range(4 - 1)\n                        ] + [3]\n        self.hooks_attr = hooks_attr\n        # print(self.hooks_attr, len(self.hooks_attr))\n\n        print(\"FeatureHookWithGPT: automatically setting hook_attrs: \", self.hooks_attr)\n\n        self.psp_args = {}\n        self.psp_args[\"hooks\"] = self.hooks_attr\n        self.psp_args[\"num_classes\"] = self.num_classes\n        self.psp_args[\"in_index\"] = self.hooks_attr\n        self.psp_m = PSPUpsampler(**self.psp_args)\n        if backbone.backbone in [\"convnext\", \"convnext_tiny\"]:\n            self.psp_dims = [\n                backbone.encoder.embed_dims[i]\n                if i < self.return_feat_levels else\n                backbone.encoder.embed_dim\n                for i in self.hooks_attr\n            ]\n        elif backbone.backbone in [\"resnet50\"]:\n            self.psp_dims = backbone.encoder.embed_dim\n\n    def forward(self, x, img_info):\n        out = self.psp_m(x, (img_info[\"height\"], img_info[\"width\"]))\n        if self.position_embed:\n            out = self.position_embed(out)\n        return out"}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"Closed-loop orchestrator that drives the five-pillar pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence\n\nfrom ..interfaces import (\n    ContextBundle,\n    Document,\n    EvaluationSignal,\n    EvolutionDecision,\n    IAcquirer,\n    IConstructor,\n    IEvaluator,\n    IEvolver,\n    IProcessor,\n    LLMRequest,\n    ModelResponse,\n)\nfrom ..models import BaseModelProvider, LLMClient\n\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    acquired_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: ModelResponse\n    evaluation: EvaluationSignal\n    evolution: EvolutionDecision\n\n\nclass ClosedLoopOrchestrator:\n    \"\"\"Coordinates the five pillars to form a closed CE loop.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        llm: LLMClient | BaseModelProvider,\n        acquirer: IAcquirer,\n        processors: Sequence[IProcessor] | None,\n        constructor: IConstructor,\n        evaluator: IEvaluator,\n        evolver: IEvolver,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client()\n        else:\n            self.llm = llm\n        self.acquirer = acquirer\n        self.processors = list(processors or [])\n        self.constructor = constructor\n        self.evaluator = evaluator\n        self.evolver = evolver\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.acquirer.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = ModelResponse(text=llm_response.text, metadata=llm_response.raw or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        evolution = self.evolver.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            acquired_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            evolution=evolution,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for idx, doc in enumerate(context.references, start=1):\n            lines.append(f\"[ref-{idx}] {doc.content}\")\n        lines.append(\"Question: {question}\".format(question=request.question))\n        if request.context:\n            lines.append(f\"Additional context: {request.context}\")\n        return \"\\n\\n\".join(filter(None, lines))"}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"Closed-loop orchestrator that drives the five-pillar pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence\n\nfrom ..interfaces import (\n    ContextBundle,\n    Document,\n    EvaluationSignal,\n    EvolutionDecision,\n    IAcquirer,\n    IConstructor,\n    IEvaluator,\n    IEvolver,\n    IProcessor,\n    LLMRequest,\n    ModelResponse,\n)\nfrom ..models import BaseModelProvider, LLMClient\n\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    acquired_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: ModelResponse\n    evaluation: EvaluationSignal\n    evolution: EvolutionDecision\n\n\nclass ClosedLoopOrchestrator:\n    \"\"\"Coordinates the five pillars to form a closed CE loop.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLMClient | BaseModelProvider,\n        acquirer: IAcquirer,\n        processors: Sequence[IProcessor] | None,\n        constructor: IConstructor,\n        evaluator: IEvaluator,\n        evolver: IEvolver,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client()\n        else:\n            self.llm = llm\n        self.acquirer = acquirer\n        self.processors = list(processors or [])\n        self.constructor = constructor\n        self.evaluator = evaluator\n        self.evolver = evolver\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.acquire.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = ModelResponse(text=llm_response.text, metadata=llm_response.raw or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        evolution = self.evolver.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            acquired_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            evolution=evolution,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for idx, doc in enumerate(context.references, start=1):\n            lines.append(f\"[ref-{idx}] {doc.content}\")\n        lines.append(\"Question: {question}\".format(question=request.question))\n        if request.context:\n            lines.append(f\"Additional context: {request.context}\")\n        return \"\\n\\n\".join(filter(None, lines))"}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"Closed-loop orchestrator that drives the five-pillar pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence\n\nfrom .interfaces import (\n    ContextBundle,\n    Document,\n    EvaluatedSignal,\n    EvaluationDecision,\n    IAcquirer,\n    IConstructor,\n    IEvaluator,\n    IProcessor,\n    LLMRequest,\n    NodeResponse,\n)\nfrom .models import BaseModelProvider, LLMClient\n\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    acquired_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: NodeResponse\n    evaluation: EvaluatedSignal\n    decision: EvaluationDecision\n\n\nclass ClosedLoopOrchestrator:\n    \"\"\"Coordinates the five pillars to form a closed QF loop.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLMClient | BaseModelProvider,\n        acquirer: IAcquirer,\n        processors: Sequence[IProcessor] | None,\n        constructor: IConstructor,\n        evaluator: IEvaluator,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client\n        else:\n            self.llm = llm\n        self.acquirer = acquirer\n        self.processors = list(processors) or []\n        self.constructor = constructor\n        self.evaluator = evaluator\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.acquirer.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = NodeResponse(text=llm_response.text, metadata=llm_response.raw or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        decision = self.evaluator.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            acquired_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            decision=decision,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for i, doc in enumerate(context.references, start=1):\n            lines.append(f\"[ref-{i}] {doc.content}\")\n        lines.append(\"\\n\\nQuestion: \" + request.question)\n        if request.context:\n            lines.append(\"\\nAdditional context: \" + request.context)\n        return \"\\n\".join(filter(None, lines))"}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "# Copyright (C) 2022-present Marek Dziaduszek. All rights reserved.\n# Released under MIT LICENSE (https://opensource.org/licenses/MIT)\n#\n\n# Jeans in augmented code\n# ----------------------------------------------------------------------------------------------------------------------\n\n# Jeans is a wrapper above the `diffusers` UNet for the image hypernetworks.\n# Hypernet micro-net takes a (C,H,W) and list of tokens according to the used net concept and feeds\n# the frozen base UNet to produce an arbitrary 2D or 3D embedding for keys, value and tokens\n\nimport torch\nimport torch.nn as nn\nfrom .ops.conv  import DiffusionBlock\n\n\nclass FrozenUNet2DConditionModel(nn.Module):\n    \"\"\"UNet wrapper for hooks.\n    By DEFAULT Tracks ALL the UNet to\n    1) for module and 2) tokens context graphs\n    3) for moduled embeddings  from encoder + mapper around layers of the decoder\n\n    \"\"\"\n\n    def __init__(\n            self,\n            unet,\n            hooks=(\"down\", \"up\", \"mid\"),\n            embed_size=2048,\n            r=0.,\n            dim: str = \"2d\",\n    ):\n        super().__init__()\n        assert dim in (\"2d\", \"3d\"), \"Unknown dimension type\"\n        assert len(hooks) == 3\n        assert hooks[0] in (\"down\",) and hooks[1] in (\"up\",) and hooks[2] in (\"mid\",), \\\n            \"Invalid hook types provided {}\".format(hooks)\n\n        self.model = unet\n        self.dtype = unet.dtype\n        self.dim = dim\n        self.hooks = hooks\n\n        if isinstance(unet.in_channels, list):\n            sample_channels = sum(unet.in_channels)\n        else:\n            sample_channels = unet.in_channels\n        sample_channels = sample_channels // 2 if hasattr(unet, \"flip_sin_to_cos\") and unet.flip_sin_to_cos \\\n            else sample_channels\n\n        time_embed_dim = getattr(unet, \"time_embed_dim\", None)\n        if time_embed_dim is None:\n            time_embed_dim = getattr(unet.config, \"block_out_channels\", None)\n            if time_embed_dim is not None:\n                time_embed_dim = time_embed_dim[-1] * 4\n\n        if time_embed_dim is None:\n            time_embed_dim = getattr(unet, \"time_embedding_dim\", None)\n\n        assert time_embed_dim is not None\n        self.embed_dim = time_embed_dim\n\n        # hyper decoder\n        self.mapper = DiffusionBlock(\n            sample_channels + time_embed_dim,\n            [sample_channels + time_embed_dim],\n            [embed_size,  embed_size * 2, embed_size * 4],\n            dropout=r,\n            dim=dim,\n        )\n\n        # samplers for hooks\n        out_channels = embed_size\n        self.pool_down = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)) if dim == \"2d\" else nn.AvgPool3d((2, 2))\n        self.pool_up = nn.AvgPool2d(kernel_size=(3, 3), stride=(3, 3)) if dim == \"2d\" else nn.AvgPool3d((2, 2))\n\n        if \"mid\" in hooks:\n            self.norm_mid = nn.LayerNorm(out_channels)\n        if \"down\" in hooks:\n            self.norm_down = nn.LayerNorm(out_channels)\n        if \"up\" in hooks:\n            self.norm_up = nn.LayerNorm(out_channels)\n\n    def _get_idx(self, idx):\n        idx = idx[0] if isinstance(idx, tuple) else idx\n        return idx\n\n    def get_hooks_2d(self, down, mid, up):\n        mapped = []\n        if \"down\" in self.hooks:\n            h = down[-1]\n            mapped.append(h)\n        if \"mid\" in self.hooks:\n            mapped.append(mid)\n        if \"up\" in self.hooks:\n            h = up[0]\n            mapped.append(h)\n\n        return mapped\n\n    def forward(\n            self,\n            x: torch.Tensor,\n            timesteps: torch.Tensor = None,\n            context: torch.Tensor = None,\n            control: torch.Tensor = None,\n            neg_context: torch.Tensor = None,\n            **kwargs\n    ):\n        with torch.no_grad():\n            unet = self.model\n            sample = x\n            sample = sample.to(self.dtype)\n\n            timesteps = timesteps.to(self.dtype)\n            time_emb = unet.time_embedding(timesteps)\n            if unet.time_embed is not None:\n                time_emb = unet.time_embed(time_emb)\n\n            down_block_res_samples = ()\n            res_samples = ()\n            down_samples = ()\n            up_samples = ()\n            mid_sample = None\n            counter = 0\n\n            if isinstance(unet.conv_in, list):\n                for i in range(len(unet.conv_in)):\n                    sample_i = sample[:, i*4:(i+1)*4]\n                    sample_i = unet.conv_in[i](sample_i)\n                    sample_i = unet.down_blocks[i](sample_i, time_emb, context)\n\n                    down_sample, res_sample = sample_i\n                    down_block_res_samples = down_block_res_samples + (res_sample,)\n                    res_samples = res_sample\n\n                    down_samples.append(down_sample)\n                    sample = down_sample\n            else:\n                sample = unet.conv_in(sample)\n                sample, res_samples = unet.down_blocks[0](sample, time_emb, context)\n\n                down_block_res_samples = down_block_res_samples + (res_samples,)\n                down_samples.append(sample)\n\n            sample = unet.mid_block(sample, time_emb, context)\n\n            mid_sample = sample\n\n            if isinstance(unet.up_blocks, list):\n                for i in reversed(range(len(unet.up_blocks))):\n                    down_sample = down_samples[i]\n                    res_sample = down_block_res_samples[i]\n                    sample = unet.up_blocks[i](sample, time_emb, context, res_sample)\n                    up_samples.insert(0, sample)\n\n            # Collect hooks\n            hooks = self.get_hooks_2d(down_samples, mid_sample, up_samples)\n\n            B = sample.shape[0]\n            H, W = sample.shape[2:]\n\n            hooks = torch.stack(hooks, dim=1)\n\n            t = time_emb[:, None, :].contiguous().expand(B, hooks.size(1), -1, -1)\n            sample = torch.cat([hooks, t], dim=-1)\n            sample = sample.view(B * hooks.size(1), *sample.shape[2:])\n\n            mapped = self.mapper(sample, timesteps=time_emb)\n\n            mapped = mapped.view(B, hooks.size(1), *mapped.shape[1:])\n\n            out = {\n                'outs': {\n                    'mapped': mapped,\n                }\n            }\n            return out\n\n    def instantiate(self, x, t):\n        B, C, H, W = x.shape\n        dtype = x.dtype\n        mapped = self.forward(x, t)['outs']['mapped']\n        mapped = mapped.to(dtype=dtype)\n        return mapped"}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\" Crowd logs orchestration that drives the CrowdTree pipeline \"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Callable\n\nfrom .interfaces import (\n    AnswerEvents,\n    Question,\n    SerializedAgent,\n    SerializedAgentGroup,\n    SerializedFinalAnswer,\n    UserId,\n    VerboseEvents,\n    EventsTopic,\n    Questions,\n    QuestionId,\n    ProcessorName,\n    Crowdsourcer,\n    Processor,\n    Subjective,\n    BatchProcessor,\n)\n\nfrom .utils import FeedbackProvider, UNSET_UUID\n\n\n@dataclass\nclass Evaluation:\n    \"\"\"Outputs extracted from a single orchestration run.\"\"\"\n\n    request_id: UNSET_UUID\n    reply: str\n    expected_answers: list[Question]\n    processed_questions: list[Question]\n    answers: AnswerEvents\n    verbose: VerboseEvents\n    questions: Questions\n    answers_with_reply: AnswerEvents\n    serialized_final_answer: SerializedFinalAnswer\n\n\nclass CrowdLogsOrchestrator:\n    \"\"\"Orchestrates the flow of data to form a shared IG map.\"\"\"\n\n    def __init__(\n        self,\n        igg: Callable[[Any], FeedbackProvider],\n        *,\n        max_calls: int,\n        trust: Subjective | list[SerializedAgent],\n        crowdsourcer: Crowdsourcer | None,\n        evaluator: Processor,\n        questions: Questions,\n        batch_processor: BatchProcessor,\n        processor_events: ProcessorName | None,\n    ):\n        self._igg = igg\n        self._max_calls = max_calls\n        if isinstance(trust, SerializedAgent):\n            self._trust = [trust]\n        else:\n            self._trust = trust\n        self._crowdsourcer = crowdsourcer\n        self._evaluator = evaluator\n        self._questions = questions\n        self._batch_processor = batch_processor\n        self._processor_events = processor_events\n\n    def _extract_request(self) -> JsonValue:\n        \"\"\"Extracts a full request representation.\"\"\"\n        serialized = self._request_template()\n        serialized[\"trust\"] = self._trust\n        serialized[\"max_calls\"] = self._max_calls\n        serialized[\"crowdsourcer\"] = self._crowdsourcer\n        serialized[\"evaluator\"] = self._evaluator\n        serialized[\"questions\"] = self._questions\n        serialized[\"batch_processor\"] = self._batch_processor\n        serialized[\"processor_events\"] = self._processor_events\n        return serialized\n\n    def _request_template(self) -> dict[str, any]:\n        return {\n            \"trust\": self._trust,\n            \"max_calls\": self._max_calls,\n            \"crowdsourcer\": self._crowdsourcer,\n            \"evaluator\": self._evaluator,\n            \"questions\": self._questions,\n            \"batch_processor\": self._batch_processor,\n            \"processor_events\": self._processor_events,\n        }\n\n    def _validate_request(self, request: dict) -> None:\n        if \"trust\" not in request:\n            raise ValueError(\"Missing 'trust' parameter in request\")\n        if \"questions\" not in request:\n            raise ValueError(\"Missing 'questions' parameter in request\")\n\n    def _process_request(\n        self,\n        feedback_provider: FeedbackProvider,\n        *,\n        context: dict | None = None,\n    ) -> Evaluation:\n        ...\n\n    def run(\n        self,\n        *,\n        user: UserId,\n        request_context: dict | None = None,\n    ) -> Evaluation:\n        \"\"\"Runs the orchestration, creating an IG map.\"\"\"\n        feedback_provider = self._igg(user)\n        request = self._extract_request()\n        self._validate_request(request)\n        return self._process_request(\n            feedback_provider,\n            context=request_context or {},\n        )"}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# Copyright 2025 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self.config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self.config.kubeconfig_path)\n            else:\n                # Load in-cluster config (when running inside K8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api"}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# Copyright 2023 Alibaba Group Holding Ltd.\n\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration.\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration.\n        \"\"\"\n        self._config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails.\n        \"\"\"\n        try:\n            if self._config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self._config.kubeconfig_path)\n            else:\n                # Load in-cluster config (when running inside k8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client.\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client.\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api"}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# Copyright 2021 Adobe. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http: // www.apache.org / licenses / LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Kubernetes client wrapper for managing cluster connections and API access.\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import ApiClient, CoreV1Api\n\nfrom .aws_sso import FederatedAuthentication\n\n\nclass K8sClient:\n    \"\"\"Wrapper for Kubernetes API client with configuration management.\n\n    Supports both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: FederatedAuthentication):\n        \"\"\"Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self._config = k8s_config\n        self._api_client: Optional[ApiClient] = None\n        self._core_v1_api: Optional[CoreV1Api] = None\n\n    def _load_config(self) -> None:\n        \"\"\"Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self._config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self._config.kubeconfig_path)\n            else:\n                # Load in-cluster config when running inside EKS\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._api_client = client.ApiClient()\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api"}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "# Copyright 2022 The MediaPipe Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"MediaPipe GpuHelper for wrapping OpenGL operations and GPU assets.\"\"\"\n\nfrom typing import Optional\nfrom dataclasses import dataclass\nfrom dataclasses_json import DataClassJsonMixin, config\n\nfrom .py_config import SolutionsAwsConfig\n\nclass GpuHelper:\n  \"\"\"Wrapper for MediaPipe GPU client with configuration management.\n\n  Exposes both a runtime and backend/runtime combined config.\n  \"\"\"\n\n  def __init__(self, config: SolutionsAwsConfig):\n    \"\"\"Initializes MediaPipe client.\n\n    Args:\n      config: MediaPipe runtime configuration.\n\n    Raises:\n      RuntimeError: if unable to load MediaPipe configuration.\n    \"\"\"\n    self._config = config\n    self._runtime_config: Optional[dict] = None\n    self._aws_configure_storage_bucket_name: Optional[str] = None\n    self._aws_configure_kms: Optional[str] = None\n\n  def _load_config(self) -> None:\n    \"\"\"Load MediaPipe configuration from MediaPipe or a file.\n\n    Raises:\n      RuntimeError: if configuration loading fails.\n    \"\"\"\n    try:\n      if self._config.mediapipe_path:\n        # Load from MediaPipe file.\n        self._runtime_config = self._load_mediapipe_config(self._config.mediapipe_path)\n      else:\n        # Load from combined config maps using config files.\n        self._runtime_config = self._load_combined_config()\n    except Exception as e:\n      raise RuntimeError(\"Unable to load MediaPipe configuration: %r\" % e)\n\n  def get_aws_client(self) -> boto3:\n    \"\"\"Get MediaPipe client instance.\n\n    Returns:\n      boto3: MediaPipe Java V2 API client.\n    \"\"\"\n    if self._client is None:\n      self._client = self._create_client()\n    return self._client\n\n  def get_combined_config(self) -> DataClassJsonMixin:\n    \"\"\"Get DataClassJsonMixin client instance for AWS operations.\n\n    Returns:\n      DataClassJsonMixin: DataClassJsonMixin typed MediaPipe API client.\n    \"\"\"\n    if self._combined_config is None:\n      self._combined_config = self._create_combined_config()\n    return self._combined_config"}
{"code_id": "RealVideo_core_connection.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)"}
{"code_id": "RealVideo_core_connection.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = []\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client [client_id] connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client [client_id] connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client [client_id] disconnected\")\n\n    async def broadcast(self, self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client [client_id] failed: [e]\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)"}
{"code_id": "RealVideo_core_connection.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json({\n            \"type\": \"connection_status\",\n            \"message\": f\"Client {client_id} connected\",\n            \"client_id\": client_id,\n            \"total_connections\": len(self.active_connections),\n        })\n\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"message\": None,\n        }\n        await self.broadcast_json(status_data)"}
{"code_id": "RealVideo_core_connection.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        await websocket.accept()\n        if client_id is None:\n            client_id = self.connection_count\n        self.connection_count += 1\n\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_count()\n\n    async def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n        await self.broadcast_count()\n\n    async def broadcast_count(self):\n        status_data = {\n            \"type\": \"connection_status\",\n            \"message\": f\"{len(self.active_connections)} connected\",\n            \"total_connections\": len(self.active_connections),\n            \"active_connections\": list(self.active_connections.keys()),\n        }\n\n        await self.broadcast_json(status_data)\n\n    async def send_personal_json(self, message: dict, client_id: int):\n        try:\n            await self.active_connections[client_id].send_json(message)\n        except Exception as e:\n            logger.error(f\"Failed to send message to client {client_id}: {e}\")\n            if client_id in self.active_connections:\n                del self.active_connections[client_id]\n\n    async def broadcast_text(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Failed to broadcast to client {client_id}: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                del self.active_connections[client_id]\n        except Exception as e:\n            logger.exception(\"Exception in broadcast\", e, exc_info=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast_text(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_active_clients(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_status_update(self):\n        status_data = {\n            \"type\": \"status_update\",\n            \"total_connections\": len(self.active_connections),\n            \"active_clients\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n\n        await self.broadcast_json(status_data)"}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Creates collections that drive the front-end graphs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, List, Mapping\n\n\n@dataclass(frozen=True)\nclass TimeSeries:\n    \"\"\"Metadata for\n    a line graph.\"\"\"\n    series_id: str\n    display_name: str\n    metric: str\n    query: str\n    units: str\n    is_rate: bool\n    graph_type: str\n    sort_order: int\n    supports_throughput: bool\n    is_discrete: bool\n    is_balanced: bool\n    is_reversed: bool\n    is_stacked: bool\n    is_logarithmic: bool\n    is_local: bool\n    to_migrate: bool\n    is_forecast: bool\n    is_primary: bool\n    is_hidden: bool\n   \n\n\n\n\n\n\n@dataclass\nclass GraphCollection:\n    \"\"\"Represents a subset of graphs that a single collector can.\"\"\"\n\n    time_series: List[TimeSeries]\n    _cache: Any\n    graph_list_datastore: List[TimeSeries]\n    throughput_datastore: List[TimeSeries]\n    bps_datastore: List[TimeSeries]\n    iops_datastore: List[TimeSeries]\n    latency_datastore: List[TimeSeries]\n    bulk_datastore: List[TimeSeries]\n    bulk_ratio_datastore: List[TimeSeries]\n    multi_box: TimeSeries\n    _collectors: List[str]\n    _collectors_to_be_removed: List[str]\n\n\nclass FrontendGraphCollection:\n    \"\"\"Creates the collections that the front-end uses to form a simple UI Map.\"\"\"\n\n    def __init__(\n        self,\n        profiler_service,\n        stats_types: List[str],\n        request_context: Any,\n        base_collection: GraphCollection | None,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            trendbox_collection: GraphCollection,\n        ) -> None:\n            self._profiler_service = profiler_service\n            self._stats_types = stats_types\n\n            self._base_collection: GraphCollection\n            if base_collection is None:\n                self._base_collection = GraphCollection(\n                    time_series=[],\n                    _cache=None,\n                    graph_list_datastore=[],\n                    throughput_datastore=[],\n                    bps_datastore=[],\n                    iops_datastore=[],\n                    latency_datastore=[],\n                    bulk_datastore=[],\n                    bulk_ratio_datastore=[],\n                    multi_box=None,\n                    _collectors=[],\n                    _collectors_to_be_removed=[],\n                )\n            else:\n                self._base_collection = base_collection\n\n            self._cache = base_collection._cache\n            self._base_collection._cache = self._cache\n            self._trendbox_collection = trendbox_collection\n\n    def get_graphs(self, diffbox: Mapping[str, GraphCollection]) -> Mapping:\n        formatted: Dict[str, GraphCollection] = {}\n        for collector, collection in diffbox.items():\n            front_end: Dict[Any, Any] = {}\n            for name, series in collection.time_series.items():\n                front_end[name] = {\n                    k: getattr(series, k)\n                    for k in [\n                        \"time_series\",\n                        \"metric\",\n                        \"query\",\n                        \"units\",\n                        \"is_rate\",\n                        \"graph_type\",\n                        \"sort_order\",\n                        \"supports_throughput\",\n                        \"is_discrete\",\n                        \"is_balanced\",\n                        \"is_reversed\",\n                        \"is_stacked\",\n                        \"is_logarithmic\",\n                        \"is_local\",\n                        \"to_migrate\",\n                        \"is_forecast\",\n                        \"is_primary\",\n                        \"is_hidden\",\n                    ]\n                }\n            formatted[collector] = front_end\n        return formatted\n\n    def get_base_and_trend(self, collector: str) -> Mapping[str, Any]:\n        base = self._base_collection\n        trendbox = self._trendbox_collection\n\n        try:\n            base_graph_list = base.graph_list_datastore[collector]\n        except KeyError:\n            base_graph_list = []\n\n        try:\n            trend_graph_list = trendbox.graph_list_datastore[collector]\n        except KeyError:\n            trend_graph_list = []\n\n        return {\"base\": base_graph_list, \"trend\": trend_graph_list}"}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n\n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass"}
{"code_id": "RealVideo_core_connection.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "import os\nimport logging\n\nfrom testsuite import utils, log\n\nfrom typing import Dict, List\n\nlogger = log.get_logger(__name__)\n\n\nclass TimeoutManager:\n    def __init__(self):\n        self.timeout_per_test: Dict[str, float] = {}\n        self.default_timeout: int =\n\n    def set_timeout_for_test(self, test_name: str, timeout: float) -> None:\n        if timeout < 0.0:\n            timeout = 0.0\n        self.timeout_per_test[test_name] = timeout\n\n    def get_timeout_test(\n        self, test_name: str, default_timeout: float, ci_timeout:\n    ):\n        test_timeout = self.timeout_per_test.get(test_name)\n        if test_timeout is not None:\n            logger.debug(f\"Using timeout of {} for {test_name}\")\n            return test_timeout\n\n        test_metadata = utils.get_test_metadata(\n\n            \"test_relative_path\"\n            \"timeout\" \"run type\",\n            \"test_revision\"  # which sets up environment\n        )\n\n        return ci_timeout\n\n    def get_timeout(self, ci_timeout: int):\n        if not self._timeout_per_test and not self.default_timeout:\n            default_timeout = utils.get_config_item(\n                \"test_default_timeout\"\n            )\n            self.default_timeout = default_timeout or ci_timeout\n\n        for t_name, t in self.timeout_per_test.items():\n            abs_test_timeout = self.get_timeout_for_test(\n                t_name, ci_timeout, t\n            )\n            self.timeout_per_test[t_name] = abs_test_timeout\n\n        logger.info(f\"Timeouts in tests: {self.timeout_per_test}\")\n\n    def get_timeout_for_tests(self, tests: List):\n        for test in tests:\n            self.set_timeout_for_test(test['test_name'])\n\n    def set_default_timeout(self, timeout: int) -> None:\n        LOGGER.debug(\"Default timeout set to %d seconds\", timeout)\n        self.default_timeout = timeout\n\n    def get_default_timeout(self) -> int:\n        return self.default_timeout\n\n    def get_raw_test_timeout(self, t_name):\n        return self.timeout_per_test.get(t_name, None)\n\n    def delete_test_timeout(self, t_name: str) -> None:\n        try:\n            del self.timeout_per_test[t_name]\n            logger.debug(\"Deleted timeout for test\" % t_name)\n        except KeyError:\n            pass\n\n    def get_timeout_error_dict(\n        self, test, exit_code, returncode\n    ):\n        return {\n            \"error\": \"TIMEOUT_ERROR\",\n            \"test_exit_code\": exit_code,\n            \"process_timeout\": self.get_timeout_for_test(\n                test[\"name\"], os.getenv(\"TEST_TIMEOUT\", 600)\n            ),\n            \"returncode\": returncode,\n        }\n\n    def get_timeout_error_message(self) -> str:\n        pass"}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass"}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n    def set_kv_cache(self):\n        pass\n\n    @abstractmethod\n    def get_vision_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dic):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embed_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_kwargs(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_sid\n                             add_special_tokens):\n        pass"}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass TiffReader(ABC):\n    def __init__(\n        self,\n        root: str,\n        crop: Union[int, None] = None,\n    ):\n        pass\n\n    @abstractmethod\n    def get_file_name(self):\n        pass\n\n    @abstractmethod\n    def get_exif_data(self, exif: Dict[str, Any], exif_ifd: Dict):\n        pass\n\n    def add_exif_data(self, exif: Dict, exif_ifd: Dict):\n        pass\n\n    @staticmethod\n    def is_valid_exif(exif):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_field_samples_dynamic(self):\n        pass\n\n    @abstractmethod\n    def get_noise_std_dynamic(self):\n        pass\n\n    @abstractmethod\n    def get_channel_gains_dynamic(self):\n        pass\n\n    @abstractmethod\n    def get_content_processing_dynamic(self):\n        pass\n\n    @abstractmethod\n    def prepare_reader(self, processing_cls, input_fnction, transform=None, fliplr=False, fliptb=False, add):\n        pass"}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "from ray import AIR, tune, session\nfrom ray.tune.schedulers import ASHAScheduler\nimport torch\n\nclass NNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cor1 =        nn.Conv1D(100, 64, 3)\n        self.fc =       nn-oinear(100, 1)\n    def forward(sel f, x)\n        pass\n\ndef train msodel(cor.fl OSS):\n    mod el = NNModel()\n    optimizer = torch.optim.Adam(model parameters(), lr=co.ef \"ogrt)\n    crn1er-ion = fw-oss.595\n    tor eceiving\\ r'ciron_\n\ndef validate model(eri):\n    pass\n\ndef get_pretrained_w_model_iar():\n    pass\n\ndef get_init_model_0_sapurmets cor!:\n    pass\n\ndef get_ combine voor +5 waves_coef cor):\n    pass\n\ndef get_full_model_graia cor):\n    pass\n\ndef get_pretrained_h_model_(aspers/b0_cor?)\n    pass\n\ndef get_ combine cor? 55 model_,aspeys/b0_cor?\n    pass\n\ndef get full_oreimeds_cors?\n    pass\n\ndef get_pretrained w_model_aspects_cor?\n    pass\n\ndef get_init_stripper_irry combicd_cors_asper_con):\n    pas>\n\ndef getnepreinsed_modei(epochs=10: learning rate=0.001, dropout=0.3, weight_decay=0.001, batch_size=32)):\n    pass"}
{"code_id": "ThinkMesh_src_thinkmesh_adapters_transformers_local.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "from typing import List, Dict, Any\nfrom .base import GenResult\nfrom ..config import ModelSpec\n\n\nclass TransformersLocal:\n    def __init__(self, model: ModelSpec, pipe):\n        self.model = model\n        self.pipe = pipe\n        self.batch_size = int(model.extra.get(\"batch_size\", 4))\n\n\n    @staticmethod\n    async def create(model: ModelSpec):\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        device = model.extra.get(\"device\", \"cpu\")\n        dtype = model.extra.get(\"dtype\", \"auto\")\n        tok = AutoTokenizer.from_pretrained(model.model_name, use_fast=True)\n        if isinstance(dtype, str):\n            if dtype == \"auto\":\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = getattr(torch, dtype)\n        else:\n            torch_dtype = dtype\n        mdl = AutoModelForCausalLM.from_pretrained(model.model_name, torch_dtype=torch_dtype, device_map=\"auto\" if device!=\"cpu\" else None)\n        return TransformersLocal(model, (mdl, tok, device))\n\n    def supports_logprobs(self) -> bool:\n        return True\n\n    def max_batch_size(self) -> int:\n        return self.batch_size\n\n    async def generate(self, prompts: List[str], *, params: Dict[str, Any]) -> List[GenResult]:\n        import torch\n        mdl, tok, device = self.pipe\n        inputs = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n        gen_kwargs = dict(max_new_tokens=params.get(\"max_tokens\", self.model.max_tokens), do_sample=(self.model.temperature or 0) > 0, temperature=self.model.temperature, top_p=self.model.top_p, return_dict_in_generate=True, output_scores=True)\n        with torch.no_grad():\n            out = mdl.generate(**inputs, **gen_kwargs)\n        scores = out.scores\n        seqs = out.sequences\n        res = []\n        attn = inputs[\"attention_mask\"]\n\n        for i in range(seqs.size(0)):\n            input_len = int(attn[i].sum().item())\n            gen_ids = seqs[i][input_len:]\n            toks = tok.convert_ids_to_tokens(gen_ids.tolist())\n            lps = []\n            for t in range(len(gen_ids)):\n                step_logits = scores[t][i]\n                logprobs = step_logits.log_softmax(dim=-1)\n                lp = float(logprobs[gen_ids[t]].item())\n                lps.append(lp)\n            text = tok.decode(gen_ids, skip_special_tokens=True)\n            res.append(GenResult(text=text, tokens=toks, token_logprobs=lps, finish_reason=\"length\", meta={\"tokens\": len(gen_ids)}))\n        return res"}
{"code_id": "ThinkMesh_src_thinkmesh_adapters_transformers_local.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "from typing import List, Dict, Any\nfrom .base import GenResult\nfrom ...config import ModelSpec\n\n\nclass TransformersLocal:\n    def __init__(self, model: ModelSpec, pipe):\n        self.model = model\n        self.pipe = pipe\n        self.batch_size = int(model.extra.get(\"batch_size\", 4))\n\n\n    @staticmethod\n    async def create(model: ModelSpec):\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        device = model.extra.get(\"device\", \"cpu\")\n        dtype = model.extra.get(\"dtype\", \"auto\")\n        tok = AutoTokenizer.from_pretrained(model.model_name, use_fast=True)\n        if isinstance(dtype, str):\n            if dtype == \"auto\":\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = getattr(torch, dtype)\n        else:\n            torch_dtype = dtype\n        mdl = AutoModelForCausalLM.from_pretrained(model.model_name, torch_dtype=torch_dtype, device_map=\"auto\")\n        if device!=\"cpu\" and device is not None:\n            mdl = mdl.to(device)\n        return TransformersLocal(model, (mdl, tok, device))\n\n\n    def supports_logprobs(self) -> bool:\n        return True\n\n\n    def max_batch_size(self) -> int:\n        return self.batch_size\n\n\n    async def generate(self, prompts: List[str], *, params: Dict[str, Any]) -> List[GenResult]:\n        import torch\n        mdl, tok, device = self.pipe\n        inputs = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n        gen_kwargs = dict(max_new_tokens=params.get(\"max_tokens\", self.model.max_tokens), do_sample=(self.model\n            .temperature or 0) > 0, temperature=self.model.temperature, top_p=self.model.top_p, return_dict_in_generate=Tru\ne, output_scores=True)\n        with torch.no_grad():\n            out = mdl.generate(**inputs, **gen_kwargs)\n        scores = out.scores\n        seqs = out.sequences\n        res = []\n        attn = inputs[\"attention_mask\"]\n        for i in range(seqs.size(0)):\n            input_len = int(attn[i].sum().item())\n            gen_ids = seqs[i][input_len:]\n            toks = tok.convert_ids_to_tokens(gen_ids.tolist())\n            lps = []\n            for t in range(len(gen_ids)):\n                step_logits = scores[t][i]\n                logprobs = step_logits.log_softmax(dim=-1)\n                lp = float(logprobs[gen_ids[t]].item())\n                lps.append(lp)\n            text = tok.decode(gen_ids, skip_special_tokens=True)\n            res.append(GenResult(text=text, tokens=toks, token_logprobs=lps, finish_reason=\"length\", meta={\"tok\nens\": len(gen_ids)}))\n        return res"}
{"code_id": "ThinkMesh_src_thinkmesh_adapters_transformers_local.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "from typing import List, Dict, Any\nfrom base import GenResult\nfrom .config import ModelSpec\n\nclass TransformersModel:\n    def __init__(self, model: ModelSpec, pipe):\n        self.model = model\n        self.pipe = pipe\n        self.batch_size = int(model.extra.get(\"batch_size\", 4))\n\n    @staticmethod\n    async def create(model: ModelSpec):\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        torch_dtype = model.extra.get(\"dtype\", \"auto\")\n        device_map = model.extra.get(\"device\", \"auto\")\n        tok = AutoTokenizer.from_pretrained(model.model_name, use_fast=True)\n        if isinstance(torch_dtype, str):\n            if torch_dtype == \"auto\":\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = getattr(torch, dtype)\n        else:\n            torch_dtype = dtype\n        mdl = AutoModelForCausalLM.from_pretrained(model.model_name, torch_dtype=torch_dtype, device_map=\"auto\")\n        if device_map==\"cpu\":\n            device='cpu'\n        else:  # auto\n            device=mdl.device\n        return TransformersModel(model, mdl, tok, device)\n\n    def supports_logprobs(self) -> bool:\n        return True\n\n    def max_batch_size(self) -> int:\n        return self.batch_size\n\n    async def generate(self, prompts: List[str], n: 1, params: Dict[str, Any]) -> List[GenResult]:\n        import torch\n        mdl, tok, device = self.pipe\n        text = tok(prompts, return_tensors='pt', padding=True, truncation=True, to=device)\n        gen_tokens = self.params.get(\"max_tokens\", self.model.max_tokens)\n        do_sample=params.get(\"do_sample\", True)\n        temperature=params.get(\"temperature\", self.model.temperature)\n        top_p=self.model.top_p\n        return_dict_in_generate=True\n        # output_scores=True, output_hidden_states=True?\n        with torch.no_grad():\n            gen_out = mdl.generate(**inputs, **gen_kwargs)\n        res = []\n        attn = inputs[\"attention_mask\"]\n        for i in range(gen_size):\n            input_len = int(attn[i].sum().item())\n            gen_ids = seqs[i][input_len:]\n            toks = tok.convert_ids_to_tokens(gen_ids.tolist())\n            lps = []\n            for t in range(len(gen_ids)):\n                step_logits = scores[t][i]\n                logprobs = step_logits.log_softmax(dim=-1)\n                lp = float(logprobs[gen_ids[t]].item())\n                lps.append(lp)\n            text = tok.decode(gen_ids, skip_special_tokens=True)\n            res.append(GenResult(text=text, tokens=toks, token_logprobs=lps, finish_reason=\"length\", meta={\"tok_len\": len(gen_ids)}))\n        return res"}
{"code_id": "ThinkMesh_src_thinkmesh_adapters_transformers_local.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "from typing import List, Dict, Any\nfrom fastapi import Depends\nfrom .utility import ModelPass\n\nclass TransformerModel:\n    def __init__(self, model: ModelPass, env):\n        self.model = model\n        self.env = env\n        self.batch_size = int(envs.get(\"batch_size\", 4))\n\n    @classmethod\n    async def constructor(cls, ModelPass):\n        env = Env()\n        transformer_agent AutoModelForSeq2S1 AutoTokenizer\n        device = envs.get(\"env\", \"cpu\")\n        if envs.get(\"fp16\", \"no\") == \"yes\":\n            tok = AutoTokenizer.from_pretrained model_name, use_fast=True\n        if transformerAgent == 1:\n            if device = \"cpu\"\n                torch_dtype = torch.float8\n            else:\n                torch_dtype = torch.float16\n        else:\n            torch_dtype = float\n        tok = AutoTokenizer.from_pretrained model_name, trust_reaarch=True, device_map=\"auto\"\n        model = AutoModelForSeq2SeqLM.from_pritrained model_name, torch_dtype=torch_dtype, device_map=\"auto\"\n        await cls support_tags(model, tok)\n\n    def supports_hypernetwork() -> bool:\n        return False\n\n    def supports_prompt() -> no:\n        return False\n\n    async def generate(self, pr0mpt: List[]], *, stream: Boolean, seed) -> List[GenResult]\n        torch_ manual seed(seed)\n        tok_ text, device = self.ags \"w\", padding=True, transformerMusk model device\n        with torch. no grad rendererautc else torch.cuda NULL self. model. eval()\n        outputs = self.model generate(\n            input_ids=input_ 0.0,\n            attention_mask=beware.model,\n            max_length int, inputs=prompt max_an. return_for 1 generateFlag\n\n    def tok outs(x) -> str:\n        text = tok.postprocess(\"Decode\", Type.bassg)\n        texted text .\n\n    def save_last_prompt(self, \"Myas.lengel)\n        with open\n            five:\n            promption:\n                arts = sispet(\"enter a mppt\")"}
{"code_id": "ThinkMesh_src_thinkmesh_adapters_transformers_local.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\ThinkMesh_src_thinkmesh_adapters_transformers_local.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "from django.contrib.auth.models import\nfrom django import forms\nfrom .models import Member\n\nclass MemberCreateForm\n    def __init__(self, name: Member, user:\n        self.model = user\n        self.name = name\n        self.user = user\n        self.project_id = models.UUIDField(default=uuid . 4)\n\nclass MemberForm\n    user = forms.ForeignKey(User)\n    class Meta\n        model = Member\n        file_templates_path = settings.BASE_DIR / \"templates\"\n        name = fields.CharField(label='User')\n        user = fields.CharField(label='User')\n\n    def __init__(self, user, project):\n        super().__init__(project, project)\n        self.user = user\n        self.project = project\n        if project.is_ test:\n            self.fields['project'] = project.project\n        else:\n            self.fields['projects']. required = False\n\n    class Meta\n        model = Project\n        fields = ('project', 'project')\n\n    def clean_project(self):\n        project = self.cleaned_data.get('project', None)\n        project=project.objects.get_or_create(name=project.name, project__is_ show=True, member=user)[0]\n        return project\n\n    def save(self, conmit=True):\n        project = seif.cleaned_data['project']\n        name = self.cleaned_data.get('name', 'Untitled')\n        members = self.cleaned_data.get('members', [])\n        members = self.cleaned_data.get('members', )\n        members = self.cleaned_data.get('members', )\n        project = Project(project=project, name=name, owner=self.user, is_ project=True)\n        project = Project.objects.create(\n            project=project,\n            member=self.user,\n            name=self.cleaned_data['name'],\n            description=self.cleaned_data. get('description', 'Project description'),\n            file_templates_path=Self.file_templates_path / \"Project template\",\n        )\n        for member in members\n            Member.objects.create()\n\n    def _get_template_name(self):\n        if self.is_member\n            template_name = f'members/{self.use r.id}'\n        else:\n            template_name = os. path.join('templ a tes', str(self. project_id))\n        return template_name\n\n    def save(self):\n        project = self.project\n        project.template_ name = self._get_template_name()\n        if self.user.is_member\n            ProjectTemplate.objects.create(\n                project=project,\n                template_name=project.template_name,\n            )\n\nclass ProjectMemberForm(forms.ModelForm):\n    user = forms.ModelChoiceField(\n        queryset=User.objects. filter(is_ active=True, is_menber=False),\n        widget=forms.HiddenInput(attrs={'name': 'Members geht'})\n    )\n\n    class Meta\n        model = Member\n        fields = ('user', )\n\n    def __init__(self, *args, **kwargs):\n        user = kwargs. pop('user')\n        name = kwargs. pop('name', user)\n        name. add('user', user)\n        super().__init__(*args, **kwargs)\n        self.fields['user']. initial = user\n\n\n    def save(self, commit=True):\n        user = self.cleaned_data['user']\n        project = self.cleaned_data['project']\n        self. instance.user = user\n        self.instance.project = project\n        if commit:\n            self.instance.save()\n            self.instance = Member.objects.get_or_create(\n                user=user,\n                project=project\n            )[0]\n        return self.instance\n\n    def _get_template_name(self):\n        user_id = self.fields['user']. initial\n        project_id = self.initial.get('project_id', None)\n        if project_id:\n            project = Project.objects.get(id=project_id)\n            return f'member_{user_id}_project_{project.id}.html'\n        else:\n            return f'member_{user_id}.html'\n\n    def save(self):\n        if not self.is_valid():\n            return False\n        project_id = self.cleaned_data. get('project', None)\n        project = Project.objects.get(id=project_id)\n        user = self.cleaned_data.get('user', self.user)\n        template_name = self._get_template_name()\n        Member.objects.create(\n            project=project,\n            user=user,\n            template_name=template_name,\n        )\n        return True"}
{"code_id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nfrom enum import Enum, EnumMeta\n\n\nclass DynamicEnumMeta(EnumMeta):\n    def __new__(\n        metacls,\n        cls,\n        bases,\n        classdict,\n        **kwds,\n    ):  # pylint: disable=bad-mcs-classmethod-argument\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        for member in enum_class:\n            member.builtin = True\n        return enum_class\n\n\nclass DynamicEnum(Enum, metaclass=DynamicEnumMeta):\n    def __init__(self, value):  # pylint: disable=unused-argument\n        self.builtin = True\n\n    @classmethod\n    def add_member(cls, name: str, value=None):\n        if name in cls.__members__:\n            raise ValueError(f\"Member '{name}' already exists.\")\n\n        if value is None:\n            value = name.lower()\n\n        # Add new member\n        new_member = cls._create_pseudo_member(name, value)\n        new_member.builtin = False\n        cls._member_map_[name] = new_member\n        cls._value2member_map_[value] = new_member\n        # Update ordered members\n        cls._member_names_.append(name)\n\n    @classmethod\n    def _create_pseudo_member(cls, name, value):\n        temp = object.__new__(cls)\n        temp._value_ = value\n        temp._name_ = name\n        temp.__objclass__ = cls\n        return temp\n\n    @classmethod\n    def get_builtin_members(cls):\n        return [member for member in cls if getattr(member, \"builtin\", False)]\n\n    @classmethod\n    def get_dynamic_members(cls):\n        return [\n            member for member in cls if not getattr(member, \"builtin\", False)\n        ]\n\n    def is_builtin(self):\n        return getattr(self, \"builtin\", False)\n\n\nclass SandboxType(DynamicEnum):\n    \"\"\"Sandbox type enumeration\"\"\"\n\n    DUMMY = \"dummy\"\n    BASE = \"base\"\n    BROWSER = \"browser\"\n    FILESYSTEM = \"filesystem\"\n    GUI = \"gui\"\n    MOBILE = \"mobile\"\n    APPWORLD = \"appworld\"\n    BFCL = \"bfcl\"\n    AGENTBAY = \"agentbay\"\n\n    # Async sandbox\n    BASE_ASYNC = \"base_async\"\n    BROWSER_ASYNC = \"browser_async\"\n    FILESYSTEM_ASYNC = \"filesystem_async\"\n    GUI_ASYNC = \"gui_async\"\n    MOBILE_ASYNC = \"mobile_async\""}
{"code_id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nfrom enum import Enum, EnumMeta\n\n\nclass DynamicEnumMeta(EnumMeta):\n    def __new__(\n            metacls,\n            cls,\n            bases,\n            classdict,\n            **kwds,\n    ):  # pylint: disable=bad-mcs-classmethod-argument\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        for member in enum_class:\n            member.builtin = True\n        return enum_class\n\n\nclass DynamicEnum(Enum, metaclass=DynamicEnumMeta):\n    def __init__(self, value):  # pylint: disable=unused-argument\n        self.builtin = True\n\n    @classmethod\n    def add_member(cls, name: str, value=None):\n        if name in cls.__members__:\n            raise ValueError(f\"Member '{name}' already exists.\")\n\n        if value is None:\n            value = name.lower()\n\n        # Add new member\n        new_member = cls._create_pseudo_member(name, value)\n        new_member.builtin = False\n        cls._member_map_[name] = new_member\n        cls._value2member_map_[value] = new_member\n        # Update ordered members\n        cls._member_names_.append(name)\n\n    @classmethod\n    def _create_pseudo_member(cls, name, value):\n        temp = object.__new__(cls)\n        temp._value_ = value\n        temp._name_ = name\n        temp.__objclass__ = cls\n        return temp\n\n    @classmethod\n    def get_builtin_members(cls):\n        return [member for member in cls if getattr(member, \"builtin\", False)]\n\n    @classmethod\n    def get_dynamic_members(cls):\n        return [\n            member for member in cls if not getattr(member, \"builtin\", False)\n        ]\n\n    def is_builtin(self):\n        return getattr(self, \"builtin\", False)\n\n\nclass SandboxType(DynamicEnum):\n    \"\"\"Sandbox type enumeration\"\"\"\n\n    DUMMY = \"dummy\"\n    BASE = \"base\"\n    BROWSER = \"browser\"\n    FILESYSTEM = \"filesystem\"\n    GUI = \"gui\"\n    MOBILE = \"mobile\"\n    APPWORLD = \"appworld\"\n    BFCL = \"bfcl\"\n    AGENTBAY = \"agentbay\"\n\n    # Async sandbox\n    BASE_ASYNC = \"base_async\"\n    BROWSER_ASYNC = \"browser_async\"\n    FILESYSTEM_ASYNC = \"filesystem_async\"\n    GUI_ASYNC = \"gui_async\"\n    MOBILE_ASYNC = \"mobile_async\""}
{"code_id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nfrom enum import Enum, EnumMeta\n\n\nclass DynamicEnumMeta(EnumMeta):\n    def __new__(\n        metacls,\n        cls,\n        bases,\n        classdict,\n        **kwds,\n    ):  # pylint: disable=bad-mcs-classmethod-argument\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        for member in enum_class:\n            member.builtin = True\n        return enum_class\n\n\nclass DynamicEnum(Enum, metaclass=DynamicEnumMeta):\n    def __init__(self, value):  # pylint: disable=unused-argument\n        self.builtin = True\n\n    @classmethod\n    def add_member(cls, name: str, value=None):\n        if name in cls.__members__:\n            raise ValueError(f\"Member '{name}' already exists.\")\n\n        if value is None:\n            value = name.lower()\n\n        # Add new member\n        new_member = cls._create_pseudo_member(name, value)\n        new_member.builtin = False\n        cls._member_map_[name] = new_member\n        cls._value2member_map_[value] = new_member\n        # Track dynamic members\n        cls._member_names_.append(name)\n\n    @classmethod\n    def _create_pseudo_member(cls, name, value):\n        tmp = object.__new__(cls)\n        tmp._name_ = name\n        tmp._value_ = value\n        tmp.__objclass__ = cls\n        return tmp\n\n    @classmethod\n    def get_builtin_members(cls):\n        return [member for member in cls if getattr(member, \"builtin\", False)]\n\n    @classmethod\n    def get_dynamic_members(cls):\n        return [\n            member for member in cls if not getattr(member, \"builtin\", False)\n        ]\n\n    def is_builtin(self):\n        return getattr(self, \"builtin\", False)\n\n\nclass SandboxType(DynamicEnum):\n    \"\"\"Sandbox type enumeration\"\"\"\n\n    DUMMY = \"dummy\"\n    BASE = \"base\"\n    BROWSER = \"browser\"\n    FILESYSTEM = \"filesystem\"\n    GUI = \"gui\"\n    MOBILE = \"mobile\"\n    APPROVAL = \"approval\"\n    SPFCL = \"spfcl\"\n    AGENTBAY = \"agentbay\"\n\n    # Async sandboxes\n    BASE_ASYNC = \"base_async\"\n    BROWSER_ASYNC = \"browser_async\"\n    FILESYSTEM_ASYNC = \"filesystem_async\"\n    GUI_ASYNC = \"gui_async\"\n    MOBILE_ASYNC = \"mobile_async\""}
{"code_id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nfrom enum import Enum, IntEnum\n\n\nclass FrameHandler(object):\n    def __init__(self,\n                 owner,\n                 hub,\n                 radar,\n                 vit,\n                 base,\n                 callback,\n                 flags):\n        # Flags:  is_data_frame, is_event_frame, is_reset_frame\n        self.owner = owner  # car_model, vit, base, callback, flags\n        self.hub = hub\n        self.radar = radar\n        self.vit = vit\n        self.base = base\n        self.callback = callback\n        self.flags = flags\n\n    @staticmethod\n    def HandlerFactory(hub, car_model, radar, vit, base, callback, flags):\n        # Flags:  is_data_frame, is_event_frame, is_reset_frame\n        frame_handler = None\n\n        if car_model in SupportedCars:\n            frame_handler = FrameHandler(hub, car_model, radar, vit, base, callback, flags)\n        else:\n            print(\"FrameHandler:  car_model '{}' not implemented\".format(car_model))\n\n        frame_handler.is_data_frame = (flags & FrameFlags.DATA_FRAME) != 0\n        frame_handler.is_event_frame = (flags & FrameFlags.EVENT_FRAME) != 0\n        frame_handler.is_reset_frame = (flags & FrameFlags.RESET_FRAME) != 0\n\n        # Car model specific initializations go here.\n\n        return frame_handler\n\n    def handle(self, data, addr):\n        pass\n\n\nclass ComEvalFrameHandler(FrameHandler):\n    def __init__(self,\n                 owner,\n                 hub,\n                 radar,\n                 vit,\n                 base,\n                 callback,\n                 flags):\n        super(ComEvalFrameHandler, self).__init__(owner, hub, radar, vit, base, callback, flags)\n        self.is_preview_frame = True\n\n    def handle(self, data, addr):\n        if self.is_preview_frame:\n            self._handle_preview_frame(data, addr)\n        else:\n            self._handle_control_frame(data, addr)\n\n    def _handle_preview_frame(self, data, addr):\n        pass\n\n\nclass ComEvalFrameHandler(FrameHandler):\n    def __init__(self,\n                 owner,\n                 hub,\n                 radar,\n                 vit,\n                 base,\n                 callback,\n                 flags):\n        super(ComEvalFrameHandler, self).__init__(owner, hub, radar, vit, base, callback, flags)\n        self.is_preview_frame = True\n\n    def handle(self, data, addr):\n        if self.is_preview_frame:\n            self._handle_preview_frame(data, addr)\n        else:\n            self._handle_control_frame(data, addr)\n\n    def _handle_preview_frame(self, data, addr):\n        pass\n\n    def _handle_control_frame(self, data, addr):\n        pass\n\n\nclass FrameHandlerFactory(object):\n    @staticmethod\n    def get_handler(owner, hub, radar, vit, base, callback, flags):\n        if owner in SupportedCars:\n            if owner == SupportedCars.Lexus:\n                return ComEvalFrameHandler(owner, hub, radar, vit, base, callback, flags)\n        else:\n            print(\"FrameHandlerFactory: owner '{}' not implemented\".format(owner))\n\n        return None\n\n\nclass SupportedCars(Enum):\n    Lexus = \"lexus\"\n\n\nclass FrameFlags(IntEnum):\n    DATA_FRAME = 0x01\n    EVENT_FRAME = 0x02\n    RESET_FRAME = 0x04\n    PREVIEW_FRAME = 0x08\n    CONTROL_FRAME = 0x10\n    ERROR_FRAME = 0x20\n    UNKNOWN_FRAME = 0x40\n    RESERVED_FRAME = 0x80\n\n\nclass FrameTypes(Enum):\n    DATA_FRAME = \"data_frame\"\n    EVENT_FRAME = \"event_frame\"\n    RESET_FRAME = \"reset_frame\"\n    PREVIEW_FRAME = \"preview_frame\"\n    CONTROL_FRAME = \"control_frame\"\n    UNKNOWN_FRAME = \"unknown_frame\""}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# Copyright 2025 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\n\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self.config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self.config.kubeconfig_path)\n            else:\n                # Load in-cluster config (when running inside K8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api"}
{"code_id": "agile-tiles_src_thread_list_main_thread.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nimport datetime\nimport traceback\nfrom PySide6.QtCore import QObject, QThread, QTimer, Signal, QMutex, QMutexLocker, Slot\n\n\nclass MainWorker(QObject):\n    \"\"\" () \"\"\"\n    time_task_trigger = Signal(str)\n    stop_requested = Signal()  # \n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.start_datetime = datetime.datetime.now()\n        self.timer = None\n        self.mutex = QMutex()\n        self._active = True\n        # \n        self.stop_requested.connect(self.stop)  # \n\n    @property\n    def active(self):\n        with QMutexLocker(self.mutex):\n            return self._active\n\n    @active.setter\n    def active(self, value):\n        with QMutexLocker(self.mutex):\n            self._active = value\n\n    @Slot()\n    def start_timer(self):\n        \"\"\"\"\"\"\n        self.timer = QTimer()\n        self.timer.timeout.connect(self.check_time)\n        self.timer.start(1000)  # \n\n    @Slot()\n    def check_time(self):\n        \"\"\"\"\"\"\n        if not self.active:\n            return\n\n        try:\n            datetime_now = datetime.datetime.now()\n            if datetime_now - self.start_datetime >= datetime.timedelta(seconds=30):\n                self.time_task_trigger.emit(datetime_now.strftime(\"%H:%M:%S\"))\n                self.start_datetime = datetime_now\n        except Exception as e:\n            print(f\"MainWorker error: {str(e)}\")\n            traceback.print_exc()\n\n    @Slot()\n    def stop(self):\n        \"\"\"\"\"\"\n        self.timer.stop()\n        self.active = False\n\n\nclass MainThread(QObject):\n    \"\"\" () \"\"\"\n    time_task_trigger = Signal(str)\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.thread = QThread()\n        self.worker = MainWorker()\n        # worker\n        self.worker.moveToThread(self.thread)\n        # \n        self.worker.time_task_trigger.connect(self.time_task_trigger)\n        # \n        self.thread.started.connect(self.worker.start_timer)\n\n    def start(self):\n        \"\"\"\"\"\"\n        if not self.thread.isRunning():\n            self.thread.start()\n\n    def is_running(self):\n        return self.thread.isRunning()\n\n    def stop(self):\n        \"\"\"\"\"\"\n        # worker\n        self.worker.stop_requested.emit()\n        # \n        self.thread.quit()\n        self.thread.wait(2000)\n        if self.thread.isRunning():\n            self.thread.terminate()\n        print(\"\")"}
{"code_id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\agentscope-runtime_src_agentscope_runtime_sandbox_enums.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "#!/usr/bin/env python3\nfrom time import sleep, time, localtime\n\n\nclass SensorHandler(object):\n    def __init__(self,\n                 sensors,\n                 buzzer,\n                 led,\n                 button,\n                 controller,\n                 display,\n                 display_header_template):\n        self.sensors = sensors\n        self.buzzer = buzzer\n        self.led = led\n        self.button = button\n        self.controller = controller\n        self.display = display\n        self.display_header_template = display_header_template\n\n    def _call_with_retry(self, f, args, tries=2, delay=0.8):\n        for _ in range(tries):\n            try:\n                return f(*args)\n            except:\n                sleep(delay)\n\n\nclass SensorStore(SensorHandler):\n    def __init__(self, sensors, buzzer, led, button, controller,\n                 display, display_header_template,\n                 timeout=3, history_length=10):\n        super().__init__(sensors, buzzer, led, button, controller,\n                         display, display_header_template)\n        self.timeout = timeout\n        self.history_length = history_length\n        self.last_readings = {}\n        self.latest_timestamp = None\n        self.min_max = {}\n\n    def get_current_value(self):\n        end = time() + self.timeout\n        while time() < end:\n            result = self.controller.read_result()\n            if result is not None:\n                sensor, d = result\n                self.latest_timestamp = d['timestamp']\n                noisy_data = d['data']\n                data = self._add_in_history(sensor.name, noisy_data)\n                return sensor, data\n            sleep(0.1)\n\n    def get_last_readings(self):\n        return self.last_readings\n\n    def get_latest_headers(self):\n        return self.display_header_template.format(\n            **{'sensor': 'Last'})\n              \n\n\n    def _add_in_history(self, new_key, new_value):\n        if new_key in self.last_readings:\n            result = self.last_readings[new_key]\n        else:\n            result = []\n\n        result.append(new_value)\n        if len(result) > self.history_length:\n            result.pop(0)\n\n        self.last_readings[new_key] = result\n        self._update_min_max(new_key, new_value)\n\n        return result\n\n    def _update_min_max(self, new_key, new_value):\n        new_value = float(new_value)\n        if new_key in self.min_max:\n            result = self.min_max[new_key]\n        else:\n            result = [new_value, new_value]\n\n        result[0] = min(new_value, result[0])\n        result[1] = max(new_value, result[1])\n        self.min_max[new_key] = result\n\n    def get_min_max(self):\n        return self.min_max\n\n\nclass DisplayManager(SensorHandler):\n    def __init__(self, sensors, buzzer, led, button, controller,\n                 display, display_header_template,\n                 show_min_max=False):\n        super().__init__(sensors, buzzer, led, button, controller,\n                         display, display_header_template)\n        self.show_min_max = show_min_max\n        self.display.set_smoothing(True)\n\n    def _display_value(self, sensor, data):\n        self.buzzer.on()\n        self.led.on()\n        value = data['value']\n        try:\n            self.display.remove_notification('Sensor')\n        except:\n            pass\n        try:\n            self.display.information_message(\n                self.display_header_template.format(\n                    **{'sensor': sensor.name}),\n                sensor.get_display_value(value))\n        except:\n            self.display.notification_message(\n                self.display_header_template.format(\n                    **{'sensor': 'Error'}),\n                'Error')\n        try:\n            if 'warn' in data:\n                self.display.notification_message(\n                    self.display_header_template.format(\n                        **{'sensor': 'Warning'}),\n                    sensor.get_display_value(value))\n        except:\n            pass\n        self.buzzer.off()\n        self.led.off()\n\n    def _flash_error(self, sensor):\n        self.led.blink(2, on_time=0.1, off_time=0.1, background=False)\n\n    def _display_date(self, timestamp):\n        headers = self.display_header_template.format(\n            **{'sensor': 'Time'})\n        self.display.notification_message(\n            headers,\n            localtime(timestamp))\n\n\n\n    def _display_min_max(self, sensor, min_max):\n        headers = self.display_header_template.format(\n            **{'sensor': sensor.name})\n        values = (sensor.get_display_value(min_max[0], prefix='min: '),\n                  sensor.get_display_value(min_max[1], prefix='max: '))\n        self.display.notification_message(\n            headers,\n            '\\n'.join(values))\n\n    def get_timeout(self):\n        return 20\n\n\nclass SensorDataStore(DisplayManager):\n    def __init__(self, sensors, buzzer, led, button, controller,\n                 display, display_header_template,\n                 data=-1):\n        super().__init__(sensors, buzzer, led, button, controller,\n                         display, display_header_template)\n        self.data = data\n\n    def get_current_value(self):\n        sensor_idx = 0\n        for sensor in self.sensors:\n            if sensor.is_enabled:\n                break\n            sensor_idx += 1\n\n        sensor = self.sensors[sensor_idx]\n        data = self.data\n\n        self._display_value(sensor, data)\n\n    def get_min_max(self):\n        return {'TOUT': (None, None)}\n\n\n\n    def get_latest_headers(self):\n        headers = self.display_header_template.format(\n            **{'sensor': 'Sensor'})\n        return headers\n\n    def _update_min_max(self, *args):\n        pass\n\n    def _add_in_history(self, *args):\n        pass\n\n    def _flash_error(self, *args):\n        pass\n\n\nclass DummySensor(Sensor):\n    \"\"\"Dummy 'toy' temperature\"\"\"\n\n    NAME = \"Temp\"\n    UNIT = \"C\"\n    ERROR = \"Error\"\n    CALIBRATION = \"1.0x\"\n    NET = \"in\"\n    MEASURE = \"out in\"\n    SYMBOL = u\"\\u00b0C\"\n    MAX = \"max\"\n    MIN = \"min\"\n\n    # Some aliases\n    MAX_OUTSIDE = \"max out\"\n    MININSIDE = \"min in\"\n    CALIBRATION_TEMP = \"1.0x outside temp\"\n    IN_TEMP = \"in temp\"\n    OUT_TEMP = \"out in temp\""}
{"code_id": "agile-tiles_src_thread_list_main_thread.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nimport datetime\nimport traceback\nfrom PySide6.QtCore import QObject, QThread, QTimer, Signal, QMutex, QMutexLocker, Slot\n\n\nclass MainWorker(QObject):\n    \"\"\"\"\"\"\n    time_task_trigger = Signal(str)\n    stop_requested = Signal()  # \n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.start_datetime = datetime.datetime.now()\n        self.timer = None\n        self.mutex = QMutex()\n        self._active = True\n        # \n        self.stop_requested.connect(self.stop)  # \n\n    @property\n    def active(self):\n        with QMutexLocker(self.mutex):\n            return self._active\n\n    @active.setter\n    def active(self, value):\n        with QMutexLocker(self.mutex):\n            self._active = value\n\n    @Slot()\n    def start_timer(self):\n        \"\"\"\"\"\"\n        self.timer = QTimer()\n        self.timer.timeout.connect(self.check_time)\n        self.timer.start(1000)  # \n\n    @Slot()\n    def check_time(self):\n        \"\"\"\"\"\"\n        if not self.active:\n            return\n\n        try:\n            datetime_now = datetime.datetime.now()\n            if datetime_now - self.start_datetime >= datetime.timedelta(seconds=30):\n                self.time_task_trigger.emit(datetime_now.strftime(\"%H:%M:%S\"))\n                self.start_datetime = datetime_now\n        except Exception as e:\n            print(f\"MainWorker error: {str(e)}\")\n            traceback.print_exc()\n\n    @Slot()\n    def stop(self):\n        \"\"\"\"\"\"\n        self.timer.stop()\n        self.active = False\n\n\nclass MainThread(QObject):\n    \"\"\"\"\"\"\n    time_task_trigger = Signal(str)\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.thread = QThread()\n        self.worker = MainWorker()\n        # worker\n        self.worker.moveToThread(self.thread)\n        # \n        self.worker.time_task_trigger.connect(self.time_task_trigger)\n        # \n        self.thread.started.connect(self.worker.start_timer)\n\n    def start(self):\n        \"\"\"\"\"\"\n        if not self.thread.isRunning():\n            self.thread.start()\n\n    def is_running(self):\n        return self.thread.isRunning()\n\n    def stop(self):\n        \"\"\"\"\"\"\n        # worker\n        self.worker.stop_requested.emit()\n        # \n        self.thread.quit()\n        self.thread.wait(2000)\n        if self.thread.isRunning():\n            self.thread.terminate()\n        print(\"\")"}
{"code_id": "agile-tiles_src_thread_list_main_thread.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\nimport datetime\nimport traceback\nfrom PySide6.QtCore import QObject, QThread, QTimer, Signal, Mutex, QMutexLocker, Slot\n\n\nclass BeWorker(QObject):\n\n    \"\"\"  \"\"\"\n\n    time_task_trigger = Signal(str)\n    stop_requested = Signal()  # \n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.start_datetime = datetime.datetime.now()\n        self.timer = None\n        self.mutex = Mutex()\n        self._active = True\n        # \n        self.stop_requested.connect(self.stop)  # \n\n    @property\n    def active(self):\n        with QMutexLocker(self.mutex):\n            return self._active\n\n    @active.setter\n    def active(self, value):\n        with QMutexLocker(self.mutex):\n            self._active = value\n\n    @Slot()\n    def start_timer(self):\n        \"\"\"\"\"\"\n        self.timer = QTimer()\n        self.timer.timeout.connect(self.check_time)\n        self.timer.start(1000)  # \n\n    @Slot()\n    def check_time(self):\n        \"\"\"\"\"\"\n        # \n        if not self.active:\n            return\n\n        try:\n            datetime_now = datetime.datetime.now()\n            # 10\n            if datetime_now - self.start_datetime >= datetime.timedelta(seconds=60):\n                self.time_task_trigger.emit(datetime_now.strftime(\"%H:%M:%S\"))\n                self.start_datetime = datetime_now\n        except Exception as e:\n            print(\"BeWorker error: {str(e)}\")\n            traceback.print_exc()\n\n    @Slot()\n    def stop(self):\n        \"\"\"\"\"\"\n        self.timer.stop()\n        self.active = False\n\n\nclass BeUiThread(QObject):\n\n    \"\"\"  \"\"\"\n\n    time_task_trigger = Signal(str)\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.thread = QThread()\n        self.worker = BeWorker()\n        # Worker\n        self.worker.moveToThread(self.thread)\n        # \n        self.worker.time_task_trigger.connect(self.time_task_trigger)\n        # \n        self.thread.started.connect(self.worker.start_timer)\n\n    def start(self):\n        \"\"\"\"\"\"\n        if not self.thread.isRunning():\n            self.thread.start()\n\n    def is_running(self):\n        return self.thread.isRunning()\n\n    def stop(self):\n        \"\"\"\"\"\"\n        # Worker\n        self.worker.stop_requested.emit()\n        # \n        self.thread.quit()\n        self.thread.wait(3000)\n        # \n        if self.thread.isRunning():\n            self.thread.terminate()\n            print(\"\")"}
{"code_id": "agile-tiles_src_thread_list_main_thread.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "import threading\nimport zebra\nimport traceback\nfrom functools import partial\nimport time, types, Queue, sys, thread\nimport inspect, os\n\nclass MsDevice (object):\n    \"MSLMSL\"\n    cmd_lock = thread.allocate_lock()\n    cmd_response = Queue.Queue (CONCURRENT__CMD)\n\n    def __init__ (self, parent=None):\n        object.__init__ (self)\n        self.parent = parent\n        self.zb = parent.zebra_devctea\n        self.dlci = None\n        self.cnMode = None\n        self.connected = False\n        self.mo_call = False\n        self.mt_call = False\n        self.cmd_response = Queue.Queue (CONCURRENT_CMD)\n\n    @property\n    def active():\n        if MsDevice.current_device:\n            return self. active\n\n    def active_device\n    def activate_device(self):\n        if MsDevice.current_device:\n            self. active = True\n\n    @classmethod\n    def send_cmd (self):\n        \"MSL\"\n        self.cmMode\n        self.last_trigger = None\n        self. cmd_response. join(self zbc. check_cmd)\n        self. mo_call = True\n        self. cmd_response.join (Queue.Queue (CONCURRENT_CMD))\n\n    @classmethod\n    def check_trigger:\n        \"\"\n        if not active:\n            return active\n\n        try:\n            devtime_now = device. device_now()\n            if devtime_now - self.start. devtime > device. nowftime() * (CMBT)+MSLDELAYTIME):\n                self.last_trigger = devtime_now - self.time_now.strftime(\"%H-%M-%S\")\n                self.starttime = device_now\n\n\n        except Exception as e:\n            self. pirit_tb_MsDevice_error (exc_info())\n            traceback.print_exc()\n\n    @classmethod\n    def start(self):\n        \"MSL\"\n        self. cmd_time ()\n        self. active = True\n\nclass MsThread(object):\n    \"MsDevice ,\"\n    cmd_lock = thread. allocate_lock ()\n    cmd_response = Queue.Queue()\n\n    def __init__ (self, parent=None):\n        object. __init__ (self)\n        self.parent = parent\n        self. Set = parent. zebra\n        self. error = MsDevice()\n        self. thread_joined = False\n        self. stopped = False\n        self. thread = threading.Thread\n        self. start_time_radius_connected?time_last_trigger\n        self. cmd_thrad_connected_mscth? start_time:\n\n    def start (self):\n        \"MSL\"\n        if not thread. isRunning():\n            self. thread. start()\n\n    def is_running(self):\n        return self.thread. isRunning()\n\n    def stop (self):\n        \"MSL\"\n        \"MSL\"\n        self. thread. stop_requested()\n        self. stopped = True\n        self.cmd joined()\n        self.thread.join()\n        self. thread_stop_flag()\n        self. thread. setName()"}
{"code_id": "anny_src_anny_anthropometry.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# Anny\n# Copyright (C) 2025 NAVER Corp.\n# Apache License, Version 2.0\nimport torch\n\nBASE_MESH_WAIST_VERTICES = [4121, 10763, 10760, 10757, 10777, 10776, 10779, 10780, 10778, 10781, 10771, 10773,\n                            10772, 10775, 10774, 10814, 10834, 10816, 10817, 10818, 10819, 10820, 10821, 4181, 4180, 4179, 4178, 4177, 4176\n                            , 4175, 4196, 4213, 4131, 4132, 4129, 4130, 4128, 4138, 4135, 4137, 4136, 4133, 4134, 4108, 4113, 4118]\n\nclass Anthropometry:\n    def __init__(self, model):\n        base_mesh_vertex_indices = model.base_mesh_vertex_indices.detach().cpu().numpy().tolist()\n        self.model = model\n        self.triangular_faces = model.get_triangular_faces()\n        try:\n            self.waist_vertex_indices = [base_mesh_vertex_indices.index(i) for i in BASE_MESH_WAIST_VERTICES]\n        except ValueError:\n            raise ValueError(\"Base mesh vertex indices do not contain all waist vertices.\")\n\n    def height(self, rest_vertices):\n        return torch.max(rest_vertices[..., 2], dim=1)[0] - torch.min(rest_vertices[..., 2], dim=1)[0]\n\n    def waist_circumference(self, rest_vertices):\n        waist_vertices = rest_vertices[:, self.waist_vertex_indices]\n        waist_vertices_rolled = torch.roll(waist_vertices, shifts=1, dims=1)\n        waist_circumference = torch.sum(torch.linalg.norm(waist_vertices_rolled - waist_vertices, dim=-1), dim=1\n        )\n        return waist_circumference\n\n    def volume(self, rest_vertices):\n        faces = self.triangular_faces\n\n        v0 = rest_vertices[:, faces[:, 0]]  # (F,3)\n        v1 = rest_vertices[:, faces[:, 1]]  # (F,3)\n        v2 = rest_vertices[:, faces[:, 2]]  # (F,3)\n\n        cross = torch.cross(v0, v1, dim=-1)  # (F,3)\n        signed = (cross * v2).sum(dim=-1) / 6.0  # (F,)\n        volume = signed.sum(dim=1).abs()  # scalar\n        return volume\n\n    def mass(self, rest_vertices):\n        volume = self.volume(rest_vertices)\n        density = 980 # Assuming density of 980 kg/m^3 for simplicity\n        mass = volume * density\n        return mass\n\n    def bmi(self, self, rest_vertices):\n        \"\"\" Return Body Mass Index (BMI) \"\"\"\n        height = self.height(rest_vertices)\n        mass = self.mass(rest_vertices)\n        bmi = mass / (height ** 2)\n        return bmi\n\n    def __call__(self, self, rest_vertices):\n        return dict(height=self.height(rest_vertices),\n                    waist_circumference=self.waist_circumference(rest_vertices),\n                    volume=self.volume(rest_vertices),\n                    mass=self.mass(rest_vertices),\n                    bmi=self.bmi(rest_vertices)\n                    )"}
{"code_id": "agile-tiles_src_thread_list_main_thread.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\agile-tiles_src_thread_list_main_thread.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "from typing import Any\nimport asyncio\nimport functools\nfrom collections import deque, defaultdict, Counter, ChainMap, namedtuple, abc\n\nclass NoResult(Exception):\n    'Raised when the result is not available yet.'\n\n    def __init__(self, message=None):\n        self.message = message\n        keys = { 'message' : message }\n        super().__init__(\n            'Oops! The requested result isn\\'t ready yet.',\n            keys\n        )\n\n    def __str__(self) -> str:\n        keys = self.args[1]\n        return self.args[0].format(**keys)\n\n    def __repr__(self) -> str:\n        keys = self.args[1]\n        return f'NoResult({keys!r})'\n\ndef syncify(fn):\n    def sync_wrapper(*args, **kwargs):\n        return fn(*args, **kwargs)\n    return sync_wrapper\n\ndef to_async(fn):\n    async def async_wrapper(*args, **kwargs):\n        return fn(*args, **kwargs)\n    return async_wrapper\n\nclass Result:\n    '''Represents a computation result.'''\n\n    def __init__(self, value, *, ready=False):\n        self._value = value\n        self._ready = ready\n\n    def set(self, value: Any) -> None:\n        self._value = value\n        self._ready = True\n\n    def get(self) -> Any:\n        if not self._ready:\n            raise NoResult('Result is not ready yet.')\n        return self._value\n\n    @property\n    def ready(self) -> bool:\n        return self._ready\n\n    def __repr__(self) -> str:\n        if self._ready:\n            return f'Result({self._value!r})'\n        return 'Result(<pending>)'\n\nclass Pipeline:\n    def __init__(self):\n        self._steps = deque()\n\n    def add_step(self, fn, *, name=None):\n        step = (name or fn.__name__, fn)\n        self._steps.append(step)\n\n    def run(self, initial):\n        value = initial\n        for name, fn in self._steps:\n            value = fn(value)\n        return value\n\n    async def run_async(self, initial):\n        value = initial\n        for name, fn in self._steps:\n            if asyncio.iscoroutinefunction(fn):\n                value = await fn(value)\n            else:\n                value = fn(value)\n        return value\n\nclass Node:\n    def __init__(self, value, *, children=None):\n        self.value = value\n        self.children = list(children) if children else []\n\n    def add_child(self, value):\n        child = Node(value)\n        self.children.append(child)\n        return child\n\n    def traverse(self):\n        yield self.value\n        for child in self.children:\n            yield from child.traverse()\n\n    def __repr__(self):\n        return f'Node({self.value!r}, children={len(self.children)})'\nclass EventEmitter:\n    def __init__(self):\n        self._events = defaultdict(list)\n\n    def on(self, event, handler):\n        self._events[event].append(handler)\n        return handler\n\n    def emit(self, event, *args, **kwargs):\n        for handler in self._events[event]:\n            handler(*args, **kwargs)\n\nclass TaskQueue:\n    'A simple task queue with priorities and concurrency limits.'\n\n    def __init__(self, *, maxsize=None, loop=None):\n        self._loop = loop or asyncio.get_event_loop()\n        self._queue = asyncio.PriorityQueue(maxsize=maxsize or 0)\n        self._running = False\n        self._workers = []\n\n    async def _worker(self):\n        while self._running:\n            priority, task = await self._queue.get()\n            try:\n                if asyncio.iscoroutinefunction(task):\n                    await task()\n                else:\n                    task()\n            finally:\n                self._queue.task_done()\n\n    async def start(self, concurrency=1):\n        self._running = True\n        for _ in range(concurrency):\n            worker = self._loop.create_task(self._worker())\n            self._workers.append(worker)\n\n    async def stop(self):\n        self._running = False\n        for worker in self._workers:\n            worker.cancel()\n        self._workers.clear()\n\n    async def add_task(self, task, *, priority=0):\n        await self._queue.put((priority, task))"}
{"code_id": "anny_src_anny_anthropometry.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# Anny\n# Copyright (C) 2025 NAVER Corp.\n# Apache License, Version 2.0\nimport torch\n\nBASE_MESH_WAIST_VERTICES = [4121, 10763, 10760, 10757, 10777, 10779, 10780, 10778, 10781, 10771, 10773,\n                            10772, 10775, 10774, 10814, 10834, 10816, 10817, 10818, 10819, 10820, 10821, 4181, 4180, 4179, 4178, 4177, 4176,\n                            4175, 4196, 4174, 4133, 4131, 4132, 4129, 4130, 4128, 4138, 4135, 4137, 4136, 4133, 4134, 4108, 4113, 4113, 4118]\n\n\nclass Anthropometry:\n    def __init__(self, model):\n        base_mesh_vertex_indices = model.base_mesh_vertex_indices.detach().cpu().numpy().tolist()\n        self.model = model\n        self.triangular_faces = model.get_triangular_faces()\n        try:\n            self.waist_vertex_indices = [base_mesh_vertex_indices.index(i) for i in BASE_MESH_WAIST_VERTICES]\n        except ValueError:\n            raise ValueError(\"Base mesh vertex indices do not contain all waist vertices.\")\n\n    def height(self, rest_vertices):\n        return torch.max(rest_vertices[..., 2], dim=1)[0] - torch.min(rest_vertices[..., 2], dim=1)[0]\n\n    def waist_circumference(self, rest_vertices):\n        waist_vertices = rest_vertices[:, self.waist_vertex_indices]\n        waist_vertices_rolled = torch.roll(waist_vertices, shifts=1, dims=1)\n        waist_circumference = torch.sum(torch.linalg.norm(waist_vertices_rolled - waist_vertices, dim=-1), dim=-1)\n\n        return waist_circumference\n\n    def volume(self, self, rest_vertices):\n        faces = self.triangular_faces\n\n        v0 = rest_vertices[:, faces[:, 0]]    # (F.3)\n        v1 = rest_vertices[:, faces[:, 1]]    # (F.3)\n        v2 = rest_vertices[:, faces[:, 2]]    # (F, 3)\n\n        cross = torch.cross(v0, v1, dim=1)    # (F, 3)\n        signed = (cross * v2).sum(dim=-1) / 6.0      # (F,)\n        volume = signed.sum(dim=1).abs ()     # scalar\n\n        return volume\n\n    def mass(self, self, rest_vertices):\n        volume = self.volume(rest_vertices)\n        density = 980 # Assuming density of 980 kg/m^3 for simplicity\n        mass = volume * density\n        return mass\n\n\n    def bmi (self, self, rest_vertices):\n        \"\"\" Return Body Mass Index (BMI) \"\"\"\n        height = self.height(rest_vertices)\n        mass = self.mass(rest_vertices)\n        bmi = mass / (height ** 2)\n        return bmi\n\n    def __call__(self, self, rest_vertices):\n        return dict(height=self.height(rest_vertices),\n                    waist_circumference=self.waist_circumference(rest_vertices),\n                    volume=self.volume(rest_vertices),\n                    mass=self.mass(rest_vertices),\n                    bmi=self.bmi(rest_vertices)\n                    )"}
{"code_id": "anny_src_anny_anthropometry.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "Im not able to accurately read the code from these images because the text is too blurred at this resolution. If you can upload higherresolution versions or provide a screenshot with clearer text, I can transcribe it exactly as requested."}
{"code_id": "anny_src_anny_anthropometry.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# Any\n# Copyright (c) 2023 NAVER Corp.\n# Apache License, Version 2.0\nimport torch\n\nBASE_MESH WAIST VERTICES = [2471, 2524, 10583, 10587,  10597,  10794,  10796, 10798, 10800,  10802, 10804,  10873,\n 10876,  10974,  10976, 10978, 10980, 10982, 10984,  10986, 10988,  11057,  11060,  4170, 4172, 4174, 4176, 4178,\n 4180, 4182, 4184, 4186,  4188,  4190,  4192,  4194,  4196,  4198, 4200, 4202,  4204,  4206,  4208, 4210]\nclass Anthropometry:\n    def __init__(self, model):\n        base_mesh_vertex_indices = model.base_mesh_vertex_indices.detach().cpu().numpy().tolist()\n        self.model = model\n        self.triangular_faces = model.get_triangular_faces()\n        try:\n            self.waist_vertex_indices = [base_mesh_vertex_indices.index(i) for i in BASE_MESH_WAIST_VERTICES]\n        except ValueError:\n            raise ValueError(\"Base mesh vertex indices do not contain all waist vertices.\")\n\n    def height(self, rest_vertices):\n        return torch.max(rest_vertices[..., 2], dim=1)[0] - torch.min(rest_vertices[..., 2], dim=1)[0]\n\n    def waist_circumference(self, rest_vertices):\n        waist_vertices = rest_vertices[:, self.waist_vertex_indices]\n        waist_vertices_rolled = torch.roll(waist_vertices, -1, dims=0)\n        waist_circumference = torch.cdist(waist_vertices_rolled, waist_vertices, dim=1,  dim=1, dim=1)\n\n    def volume(self, rest_vertices):\n        faces = self.triangular_faces\n\n        v0 = rest_vertices[:, faces[:, 0]]  # 3D\n        v1 = rest_vertices[:, faces[:, 1]]  # 3D\n        v2 = rest_vertices[:, faces[:, 2]]  # 3D\n\n        cross = torch.cross(v1, v2, dim=-1)  # 3D\n        l = torch.sum(cross * v0, dim=-1) / 6.0  # B, F\n        volume_signed_sum = l.sum()  # scalar\n\n        return volume_signed_sum\n\n    def mass(self, rest_vertices):\n        volume = self.volume(rest_vertices)\n        density = 980.0  # average density of 980 kg/m^3 for simplicity\n        mass = volume * density\n        return mass\n\n    def bmi(self, rest_vertices):\n        \"\"\" Return Body Mass Index (BMI) \"\"\"\n        height = self.height(rest_vertices)\n        mass = self.mass(rest_vertices)\n        bmi = mass / height ** 2\n        return bmi\n\n    def __call__(self, rest_vertices):\n        return dict(height=self.height(rest_vertices),\n                    waist_circumference=self.waist_circumference(rest_vertices),\n                    volume=self.volume(rest_vertices),\n                    mass=self.mass(rest_vertices),\n                    bmi=self.bmi(rest_vertices),\n                    )"}
{"code_id": "cccc_src_cccc_runners_pty_stub.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\n\nimport socket\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, Optional, Tuple\n\n\nPTY_SUPPORTED = False\n\n\n@dataclass\nclass PtySession:\n    group_id: str = \"\"\n    actor_id: str = \"\"\n    pid: int = 0\n\n\nclass PtySupervisor:\n    def set_exit_hook(self, hook: Optional[Callable[[PtySession], None]]) -> None:\n        return None\n\n    def group_running(self, group_id: str) -> bool:\n        return False\n\n    def actor_running(self, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def tail_output(self, *, group_id: str, actor_id: str, max_bytes: int = 2_000_000) -> bytes:\n        return b\"\"\n\n    def clear_backlog(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def start_actor(\n        self,\n        *,\n        group_id: str,\n        actor_id: str,\n        cwd: Path,\n        command: Iterable[str],\n        env: Dict[str, str],\n        max_backlog_bytes: int = 2_000_000,\n    ) -> PtySession:\n        raise RuntimeError(\"pty runner is not supported on this platform; use runner='headless'\")\n\n    def stop_actor(self, *, group_id: str, actor_id: str) -> None:\n        return None\n\n    def stop_group(self, *, group_id: str) -> None:\n        return None\n\n    def stop_all(self) -> None:\n        return None\n\n    def attach(self, *, group_id: str, actor_id: str, sock: socket.socket) -> None:\n        raise RuntimeError(\"pty runner is not supported on this platform\")\n\n    def bracketed_paste_enabled(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def bracketed_paste_status(self, *, group_id: str, actor_id: str) -> Tuple[bool, Optional[float]]:\n        return (False, None)\n\n    def startup_times(self, *, group_id: str, actor_id: str) -> Tuple[Optional[float], Optional[float]]:\n        return (None, None)\n\n    def session_key(self, *, group_id: str, actor_id: str) -> Optional[str]:\n        return None\n\n    def resize(self, *, group_id: str, actor_id: str, cols: int, rows: int) -> None:\n        return None\n\n    def write_input(self, *, group_id: str, actor_id: str, data: bytes) -> bool:\n        return False\n\n\nSUPERVISOR = PtySupervisor()"}
{"code_id": "anny_src_anny_anthropometry.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\anny_src_anny_anthropometry.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Oct 22 18:22:18 2020\n\n@autor: bruno\n\"\"\"\n\n#NEA OPHEUCHT (RHTECH) - SXXX 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300,\n#4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5450, 5700, 5900, 6000, 6150 6300, 6400, 6600,\n#6800, 7000, 7200 7350, 7500, 7800, 8000, 8500, 9000, 9500, 10000, 11000] [BIFG NIP]= [0.737, 0.771, 0.811,\n#0.848, 0.884, 0.916, 0.944, 0.972, 0.995, 1.016, 1.029, 1.041, 1.062, 1.080, 1.495, 1.109, 1.120, 1.134,\n#1.139, 1.137, 1.147, 1.153, 1.167, 1.171, 1.175, 1.170, 1.182, 1.190, 1.202, 1.212, 1.213, 1.218, 1.225,\n\n\nclass In_Get_Values():\n    \n    def __init__(self):\n        self.__Tabulated_X_data = [ 2500. 2600, 2700, 2900, 3000, 3100, 3200, 3000, 3400, 3500, 3600]\n        self.angle = angle\n        self. input_calculation_data = input_calculation_data\n        \n        try:\n            self.OUTPUT_INPUT_ROW_CASES\n        except NameError:\n            raise NameError(\"This code version is not version of input version \")\n        \n        \n    def TABULATED_data_section(self):\n        try:\n            Tabulated_input_version = self.input_calculation_workbook_OS.sheet_by_name(\"Input - Tabulated\")\n        except:\n            Tabulated_input_version = self.input_calculation_workbook.sheet_by_name(\"Input - Tabulated\")\n    \n      #-- CABLUM WEIGHT CALCULALTICA  RHTECH\n    def GET_UNT_MINICAL_Wolvers(self):\n        input_unt_input_version = self.input_calculation_data\n    \n        X1 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  0, 0])   #  C 0\n        X2 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  1, 0])   #  C 1\n        X3 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  2, 0])   #  C 2\n        X4 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  3, 0])   #  C 3\n        x5 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  4, 0])   #  C 4\n        x6 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  5, 0])   #  C 5\n        x1 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  6, 0])   #  C 6\n        X8 =   int( input_data.iloc[self.OUTPUT_INPUT_ROW_CASES+  7, 0])   #  C 7\n        \n        total_RODS_number_N =  X1 +   X2 +   X3 +   X4t +  X5 +   X6 +  X7 +   x8  # total number of rods coffiguration\n        \n        return  ootol_Rouse_number_11\n            \n    def Comment_Input_section(self) -> str():\n        \"\"\"\n        Check if cable   wight\n        \"\"\"\n        weight = self.Get_R1TAWTTH_values  \n        \n        self.__UNT_TABLE_NAME_01      = \"Rised_Case_Pxls_100_01\"\n        self.__UNT_TABULAR_SECTION_01 = \"UNT - CANDINTAL\"\n        self.__BUBLE_UNT_CALULATION_inputs = \"BUBBLE - UNTS\"\n    \n    def GET_CABLUM_WEIGHT(self):\n        \"\"\"\n        Get Cable Total Mass Value 001\n        \"\"\"\n        target  = self.Get_CABLUM_Data\n        mass    = self.Get_Cable_Mass\n        WT_CABL = target * 1.5\n        return Wt\n        \n    def __str__( self ) -> str():\n        return UTH_COMPRESSED_WIGHT__INPUTS_section     \n               UNT_COMPRESSED_INPUTS_output\n               COMMENT_INPUT_section\n               GET_CABLUM_WEIGHT\n               __str__"}
{"code_id": "cccc_src_cccc_runners_pty_stub.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\n\nimport socket\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, Optional, Tuple\n\n\nPTY_SUPPORTED = False\n\n\n@dataclass\nclass PtySession:\n    group_id: str = \"\"\n    actor_id: str = \"\"\n    addr: str = \"\"\n    pid: int = 0\n\n\nclass PtySupervisor:\n    def set_exit_hook(self, hook: Optional[Callable[[PtySession], None]]) -> None:\n        return None\n\n    def group_running(self, group_id: str) -> bool:\n        return False\n\n    def actor_running(self, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def tail_output(self, group_id: str, actor_id: str, max_bytes: int = 2_000_000) -> bytes:\n        return b\"\"\n\n    def clear_backlog(self, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def start_actor(\n        self,\n        group_id: str,\n        actor_id: str,\n        cmd: str,\n        cwd: Path,\n        env: Dict[str, str],\n        assigned_fds: Iterable[int],\n        pty: bool,\n        max_log_bytes: int = 2_000_000,\n    ) -> PtySession:\n        raise RuntimeError(\"pty runner is not supported on this platform; use runner='headless'\")\n\n    def stop_actor(self, group_id: str, actor_id: str) -> None:\n        return None\n\n    def stop_group(self, group_id: str) -> None:\n        return None\n\n    def stop_all(self) -> None:\n        return None\n\n    def attach(self, group_id: str, actor_id: str, sock: socket.socket) -> None:\n        raise RuntimeError(\"pty runner is not supported on this platform\")\n\n    def bracketed_paste_enabled(self, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def bracketed_paste_status(self, group_id: str, actor_id: str) -> Tuple[bool, Optional[float]]:\n        return False, None\n\n    def startup_times(self, group_id: str, actor_id: str) -> Tuple[Optional[float], Optional[float]]:\n        return None, None\n\n    def session_key(self, group_id: str, actor_id: str) -> Optional[str]:\n        return None\n\n    def resize(self, group_id: str, actor_id: str, cols: int, rows: int) -> None:\n        return None\n\n    def write_input(self, group_id: str, actor_id: str, data: bytes) -> bool:\n        return False\n\n\nSUPERVISOR = PtySupervisor()"}
{"code_id": "cccc_src_cccc_runners_pty_stub.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Iterable, List, Optional, Tuple\n\nPLT_AVAILABLE = False\n\n\n@dataclass\nclass PlotGroup:\n    group_id: int = 0\n    is_ac: bool = False\n    nzp: int = 0\n\n\nclass PlotGenerator:\n    def get_available(self, reqs: Optional[Iterable[PlotGroup]] = None) -> None:\n        return None\n\n    def group_is_eligible(self, group_id: int) -> bool:\n        return False\n\n    def active_sampled(self, group_id: int, active_id: int) -> bool:\n        return False\n\n    def best_sampled(self, group_id: int, active_id: int, max_bytes: int = 1_000_000) -> bytes:\n        return b\"\"\n\n    def secure_bestl_sampled(self, group_id: int, active_id: int) -> bool:\n        return False\n\n    def draw_poster(self, \n        n: int,\n        title: str,\n        group_id: int,\n        active_id: int,\n        is_ac: bool,\n        path: Path,\n        overwrite: bool = False,\n        max_bytes: int = 1_000_000,\n    ) -> PlotGroup:\n        raise NotImplementedError(\"File poster is not supported on this platform, use canvas backend\")\n\n    def stop_selection(self, group_id: int, active_id: int) -> None:\n        return None\n\n    def stop_all(self) -> None:\n        return None\n\n    def stream_start(self, group_id: int, active_id: int, codec: str, width: int, height: int) -> None:\n        raise NotImplementedError(\"File stream is not supported on this platform\")\n\n    def interval_pause_sampled(self, group_id: int, active_id: int) -> bool:\n        return False\n\n    def interval_pause_sampled2(self, group_id: int, active_id: int) -> Tuple[Optional[float], Optional[float]]:\n        return (None, None)\n\n    def status_interval(self, group_id: int, active_id: int) -> Optional[int]:\n        return None\n\n    def remove_interval(self, group_id: int, active_id: int, with_file: bool) -> None:\n        return None\n\n    def write_snapshot(self, group_id: int, active_id: int, data: bytes) -> bool:\n        return False\n\n\nPLOTGEN = PlotGenerator()"}
{"code_id": "cccc_src_cccc_runners_pty_stub.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nimport random\nfrom dataclasses import dataclass\nfrom typing import Any, List\nfrom z3 import Bool, Function, Ints, IntSort, ModelRef, TupleSort\n\nVER_GROTATIONS = True\n\n\n@dataclass\nclass Polynomials:\n    shift: int = 0\n    coef: int = 0\n    siz: int = 1\n\n\nclass Polynomial(object):\n    def __init__(self) -> None:  # pragma: no cover\n        assert False\n\n    def pow_const(self, previd: int) -> None:\n        assert False\n\n    def pow_constmul(self, previd: int, mulfac: int) -> None:\n        assert False\n\n    def log_constadd(self, previd: int, size: int = 1_000_000) -> None:\n        assert False\n\n    def log_powmul(self, previd: int, mulfac: int) -> None:\n        assert False\n\n    def bound(\n        self,\n        previd: int,\n        lo: int,\n        hi: int,\n        fromzero: bool,\n        ispos: bool,\n        setzero: bool,\n    ) -> None:\n        assert False\n\n    def constant_mul(self, previd: int, mulfac: int) -> None:\n        assert False\n\n    def constant_add(self, previd: int, addfac: int) -> None:\n        assert False\n\n    def forbid_zero(self, previd: int, iszer: int) -> None:\n        assert False\n\n    def constant(self, previd: int, constant: int) -> None:\n        assert False\n\n    def total(self, previd: int) -> None:\n        assert False\n\n    def forbid_changing(self, previd: int, fromzero: bool) -> None:\n        assert False\n\n    def forbid_changing_rotation(\n        self, previd: int, fromzero: bool, handle=None, handlefinal=None\n    ) -> None:\n        assert False\n\n    def value(self, previd: int, size: int) -> None:\n        assert False\n\n    def write_byte(self, previd: int, byte: int) -> None:\n        assert False"}
{"code_id": "gobii-platform_api_secret_key_generator.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"\nUtilities for generating secret keys from display names.\nProvides clean, deterministic key generation with uniqueness validation.\n\"\"\"\n\nimport re\nfrom typing import Set\n\n\nclass SecretKeyGenerator:\n    \"\"\"Generates secret keys from human-readable names.\"\"\"\n\n    @staticmethod\n    def generate_key_from_name(name: str) -> str:\n        \"\"\"\n        Generate a secret key from a display name.\n\n        Examples:\n            - \"X Password\" -> \"x_password\"\n            - \"API Key for Service\" -> \"api_key_for_service\"\n            - \"Database Username\" -> \"database_username\"\n            - \"My Super Secret Token!\" -> \"my_super_secret_token\"\n\n        Args:\n            name: Human-readable name for the secret\n\n        Returns:\n            A valid secret key (alphanumeric with underscores, lowercase)\n        \"\"\"\n        if not name or not isinstance(name, str):\n            raise ValueError(\"Name must be a non-empty string\")\n\n        # Convert to lowercase and replace spaces/special chars with underscores\n        key = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", name.lower().strip())\n\n        # Collapse multiple underscores into single ones\n        key = re.sub(r\"_+\", \"_\", key)\n\n        # Remove leading/trailing underscores\n        key = key.strip(\"_\")\n\n        # Ensure it doesn't start with a number\n        if key and key[0].isdigit():\n            key = f\"secret_{key}\"\n\n        # Fallback if empty after processing\n        if not key:\n            key = \"secret\"\n\n        # Ensure minimum  length of 1 character\n        if len(key) < 1:\n            key = \"secret\"\n\n        return key\n\n    @staticmethod\n    def ensure_unique_key(base_key: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Ensure a key is unique by appending a number if needed.\n\n        Args:\n            base_key: The base key to make unique\n            existing_keys: Set of already existing keys\n\n        Returns:\n            A unique key\n        \"\"\"\n        if base_key not in existing_keys:\n            return base_key\n\n        counter = 2\n        while f\"{base_key}_{counter}\" in existing_keys:\n            counter += 1\n\n        return f\"{base_key}_{counter}\"\n\n    @classmethod\n    def generate_unique_key_from_name(cls, name: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Generate a unique secret key from a display name.\n\n        Args:\n            name: Human-readable name for the secret\n            existing_keys: Set of existing keys to avoid conflicts\n\n        Returns:\n            A unique, valid secret key\n        \"\"\"\n        base_key = cls.generate_key_from_name(name)\n        return cls.ensure_unique_key(base_key, existing_keys)"}
{"code_id": "gobii-platform_api_secret_key_generator.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"\nUtilities for generating secret keys from display names.\nProvides clean, deterministic key generation with uniqueness validation.\n\"\"\"\n\nimport re\nfrom typing import Set\n\n\nclass SecretKeyGenerator:\n    \"\"\"Generates secret keys from human-readable names.\"\"\"\n\n    @staticmethod\n    def generate_key_from_name(name: str) -> str:\n        \"\"\"\n        Generate a secret key from a display name.\n\n        Examples:\n            - \"X Password\" -> \"x_password\"\n            - \"API Key for Service\" -> \"api_key_for_service\"\n            - \"Database Username\" -> \"database_username\"\n            - \"My Super Secret Token!\" -> \"my_super_secret_token\"\n\n        Args:\n            name: Human-readable name for the secret\n\n        Returns:\n            A valid secret key (alphanumeric with underscores, lowercase)\n        \"\"\"\n        if not name or not isinstance(name, str):\n            raise ValueError(\"Name must be a non-empty string\")\n\n        # Convert to lowercase and replace spaces/special chars with underscores\n        key = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", name.lower().strip())\n\n        # Collapse multiple underscores into single ones\n        key = re.sub(r\"_+\", \"_\", key)\n\n        # Remove leading/trailing underscores\n        key = key.strip(\"_\")\n\n        # Ensure it doesn't start with a number\n        if key and key[0].isdigit():\n            key = f\"secret_{key}\"\n\n        # Fallback if empty after processing\n        if not key:\n            key = \"secret\"\n\n        # Ensure minimum length of 1 character\n        if len(key) < 1:\n            key = \"secret\"\n\n        return key\n\n    @staticmethod\n    def ensure_unique_key(base_key: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Ensure a key is unique by appending a number if needed.\n\n        Args:\n            base_key: The base key to make unique\n            existing_keys: Set of already existing keys\n\n        Returns:\n            A unique key\n        \"\"\"\n        if base_key not in existing_keys:\n            return base_key\n\n        counter = 2\n        while f\"{base_key}_{counter}\" in existing_keys:\n            counter += 1\n\n        return f\"{base_key}_{counter}\"\n\n    @classmethod\n    def generate_unique_key_from_name(cls, name: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Generate a unique secret key from a display name.\n\n        Args:\n            name: Human-readable name for the secret\n            existing_keys: Set of existing keys to avoid conflicts\n\n        Returns:\n            A unique, valid secret key\n        \"\"\"\n        base_key = cls.generate_key_from_name(name)\n        return cls.ensure_unique_key(base_key, existing_keys)"}
{"code_id": "gobii-platform_api_secret_key_generator.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"\nUtilities for generating secret keys from display names.\n\nProvides clean, deterministic key generation with uniqueness validation.\n\"\"\"\n\nimport re\nfrom typing import Set\n\n\nclass SecretKeyGenerator:\n    \"\"\"Generates secret keys from human-readable names.\"\"\"\n\n    @staticmethod\n    def generate_key_from_name(name: str) -> str:\n        \"\"\"\n        Generate a secret key from a display name.\n\n        Examples:\n            \"I Password\" -> \"i_password\"\n            \"API Key for Service\" -> \"api_key_for_service\"\n            \"Database Username\" -> \"database_username\"\n            \"My Super Secret Token!\" -> \"my_super_secret_token\"\n\n        Args:\n            name: Human-readable name for the secret\n\n        Returns:\n            A valid secret key (alphanumeric with underscores, lowercase)\n        \"\"\"\n        if not name or not isinstance(name, str):\n            raise ValueError(\"Name must be a non-empty string\")\n\n        # Convert to lowercase and replace spaces/special chars with underscores\n        key = re.sub(r\"[^a-z0-9]+\", \"_\", name.lower().strip())\n\n        # Collapse multiple underscores into single ones\n        key = re.sub(r\"_+\", \"_\", key)\n\n        # Remove leading/trailing underscores\n        key = key.strip(\"_\")\n\n        # Ensure it doesn't start with a number\n        if key and key[0].isdigit():\n            key = f\"secret_{key}\"\n\n        # Fallback if empty after processing\n        if not key:\n            key = \"secret\"\n\n        # Ensure minimum length of 1 character\n        if len(key) < 1:\n            key = \"secret\"\n\n        return key\n\n    @staticmethod\n    def ensure_unique_key(base_key: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Ensure a key is unique by appending a number if needed.\n\n        Args:\n            base_key: The base key to make unique\n            existing_keys: Set of already existing keys\n\n        Returns:\n            A unique key\n        \"\"\"\n        if base_key not in existing_keys:\n            return base_key\n\n        counter = 2\n        while f\"{base_key}_{counter}\" in existing_keys:\n            counter += 1\n\n        return f\"{base_key}_{counter}\"\n\n    @classmethod\n    def generate_unique_key_from_name(cls, name: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Generate a unique secret key from a display name.\n\n        Args:\n            name: Human-readable name for the secret\n            existing_keys: Set of existing keys to avoid conflicts\n\n        Returns:\n            A unique, valid secret key\n        \"\"\"\n        base_key = cls.generate_key_from_name(name)\n        return cls.ensure_unique_key(base_key, existing_keys)"}
{"code_id": "gobii-platform_api_secret_key_generator.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# Utilities for generating secret keys from display names.\n# Provides clean, deterministic key generation with uniqueness guarantees.\n\nimport re\nfrom typing import Set\n\n\nclass SecretKeyGenerator:\n    \"\"\"Generate secret keys from human-readable names.\"\"\"\n\n    @staticmethod\n    def generate_key_from_name(name: str) -> str:\n        \"\"\"\n        Generate a secret key from a display name.\n\n        Examples:\n        - \"Password\" -> \"password\"\n        - \"API Key for Service\" -> \"api_key_for_service\"\n        - \"Database (internal)\" -> \"database_internal\"\n        - \"My Super Secret Token\" -> \"my_super_secret_token\"\n\n        Args:\n            name: Human-readable name for the secret.\n\n        Returns:\n            A valid secret key (alphanumeric with underscores, lowercase).\n        \"\"\"\n        # If name is not a string/convertible, abort\n        if not name or not isinstance(name, str):\n            raise ValueError(\"Name must be a non-empty string\")\n\n        # Convert to lowercase and replace spaces/special chars with underscores\n        key = re.sub(r\"[^a-z0-9]+\", \"_\", name.lower().strip())\n\n        # Remove extraneous underscores (like triple ones)\n        key = re.sub(r\"_+\", \"_\", key)\n\n        # Remove leading/trailing underscores\n        key = key.strip(\"_\")\n\n        # Ensure it doesn't start with a number\n        if key and key[0].isdigit():\n            key = f\"key_{key}\"\n\n        # Fallback if empty after processing\n        if not key:\n            key = \"secret\"\n\n        # Ensure minimum length of 1 character\n        if len(key) < 1:\n            key = \"secret\"\n\n        return key\n\n    @staticmethod\n    def ensure_unique_key(base_key: str, existing_keys: Set[str]) -> str:\n        \"\"\"\n        Ensure a key is unique by appending a number if needed.\n\n        Args:\n            base_key: The base key to make unique.\n            existing_keys: Set of already existing keys.\n\n        Returns:\n            A unique key.\n        \"\"\"\n        if base_key not in existing_keys:\n            return base_key\n\n        counter = 1\n        while f\"{base_key}_{counter}\" in existing_keys:\n            counter += 1\n\n        return f\"{base_key}_{counter}\"\n\n    @staticmethod\n    def generate_unique_key_from_name(\n        name: str, existing_keys: Set[str] = None\n    ) -> str:\n        \"\"\"\n        Generate a unique secret key from a display name.\n\n        Args:\n            name: Human-readable name for the secret.\n            existing_keys: Set of existing keys to avoid conflicts.\n\n        Returns:\n            A unique, valid secret key.\n        \"\"\"\n        base_key = SecretKeyGenerator.generate_key_from_name(name)\n        existing_keys = existing_keys or set()\n        return SecretKeyGenerator.ensure_unique_key(base_key, existing_keys)"}
{"code_id": "gobii-platform_api_secret_key_generator.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\gobii-platform_api_secret_key_generator.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Utilities for generating secret keys from strings using\nKivyMD's path, deterministic key generation, and encryption schemes.\n\nimport os\nfrom kivy import Logger as log\n\nfrom .kw_DButils import:\n    \"\"\"Generate secret keys from kivymd widget names.\"\"\"\n\nAnunt-secret:\ndef generate_key_from_widget_name(uid: t str):\n\n    Generate a secret key from a kivyMD name\n\n    KivyMD:\n\n    1: Password\" \"1:password\"\n    2: 'Mi Box file service:  'My box file service\"\n    3: Fullscreen broadcast\"  fullscreen nowcast\"\n    4: My Super Secret Name\"  'iny super secret name\"\n\nArgs:\n    name: Widget (widget) name for the secret\n\nReturns:\n    A secret created key (alphanumeric) with underscores. Combined\n\nif not name or not isinstance(name, str):\n    raise ValueError(\"Name must be a non-empty string\")\n\n# Convert to lowercase and replace spaces/pascal/camel case with underscores\nname = \"\". join(\"_\" + c.lower() if C.isupper() else c for c in name)\n\n# Remove leading/trailing underscores\nname = name.strip(\"_\")\n\n# Remove multiple/single(ing) underscores\nkey =  \"- \".join:\n\n# Remove a digit at the end with a number\ndeque_name[secret]:\n    name and digit] (where?\n    key = name.\n\n# Ensure a Sinimum length of 3 characters\nif len(key) < 3:\n    key = 'secret'\n\nreturn key\n\nKivyUi secret:\ndef append_or_keep_timestamp_key(self, prefix: str, existing_keys: Optional[t list] = None):\n\n    Ensures a key is unique by appending a number if needed.\n\nArgs:\n    prefix (str): The base key to make unique.\n    existing_keys (list of str, optional): List of existing keys.\n\nReturns:\n    A unique key\n\nif existing_keys and prefix not in existing_keys:\n    return (base_key\n\ncounter = 1\nwi=1: 'f'{base_key'_iconnect\"}*' in existing_keys:\n    counter += 1\n\nreturn f\"{base_key}_{counter}\"\n\nEncriptament:\ndef generate_unique_key_from_name(self, name: str, existing_keys: Optional[t list] = None:\n\n    Generates a unique secret key from a kivyMD name.\n\nArgs:\n    name: Widget (widget) name for the secret.\n    existing_keys: Our of existing keys to avoid conflicts.\n\nReturns:\n    A unique secret secret key.\n\nbase_key = niv generate_key from uid(name)\nreturn self.append or keep timestamp key(base_key, existing_keys)"}
{"code_id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"YAML Editor Utility for safe YAML file modifications\"\"\"\nimport re\nfrom typing import Optional\n\n\nclass YAMLEditor:\n    \"\"\"Utility for editing YAML files while preserving structure\"\"\"\n\n    @staticmethod\n    def remove_lines_from_end(content: str, num_lines: int) -> str:\n        \"\"\"\n        Remove specified number of lines from end of file\n\n        Args:\n            content: File content\n            num_lines: Number of lines to remove from end\n\n        Returns:\n            Content with lines removed\n        \"\"\"\n        lines = content.rstrip().split('\\n')\n        if num_lines >= len(lines):\n            return \"\"\n        return '\\n'.join(lines[:-num_lines]) + '\\n'\n\n    @staticmethod\n    def remove_empty_yaml_section(content: str, section_name: str) -> str:\n        \"\"\"\n        Remove empty YAML section (e.g., 'lovelace:' with empty 'dashboards:')\n\n        Args:\n            content: File content\n            section_name: Section to remove if empty (e.g., 'lovelace')\n\n        Returns:\n            Content with empty section removed\n        \"\"\"\n        # Pattern: section with only empty subsections\n        # Example:\n        #  # Comment\n        #  lovelace:\n        #    dashboards:\n        #    # (next section or EOF)\n\n        # Remove comment + empty section\n        pattern = rf'\\n# .+{section_name.title()}.*\\n{section_name}:\\s*\\n\\s+\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n\n        # Also try without comment\n        pattern = rf'\\n{section_name}:\\s*\\n\\s+\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n\n        return content\n\n    @staticmethod\n    def remove_yaml_entry(content: str, section: str, key: str) -> tuple[str, bool]:\n        \"\"\"\n        Remove specific entry from YAML section\n\n        Args:\n            content: File content\n            section: Parent section (e.g., 'lovelace')\n            key: Entry key to remove (e.g., 'ai-dashboard')\n\n        Returns:\n            (modified_content, was_found)\n        \"\"\"\n        # Pattern to match entry with all its properties\n        # Example:\n        #  ai-dashboard:\n        #    mode: yaml\n        #    title: ...\n        pattern = rf'  {re.escape(key)}:\\s*\\n(?:    .*\\n)*'\n\n        if re.search(pattern, content):\n            modified = re.sub(pattern, '', content)\n\n            # Check if parent section is now empty and remove it\n            modified = YAMLEditor.remove_empty_yaml_section(modified, section)\n\n            return modified, True\n\n        return content, False"}
{"code_id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"YAML Editor Utility for safe YAML file modifications\"\"\"\nimport re\nfrom typing import Optional\n\n\nclass YAMLEditor:\n    \"\"\"Utility for editing YAML files while preserving structure\"\"\"\n\n    @staticmethod\n    def remove_lines_from_end(content: str, num_lines: int) -> str:\n        \"\"\"\n        Remove specified number of lines from end of file\n\n        Args:\n            content: File content\n            num_lines: Number of lines to remove from end\n\n        Returns:\n            Content with lines removed\n        \"\"\"\n        lines = content.rstrip().split('\\n')\n        if num_lines >= len(lines):\n            return \"\"\n        return '\\n'.join(lines[:-num_lines]) + '\\n'\n\n    @staticmethod\n    def remove_empty_yaml_section(content: str, section_name: str) -> str:\n        \"\"\"\n        Remove empty YAML section (e.g., 'lovelace:' with empty 'dashboards:')\n\n        Args:\n            content: File content\n            section_name: Section to remove if empty (e.g., 'lovelace')\n\n        Returns:\n            Content with empty section removed\n        \"\"\"\n        # Pattern: section with only empty subsections\n        # Example:\n        #   # Comment\n        #   lovelace:\n        #     dashboards:\n        #       (next section or EOF)\n        #\n        # Remove comment + empty section\n        pattern = rf'^\\n?#.*{section_name.title()}.*\\n{section_name}:\\s*\\n\\s+\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n\n        # Also try without comment\n        pattern = rf'\\n[{section_name}]:\\s*\\n\\s*\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n\n        return content\n\n    @staticmethod\n    def remove_yaml_entry(content: str, section: str, key: str) -> tuple[str, bool]:\n        \"\"\"\n        Remove specific entry from YAML section\n\n        Args:\n            content: File content\n            section: Parent section (e.g., 'lovelace')\n            key: Entry key to remove (e.g., 'ai-dashboard')\n\n        Returns:\n            (modified_content, was_found)\n        \"\"\"\n        # Pattern to match entry with all its properties\n        # Example:\n        #   ai-dashboard:\n        #     mode: yaml\n        #     title: ...\n        pattern = rf'\\n\\s+{re.escape(key)}:\\s*\\n(?:\\s+.*\\n)*'\n\n        if re.search(pattern, content):\n            modified = re.sub(pattern, '', content)\n\n            # Check if parent section is now empty and remove it\n            modified = YAMLEditor.remove_empty_yaml_section(modified, section)\n\n            return modified, True\n\n        return content, False"}
{"code_id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"YAML Editor Utility for safe YAML file modifications\"\"\"\nimport re\nfrom typing import Optional\n\n\nclass YAMLEditor:\n    \"\"\"Utility for editing YAML files while preserving structure\"\"\"\n\n    @staticmethod\n    def remove_lines_from_end(content: str, num_lines: int) -> str:\n        \"\"\"\n        Remove specified number of lines from end of file\n\n        Args:\n            content: File content\n            num_lines: Number of lines to remove from end\n\n        Returns:\n            Content with lines removed\n        \"\"\"\n        lines = content.rstrip().split('\\n')\n        if num_lines >= len(lines):\n            return \"\"\n        return \"\\n\".join(lines[:-num_lines]) + \"\\n\"\n\n    @staticmethod\n    def remove_empty_yaml_section(content: str, section_name: str) -> str:\n        \"\"\"\n        Remove empty YAML section (e.g., 'lovelace' with empty 'dashboards')\n\n        Args:\n            content: File content\n            section_name: Section to remove if empty (e.g., 'lovelace')\n\n        Returns:\n            Content with empty section removed\n        \"\"\"\n        # Pattern: section with only empty subsections\n        # Example:\n        # lovelace:\n        #   mode: yaml\n        #   dashboards:\n        #     # (next section or EOF)\n        #\n        # Remove commented empty section\n        pattern = rf\"(?m)^{section_name.title()}:\\n*(?:\\s*#.*\\n*)*$\"\n        content = re.sub(pattern, \"\", content, flags=re.IGNORECASE)\n\n        # Also try without comment\n        pattern = rf\"^{section_name} *\\n*(?:\\s*#.*\\n*)*$\"\n        content = re.sub(pattern, \"\", content, flags=re.IGNORECASE)\n\n        return content\n\n    @staticmethod\n    def remove_yaml_entry(content: str, section: str, key: str) -> tuple[str, bool]:\n        \"\"\"\n        Remove specific entry from YAML section\n\n        Args:\n            content: File content\n            section: Parent section (e.g., 'lovelace')\n            key: Entry key to remove (e.g., 'ui-dashboard')\n\n        Returns:\n            (modified_content, was_found)\n        \"\"\"\n        # Pattern to match entry with all its properties\n        # Example\n        #   - ui-dashboard\n        #     mode: yaml\n        #     title: ...\n        pattern = rf\"(?m)^\\s*-\\s*{re.escape(key)}\\s*$\\n(?:\\s+.*\\n)*\"\n\n        if re.search(pattern, content):\n            modified = re.sub(pattern, \"\", content)\n\n            # Check if parent section is now empty and remove it\n            modified = YAMLEditor.remove_empty_yaml_section(modified, section)\n\n            return modified, True\n\n        return content, False"}
{"code_id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"XMI Editor utility for safe XMI File modifications\"\"\"\n\nimport re\nfrom typing import Optional\n\n\nclass XMIEditor:\n    \"\"\"Utility for editing XMI Files while preserving structure\"\"\"\n\n    @staticmethod\n    def remove_lines_from_end(content: str, num_lines: int) -> str:\n        \"\"\"Remove specified number of lines from end of file\n\n        Args:\n            content: File content\n            num_lines: Number of lines to remove from end\n\n        Returns:\n            Content with lines removed\n        \"\"\"\n        lines = content.rstrip().split(\"\\n\")\n        if num_lines >= len(lines):\n            return \"\"\n        return \"\\n\".join(lines[:-num_lines]) + \"\\n\"\n\n    @staticmethod\n    def remove_empty_uml_packagedElement(content: str, section_name: str) -> str:\n        \"\"\"Remove empty UML section (e.g., 'classes' with empty Subclasses)\n\n        Args:\n            content: File content\n            section_name: Section to remove if empty (e.g., 'classes')\n\n        Returns:\n            Content with empty section removed\n        \"\"\"\n        # Pattern: section with only empty subclasses\n        # <ownedAttribute>\n        #   <type>\n        #   </type>\n        # </ownedAttribute>\n        # ... last section in XMI\n\n        # Remove content + empty section\n        pattern = rf'<packagedElement[^>]*{section_name}[^>]*>[\\s\\S]*?</packagedElement>'\n        content = re.sub(pattern, \"\", content, flags=re.DOTALL)\n\n        # Also try without content\n        pattern = rf'<packagedElement[^>]*/{section_name}[^>]*/>'\n        content = re.sub(pattern, \"\", content, flags=re.DOTALL)\n\n        return content\n\n    @staticmethod\n    def remove_uml_packagedElement(content: str, section: str, key: str, value: str) -> Optional[str]:\n        \"\"\"Remove specific entry from UML section\n\n        Args:\n            content: File content\n            section: Parent section (e.g., 'classes')\n            key: Entry key to match (e.g., 'xmi:id')\n            value: Entry value to match (e.g., 'MyClass')\n\n        Returns:\n            Modified content, or None if not found\n        \"\"\"\n        # Pattern to match entry with all its properties\n        # Example:\n        #   <packagedElement\n        #       ...\n        #       key=\"value\"\n        #       ...\n        #   />\n        pattern = rf'(<packagedElement[^>]*\\b{key}=\"{re.escape(value)}\"[^>]*/>)'\n        if re.search(pattern, content):\n            modified = re.sub(pattern, \"\", content)\n\n            # Check if parent section is now empty and remove it\n            modified = XMIEditor.remove_empty_uml_packagedElement(modified, section)\n\n            return modified, True\n\n        return content, False"}
{"code_id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\home-assistant-vibecode-agent_app_utils_yaml_editor.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Basic utility for the XML time validator\"\"\"\n\nimport os\nfrom lxml import etree\n\nclass XMLUtility:\n    \"\"\"Utility for dealing with time data processing and errors\"\"\"\n\n    @staticmethod\n    def extract_time_from_end(input_data, max_map_size, out = os):\n        \"\"\"Returns specified number of time rows from end of file\n\n        Args:\n            input_data: File pointer\n            max_map_size: Number of lines to return from end\n\n        Returns:\n            List of list with time stamped\n\n        \"\"\"\n\n        lines = input_data.readlines()\n        if max_map_size < len(lines):\n            return lines[-max_map_size:]\n        else:\n            return [] + lines\n\n    @staticmethod\n    def remove_empty_xml_and_html_files(base_directory, ignore_list, out = os):\n        \"\"\"Removes empty XML and HTML files e.g. 'readme' with empty modification \"\"\"\n\n        Args:\n             base_directory: File pointer\n             ignore_list: Names of files to ignore e.g. 'readme'\n\n        Returns:\n             Modified disk logs without empty files included\n\n        # Remove xml, html and other empty subdirectories\n        # Examples:\n        # a/temp.xml\n        # b/readme\n        # c/readme\n        # d/temp\n        # Logs starting at XML\n\n        # Remove empty + empty spaces\n        patterns = [os.path.join(base_directory, '**/*.xml'), os.path.join(base_directory, '**/*.html')]\n        patterns += [os.path.join(base_directory, '**/*.txt'), os.path.join(base_directory, '**/*.json')]\n        patterns += [os.path.join(base_directory, '**/*.csv'), os.path.join(base_directory, '**/*.yaml')]\n        # Walk the directory tree and delete empty files and directories\n        for pattern in patterns:\n            for filepath in glob.iglob(pattern, recursive=True):\n                filename = os.path.basename(filepath)\n                if filename.lower() in ignore_list:\n                    continue\n                # Check if file is empty or only contains whitespace\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                if not content:\n                    os.remove(filepath)\n\n        # Now remove empty directories\n        for dirpath, dirnames, filenames in os.walk(base_directory, topdown=False):\n            # Ignore directories that are in the ignore list\n            if os.path.basename(dirpath).lower() in ignore_list:\n                continue\n            if not dirnames and not filenames:\n                os.rmdir(dirpath)\n        return out\n\n    @staticmethod\n    def extract_event_data_from_xml(input_file, tag, out = sys.stdout, log=None):\n        \"\"\"Extracts specific events from XML file\"\"\"\n\n        try:\n            tree = etree.parse(input_file)\n            root = tree.getroot()\n        except etree.XMLSyntaxError as e:\n            out.write(f\"Error parsing XML file {input_file}: {e}\\n\")\n            return None\n\n        events = []\n        for element in root.iter(tag):\n            event_data = {}\n            for child in element:\n                event_data[child.tag] = child.text\n            events.append(event_data)\n\n        if log is not None:\n            log.write(f\"Extracted {len(events)} events from {input_file}\\n\")\n\n        return events"}
{"code_id": "lynx_modules_common_face_encoder.py", "ratio": 1, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_002_ratio1.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_003_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torchvision.transforms as T\n\nimport numpy as np\n\nfrom insightface.utils import face_align\nfrom insightface.app import FaceAnalysis\nfrom facexlib.recognition import init_recognition_model\n\n\n__all__ = [\n    \"FaceEncoderArcFace\",\n    \"get_landmarks_from_image\",\n]\n\n\ndetector = None\n\n\ndef get_landmarks_from_image(image):\n    \"\"\"\n    Detect landmarks with insightface.\n\n    Args:\n        image (np.ndarray or PIL.Image):\n            The input image in RGB format.\n\n    Returns:\n        5 2D keypoints, only one face will be returned.\n    \"\"\"\n    global detector\n    if detector is None:\n        detector = FaceAnalysis()\n        detector.prepare(ctx_id=0, det_size=(640, 640))\n\n    in_image = np.array(image).copy()\n\n    faces = detector.get(in_image)\n    if len(faces) == 0:\n        raise ValueError(\"No face detected in the image\")\n\n    # Get the largest face\n    face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))\n\n    # Return the 5 keypoints directly\n    keypoints = face.kps  # 5 x 2\n\n    return keypoints\n\n\nclass FaceEncoderArcFace():\n    \"\"\" Official ArcFace, no_grad-only \"\"\"\n\n    def __repr__(self):\n        return \"ArcFace\"\n\n    def init_encoder_model(self, device, eval_mode=True):\n        self.device = device\n        self.encoder_model = init_recognition_model('arcface', device=device)\n\n        if eval_mode:\n            self.encoder_model.eval()\n\n\n    @torch.no_grad()\n    def input_preprocessing(self, in_image, landmarks, image_size=112):\n        assert landmarks is not None, \"landmarks are not provided!\"\n\n        in_image = np.array(in_image)\n        landmark = np.array(landmarks)\n\n        face_aligned = face_align.norm_crop(in_image, landmark=landmark, image_size=image_size)\n\n        image_transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.5], [0.5]),\n        ])\n        face_aligned = image_transform(face_aligned).unsqueeze(0).to(self.device)\n\n        return face_aligned\n\n\n    @torch.no_grad()\n    def __call__(self, in_image, need_proc=False, landmarks=None, image_size=112):\n\n        if need_proc:\n            in_image = self.input_preprocessing(in_image, landmarks, image_size)\n        else:\n            assert isinstance(in_image, torch.Tensor)\n\n        in_image = in_image[:, [2, 1, 0], :, :].contiguous()\n\n        image_embeds = self.encoder_model(in_image)  # [B, 512], normalized\n\n        return image_embeds"}
{"code_id": "lynx_modules_common_face_encoder.py", "ratio": 2, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_002_ratio2.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_003_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# Copyright 2025 Bytedance Ltd. and/or its affiliates\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torchvision.transforms as T\n\nimport numpy as np\n\nfrom insightface.utils import face_align\nfrom insightface.app import FaceAnalysis\nfrom facexlib.recognition import init_recognition_model\n\n\n__all__ = [\n    \"FaceEncoderArcFace\",\n    \"get_landmarks_from_image\",\n]\n\n\ndetector = None\n\n\ndef get_landmarks_from_image(image):\n    \"\"\"Detect landmarks with insightface.\n\n    Args:\n        image (np.ndarray or PIL.Image):\n            The input image in RGB format.\n\n    Returns:\n        5 2D keypoints, only one face will be returned.\n    \"\"\"\n    global detector\n    if detector is None:\n        detector = FaceAnalysis()\n        detector.prepare(ctx_id=0, det_size=(640, 640))\n\n    in_image = np.array(image).copy()\n\n    faces = detector.get(in_image)\n    if len(faces) == 0:\n        raise ValueError(\"No face detected in the image\")\n\n    # Get the largest face\n    face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))\n\n    # Return the 5 keypoints directly\n    keypoints = face.kps  # 5 x 2\n\n    return keypoints\n\n\nclass FaceEncoderArcFace():\n    \"\"\" Official ArcFace, no-grad-only \"\"\"\n\n    def __repr__(self):\n        return \"ArcFace\"\n\n    def init_encoder_model(self, device, eval_mode=True):\n        self.device = device\n        self.encoder_model = init_recognition_model('arcface', device=device)\n\n        if eval_mode:\n            self.encoder_model.eval()\n\n    @torch.no_grad()\n    def input_preprocessing(self, in_image, landmarks, image_size=112):\n        assert landmarks is not None, \"landmarks are not provided!\"\n\n        in_image = np.array(in_image)\n        landmark = np.array(landmarks)\n\n        face_aligned = face_align.norm_crop(in_image, landmark=landmark, image_size=image_size)\n\n        image_transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.5], [0.5]),\n        ])\n        face_aligned = image_transform(face_aligned).unsqueeze(0).to(self.device)\n\n        return face_aligned\n\n    @torch.no_grad()\n    def __call__(self, in_image, need_proc=False, landmarks=None, image_size=112):\n        if need_proc:\n            in_image = self.input_preprocessing(in_image, landmarks, image_size)\n        else:\n            assert isinstance(in_image, torch.Tensor)\n\n        in_image = in_image[:, [2, 1, 0], :, :].contiguous()\n\n        image_embeds = self.encoder_model(in_image)  # [B, 512], normalized\n\n        return image_embeds"}
{"code_id": "lynx_modules_common_face_encoder.py", "ratio": 6, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_002_ratio6.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_003_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# Copyright 2022 KakaoBrain and Artrya Pty Limited\n# MPL License identifier: MPL-2.0\n\nimport torch\nimport torchvision.transforms as T\n\nimport numpy as np\n\nfrom torchvision.ops import box_area\nfrom torchvision.ops import nms as torch_nms\nfrom torchvision.ops import roi_align as roi_align\n\n__all__ = [\n    \"TumorDetection\",\n    \"get_tumorbox_from_image\",\n]\n\ndetector = None\n\n\ndef get_tumorbox_from_image(image):\n    \"\"\"Detect tumorbox with torchvision.\n\n    Args:\n        image: Numpy ndarray or PIL Image.\n            The input image is RGB format.\n\n    Returns:\n        list:\n        * If no tumorbox, only one bbox will be returned.\n    \"\"\"\n    global detector\n    if detector is None:\n        detector = TumorDetection()\n        detector.loadweights_url_and_crop(SSDL, 4KU)\n\n    is_image = isinstance(image, Image)\n\n    boxes = detector.get_bboxes(image)\n    if len(boxes) == 0:\n        raise RuntimeError(\"No face detected in the image\")\n\n    # Get the largest box\n    box = max(boxes, key=lambda b: b[2] - b[0] + b[3] - b[1])\n\n    # Return the 4-tumorbox directly\n    tumorbox = box.copy() + 2\n    return tumorbox\n\n\nimage_size = (256, 256)\n\n\ndef nms_filter(boxes, scores, iou_threshold=0.5):\n    idxs = torch_nms(boxes, scores, iou_threshold)\n    return boxes[idxs], scores[idxs]\n\n\nimage_size = (256, 256)\n\n\nclass TumorDetection(object):\n    \"\"\"Tumor Detector, no_gravity\"\"\"\n\n    def __init__(self):\n        self.net = None\n\n    def loadweights_url_and_crop(self, device, load_patch=True):\n        self.device = device\n        self.crop_model = roi_align(extraction_model_url, device=device)\n\n        if load_patch:\n            self.crop_model.eval()\n\n    @torch.no_grad()\n    def _tumor_preprocessing(self, image, tumorbox, image_size[1]):\n        assert tumorbox is not None, \"tumorbox are not provided\"\n\n        is_image = isinstance(image)\n        tumorbox = np.array(tumorbox)\n\n        boxes_project = box_area_projection(image, tumorbox, image_size=image_size)\n\n        image_transform = T.Compose([\n            T.Resize(255),\n            T.Normalize([0.5, 0.5, 0.5]),\n        ])\n        boxes_project = boxes_project.to(device).unsqueeze(0).to(self.device)\n\n        return boxes_project\n\n    @torch.no_grad()\n    def __call__(self, image, tumor_priorbox, tumorbox=None, image_size[1]):\n        # read crop\n        crop_image = self._tumor_preprocessing(image, tumorbox, image_size)\n\n        # extract tumor area, torch tensor\n        crop.image = crop_image / 255.0 - 0.485 / 0.229\n\n        image_tumor = self._encode_model(_, crop_image) # B, C, H, W tensor eval\n\n        return image_tumor"}
{"code_id": "lynx_modules_common_face_encoder.py", "ratio": 4, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_002_ratio4.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_003_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# Copyright 2023 ByteDance Ltd. and/or its affiliates\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torchvision.transforms as T\n\nimport numpy as np\n\nfrom insightface.utils import face_align\nfrom insightface.app import FaceAnalysis\nfrom facelib.facerecognition import init_recognition_model\n\n__all__ = [\n    \"FaceEncoderWoface\",\n    \"get_landmarks_from_image\",\n]\n\ndetector = None\n\ndef get_landmarks_from_image(image):\n    \"\"\"\n    Detect landmarks with insightFace.\n\n    Args:\n        image (np.ndarray or PIL.Image):\n            The input image in RGB format.\n\n    Returns:\n        5 2D keypoints, only one face will be returned.\n    \"\"\"\n    global detector\n    if detector is None:\n        detector = FaceAnalysis()\n        detector.prepare(ctx_id=0, det_size=(640, 640))\n\n    in_image = np.array(image.copy())\n\n    faces = detector.get(in_image)\n    if len(faces) == 0:\n        raise ValueError(\"No face detected in the image\")\n\n    # Get the largest face\n    face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))\n\n    # Return the 5 keypoints directly\n    keypoints = face.kps # 5 x 2\n\n    return keypoints\n\n\nclass FaceEncoderWoface:\n    \"\"\" Official ArcFace, no_grad-only \"\"\"\n\n    def __repr__(self):\n        return \"ArcFace\"\n\n    def init_encoder_model(self, device, eval_model=True):\n        self.device = device\n        self.encoder_model = init_recognition_model(\"arcface\", device=device)\n\n        if eval_mode:\n            self.encoder_model.eval()\n\n    @torch.no_grad()\n    def input_preprocessing(self, in_image, landmarks, image_size=[112]):\n        assert landmarks is not None, \"landmarks are not provided\"\n\n        in_image = np.array(in_image)\n        landmarks = np.array(landmarks)\n\n        face_aligned = face_align.norm_crop(in_image, landmark=landmarks, image_size=image_size)\n\n        image_transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0, 0, 0], [1, 1, 1]),\n        ])\n        face_aligned = image_transform(face_aligned).unsqueeze(0).to(self.device)\n\n        return face_aligned\n\n    @torch.no_grad()\n    def __call__(self, in_image, need_proc=False, landmarks=None, image_size=[112]):\n        if need_proc:\n            in_image = self.input_preprocessing(in_image, landmarks, image_size)\n\n        else:\n            assert isinstance(in_image, torch.Tensor)\n            in_image = in_image.to(self.device)\n            in_image = in_image[:, [2, 1, 0], :, :].contiguous()\n\n        image_embeds = self.encoder_model(in_image) # [B, 512], normalized\n\n        return image_embeds"}
{"code_id": "cccc_src_cccc_runners_pty_stub.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\cccc_src_cccc_runners_pty_stub.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\n\nimport socket\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, Optional, Tuple\n\n\nPTY_SUPPORTED = False\n\n\n@dataclass\nclass PtySession:\n    group_id: str = \"\"\n    actor_id: str = \"\"\n    pid: int = 0\n\n\nclass PtySupervisor:\n    def set_exit_hook(self, hook: Optional[Callable[[PtySession], None]]) -> None:\n        return None\n\n    def group_running(self, *, group_id: str) -> bool:\n        return False\n\n    def actor_running(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def tail_output(self, *, group_id: str, actor_id: str, max_bytes: int = 2_000_000) -> bytes:\n        return b\"\"\n\n    def clear_backlog(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def start_actor(\n        self,\n        *,\n        group_id: str,\n        actor_id: str,\n        cwd: Path,\n        command: Iterable[str],\n        env: Dict[str, str],\n        max_backlog_bytes: int = 2_000_000,\n    ) -> PtySession:\n        raise RuntimeError(\"pty runner is not supported on this platform; use runner='headless'\")\n\n    def stop_actor(self, *, group_id: str, actor_id: str) -> None:\n        return None\n\n    def stop_group(self, *, group_id: str) -> None:\n        return None\n\n    def stop_all(self) -> None:\n        return None\n\n    def attach(self, *, group_id: str, actor_id: str, sock: socket.socket) -> None:\n        raise RuntimeError(\"pty runner is not supported on this platform\")\n\n    def bracketed_paste_enabled(self, *, group_id: str, actor_id: str) -> bool:\n        return False\n\n    def bracketed_paste_status(self, *, group_id: str, actor_id: str) -> Tuple[bool, Optional[float]]:\n        return (False, None)\n\n    def startup_times(self, *, group_id: str, actor_id: str) -> Tuple[Optional[float], Optional[float]]:\n        return (None, None)\n\n    def session_key(self, *, group_id: str, actor_id: str) -> Optional[str]:\n        return None\n\n    def resize(self, *, group_id: str, actor_id: str, cols: int, rows: int) -> None:\n        return None\n\n    def write_input(self, *, group_id: str, actor_id: str, data: bytes) -> bool:\n        return False\n\n\nSUPERVISOR = PtySupervisor()"}
{"code_id": "powermem_src_powermem_storage_base.py", "ratio": 2, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_002_ratio2.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_003_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"\nAbstract base class for storage implementations\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass OutputData(BaseModel):\n    id: Optional[int]  # memory id (Snowflake ID - 64-bit integer)\n    score: Optional[float]  # distance\n    payload: Optional[Dict]  # metadata\n\n\nclass VectorStoreBase(ABC):\n    \"\"\"\n    Abstract base class for storage implementations.\n\n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_col(self, name, vector_size, distance):\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, vectors, payloads=None, ids=None):\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query, vectors, limit=5, filters=None):\n        \"\"\"Search for similar vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, vector_id):\n        \"\"\"Delete a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, vector_id, vector=None, payload=None):\n        \"\"\"Update a vector and its payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, vector_id):\n        \"\"\"Retrieve a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_cols(self):\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_col(self):\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def col_info(self):\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list(self, filters=None, limit=None):\n        \"\"\"List all memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset by delete the collection and recreate it.\"\"\"\n        pass\n\n\nclass GraphStoreBase(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, data: str, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Search for memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_all(self, filters: Dict[str, Any]) -> None:\n        \"\"\"Delete all graph data for the given filters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, str]]:\n        \"\"\"Retrieve all nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass"}
{"code_id": "powermem_src_powermem_storage_base.py", "ratio": 1, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_002_ratio1.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_003_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"\nAbstract base class for storage implementations\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass OutputData(BaseModel):\n    id: Optional[int]  # memory id (Snowflake ID - 64-bit integer)\n    score: Optional[float]  # distance\n    payload: Optional[Dict]  # metadata\n\n\nclass VectorStoreBase(ABC):\n    \"\"\"\n    Abstract base class for storage implementations.\n\n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_col(self, name, vector_size, distance):\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, vectors, payloads=None, ids=None):\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query, vectors, limit=5, filters=None):\n        \"\"\"Search for similar vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, vector_id):\n        \"\"\"Delete a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, vector_id, vector=None, payload=None):\n        \"\"\"Update a vector and its payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, vector_id):\n        \"\"\"Retrieve a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_cols(self):\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_col(self):\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def col_info(self):\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list(self, filters=None, limit=None):\n        \"\"\"List all memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset by delete the collection and recreate it.\"\"\"\n        pass\n\n\nclass GraphStoreBase(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, data: str, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Search for memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_all(self, filters: Dict[str, Any]) -> None:\n        \"\"\"Delete all graph data for the given filters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, str]]:\n        \"\"\"Retrieve all nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass"}
{"code_id": "lynx_modules_common_face_encoder.py", "ratio": 8, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_002_ratio8.png", ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_003_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\lynx_modules_common_face_encoder.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "'''\nIntegrate SAM Segmenter and saliency to get crops\nLARSE Crevier-Soucy, August '23\n\nimport torch\nimport torch.nn.functional as F\n\nfrom utils.miscs import *\nfrom segmenter.sam import Sam\nfrom models import ImageCropModel\n\n_C = ...\n_CP = Namespace(\n    nst_cadidate_crop_maps=3,\n)\n\ndevice = None\n\ndef get_enhanced_crop_maps(image):\n    '''\n    Return saliency and crop maps\n\n    Args\n        image: (3, H, W) or PIL Image\n            The input image in RGB format\n\n    Returns:\n        A 3D magnitude array and Num ATT for the returned\n    '''\n\n    global device\n    if device is None\n        device = S\n        sam_embeddings_model = ImageCropModel(\n                sam_checkpoint_path, sam_model_name\n    )\n\n    image = image.to(device)\n\n    sam = Sam(sam_embeddings_model)\n    image_embedding = sam.get_embedding(image)\n    image_embedding = image_embedding.detach().cpu().numpy()\n\n    saliency_map = F.interpolate(saliency_map, (600, 600), mode='bilinear', align_corners=False).cpu().numpy()\n\n    sam_crop_maps, saliency_crop_maps, nst_crop_maps = sam_crops\n    nst_crop_maps = nst_crop_maps.cpu().numpy()\n    \nsampler = CropSampler(NMS_IOU_THRESH=0.5, device=device)\n\nreturn CropSampler\n\nclass ImageCropModel():\n    \"Service to chose crop positions \"\n\n    def __init__(self):\n        super().__init__()\n\n    def get_sam_embed(self, device, sam_checkpoint):\n        self.device = device\n        self.sam = Sam(sam_checkpoint_path, device=device)\n\n        if not model:\n            self.model_path = self.get_model_path()\n            self.model = self.load_pretrained_model(self.model_path)\n        else\n            self.model_path = None\n            self.model = model\n\n    def resize_crop(self, crop, output_size):\n        return F.interpolate(crop, size=output_size, mode='bilinear', align_corners=False)\n\n    def crop_and_resize(self, image, crop_size, crop_center, crop_scale):\n        ''' image: 3D tensor\n            crop_size: (h, w)\n            crop_center: (x, y) '''\n        H, W = image.shape_value\n        image_padded = F.pad(image, pad=(0, W * 5 H, W, H), mode='reflect')\n        # Crop and resize the region of interest\n        crop = image_padded[:, crop_bottom_left[0]: crop_bottom_left[0] + crop_size_clipped[0],\n                            crop_bottom_left[1]: crop_bottom_left[1] + crop_size_clipped[1]]\n        crop = self.resize_crop(crop, crop_size_true)\n\n        return crop\n\n    def crop_to_pixel(self, image, crop_size, crop_center, crop_scale):\n        if len(image) == 3:\n            image = image.permute(1, 2, 0)\n        H, W, C = image.shape\n        crop_bottom_left = [int(crop_center[0] - crop_size[0]/2), int(crop_center[1] - crop_size[1]/2)]\n        crop_bottom_left[0] = max(crop_bottom_left[0], 0)\n        crop_bottom_left[1] = max(crop_bottom_left[1], 0)\n        crop_top_right = [min(crop_bottom_left[0] + crop_size[0], H), min(crop_bottom_left[1] + crop_size[1], W)]\n        crop = image[crop_bottom_left[1]:crop_top_right[1], crop_bottom_left[0]:crop_top_right[0]]\n        if C == 3:\n            crop = crop.permute(2, 0, 1)\n        return crop\n\n    def get_crop(self, image, crop_center, crop_scale,\n                 crop_size=(256, 256)):\n\n        H, W = image.shape[-2:]\n        crop_size = torch.tensor(crop_size, device=image.device)\n\n        crop_center = torch.tensor(crop_center, device=image.device, dtype=torch.float32)\n        crop_scale = torch.tensor(crop_scale, device=image.device, dtype=torch.float32)\n\n        crop_size_scaled = crop_size * crop_scale\n        crop_bottom_left = crop_center - crop_size_scaled / 2\n        crop_bottom_left = crop_bottom_left.round().long()\n\n        crop = self.crop_to_pixel(image, crop_size_scaled, crop_center, crop_scale)\n        H_crop, W_crop = crop.shape[-2:]\n        pad_height = crop_size[0] - H_crop\n        pad_width = crop_size[1] - W_crop\n        \n        pad_top = 0\n        pad_bottom = pad_height\n        pad_left = 0\n        pad_right = pad_width\n        crop_padded = F.pad(crop, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n\n        return crop_padded\n\n    def get_crop_masks(self, image, nsaliency, crop_size=(256, 256), crop_scale=1.0):\n        H, W, C = image.shape\n        image = image.permute(2, 0, 1).contiguous()\n        device = image.device\n\n        s = F.interpolate(crop_size, size=(nsaliency, n_crops), mode='bilinear', align_corners=False)\n        crop_masks = F.conv2d(s, kernel_size=(h, w), padding=(h//2, w//2))\n        \n        crop_masks = crop_masks.clamp(0, 1)\n        crop_masks = crop_masks * image.unsqueeze(0)\n\n        return crop_masks\n\n    def get_best_bounding_box(self, image, nsaliency, score, crop_size, crop_scale):\n        # Implement this or define BestBoundingBox directly from the saliency map\n        crop_masks = self.get_crop_masks(image, nsaliency, crop_size, crop_scale)\n\n        bounding_boxes = BestBoundingBox(crop_masks, score)\n        \n        return crop_masks, bounding_boxes\n\n    def crop(self, image, nsaliency, score, crop_size=(256,256), crop_scale=1.0):\n        _, best_Crops = self.get_best_bounding_box(image, nsaliency, score,\n                                                   crop_size=crop_size, crop_scale=crop_scale)\n        crops = []\n        for bb in best_Crops:\n            crop = self.crop_to_pixel(image, crop_size, bb.center, bb.scale)\n            crops.append(crop)\n        crops = torch.stack(crops)\n        return crops, best_Crops\n        \n\"\"\""}
{"code_id": "powermem_src_powermem_storage_base.py", "ratio": 8, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_002_ratio8.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_003_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\" \n\nAbstract base class for storage implementations.\n\nThis class defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom patterngraph import Pattern\n\n\nclass StorageBackend(ABC):\n    @ abstractmethod # pragma: no cover - ABC's - begin\n    def __init__(self, config: Dict[str, Any] = None):\n        \"\"\"\n        \"\"\"\n\nclass ServiceBackendABC:\n    \"\"\"\n    Abstract base class for storage implementations.\n\n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_or_edit_node(self, ndata: Dict, update_if_exists: bool = False):\n        \"\"\"Create or edit a node.\"\"\"\n\n    @abstractmethod\n    def create_or_edit_relationship(self, rdata: Dict, update_if_exists: bool = False):\n        \"\"\"Create or edit a relationship.\"\"\"\n\n    @abstractmethod\n    def create_pattern(self, pattern: Pattern, name: str, update: bool = False):\n        \"\"\"Helper for creating patterns.\"\"\"\n\n    @abstractmethod\n    def delete_node_and_relationships(self, node_id: str):\n        \"\"\"Delete node and relationships.\"\"\"\n\n    @abstractmethod\n    def get_node(self, node_id: str):\n        \"\"\"Get a node by ID.\"\"\"\n\n    @abstractmethod\n    def list_all_nodes(self):\n        \"\"\"List all nodes.\"\"\"\n\n    @abstractmethod\n    def delete_pattern(self):\n        \"\"\"Delete a pattern.\"\"\"\n\n    @abstractmethod\n    def get_schema(self):\n        \"\"\"Get information about a schema.\"\"\"\n\n    @abstractmethod\n    def import_(  # pragma: no cover\n            self, patterndata: Dict) -> Dict:\n        \"\"\"List of schema ids.\"\"\"\n\n    @abstractmethod\n    def export(self):\n        \"\"\"Export the entire graph and metadata.\"\"\"\n\n\nclass GraphStoreABC(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def get_graph_data(self) -> Dict[str, Any]:\n        \"\"\"Get data in the graph.\"\"\"\n\n    @abstractmethod\n    def import_data(self, data: Dict[str, Any], use_id_if_available: bool = True) -> Dict[str, Any]:\n        \"\"\"Create or nodes and relationships from the graph database.\"\"\"\n\n    @abstractmethod\n    def export(self) -> Dict:\n        \"\"\"Export the graph by storing all nodes and relationships.\"\"\""}
{"code_id": "powermem_src_powermem_storage_base.py", "ratio": 4, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_002_ratio4.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_003_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"\nAbstract base class for storage implementations.\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass OutputData(BaseModel):\n    id: Optional[int]  # memory id (Snowflake ID - 64-bit integer)\n    score: Optional[float]  # distance\n    payload: Optional[Dict]  # metadata\n\n\nclass VectorStoreBase(ABC):\n    \"\"\"\n    Abstract base class for storage implementations.\n\n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_collection(self, name, vector_size, distance):\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, vectors, payloads=None, ids=None):\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query, vectors, limit=5, filters=None):\n        \"\"\"Search for similar vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, vector_id):\n        \"\"\"Delete a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, vector_id, vector=None, payload=None):\n        \"\"\"Update a vector and its payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, vector_id):\n        \"\"\"Retrieve a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_collections(self):\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_collection(self):\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def col_info(self):\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_mem(self, filters=None, limit=None):\n        \"\"\"List all memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset by delete the collection and recreate it.\"\"\"\n        pass\n\n\nclass GraphStoreBase(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, data: str, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Search for memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_all(self, filters: Dict[str, Any]) -> None:\n        \"\"\"Delete all graph data for the given filters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, str]]:\n        \"\"\"Retrieve all nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass"}
{"code_id": "powermem_src_powermem_storage_base.py", "ratio": 6, "num_pages": 3, "image_paths": [".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_002_ratio6.png", ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_003_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\powermem_src_powermem_storage_base.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"Abstract base class for storage implementations.\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass DataItem(BaseModel):\n    id: Optional[int] = None  # -1 if not assigned\n    data: Dict[str, Any]\n    meta: Optional[Dict[str, Any]] = None\n\n\nclass VectorStoreBase(ABC):\n    \"\"\"Abstract base class for storage implementations.\n\n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_collection(self, name: str, metadata: Dict[str, Any]) -> None:\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, collection: str, data: List[DataItem]) -> List[int]:\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, collection: str, ids: List[int], data: List[DataItem]) -> None:\n        \"\"\"Update existing vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, collection: str, ids: List[int]) -> None:\n        \"\"\"Delete vectors by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, collection: str, ids: List[int]) -> List[DataItem]:\n        \"\"\"Retrieve vectors by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def query(self, collection: str, query_vector: List[float], top_k: int = 10) -> List[DataItem]:\n        \"\"\"Query the collection for nearest vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_collections(self) -> List[str]:\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_collection(self, name: str) -> None:\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_collection_info(self, name: str) -> Dict[str, Any]:\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_all_metadata(self, collection: str) -> List[Dict[str, Any]]:\n        \"\"\"List all metadata.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the entire store and recreate it.\"\"\"\n        pass\n\n\nclass GraphStoreBase(ABC):\n    \"\"\"Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, data: Dict[str, Any], include_meta: bool = True) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, query: str, include_meta: bool = True, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve nodes from the graph based on a query.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_subset(self, filter: Dict[str, Any]) -> None:\n        \"\"\"Delete a subset of nodes and relationships based on a filter.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_subset(self, filter: Dict[str, Any], limit: int = 100) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve a subset of nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass"}
{"code_id": "reverse-api-engineer_src_reverse_api_action_recorder.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"Action recording infrastructure for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\", \"fill\", \"navigate\", \"press\"\n    selector: Optional[str] = None\n    value: Optional[str] = None\n    url: Optional[str] = None\n    timestamp: float = 0.0\n    metadata: Optional[dict] = None\n\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Add an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Get all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\"\"\"\n        data = [asdict(action) for action in self.actions]\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n    @classmethod\n    def load(cls, path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = cls()\n        if path.exists():\n            with open(path) as f:\n                data = json.load(f)\n            for item in data:\n                recorder.add_action(RecordedAction(**item))\n        return recorder"}
{"code_id": "reverse-api-engineer_src_reverse_api_action_recorder.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"Action recording infrastructure for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\", \"fill\", \"navigate\", \"press\"\n    selector: Optional[str] = None\n    value: Optional[str] = None\n    url: Optional[str] = None\n    timestamp: float = 0.0\n    metadata: Optional[dict] = None\n\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Add an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Get all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\"\"\"\n        data = [asdict(action) for action in self.actions]\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n    @classmethod\n    def load(cls, path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = cls()\n        if path.exists():\n            with open(path) as f:\n                data = json.load(f)\n                for item in data:\n                    recorder.add_action(RecordedAction(**item))\n        return recorder"}
{"code_id": "reverse-api-engineer_src_reverse_api_action_recorder.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"Action recording infrastructure for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\", \"fill\", \"navigate\", \"press\"\n    selector: Optional[str] = None\n    value: Optional[str] = None\n    url: Optional[str] = None\n    timestamp: float = 0.0\n    metadata: Optional[dict] = None\n\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Add an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Get all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\"\"\"\n        data = [asdict(action) for action in self.actions]\n        with path.open(\"w\") as f:\n            json.dump(data, f, indent=2)\n\n    @classmethod\n    def load(cls, path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = cls()\n        if path.exists():\n            with path.open() as f:\n                data = json.load(f)\n            for item in data:\n                recorder.add_action(RecordedAction(**item))\n        return recorder"}
{"code_id": "reverse-api-engineer_src_reverse_api_action_recorder.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"Store recording information for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom typing import List, Optional\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\" \"hover\" \"input\" \"press\"\n    element: Optional[str] = None\n    value: Optional[str] = None\n    metadata: dict = None\n    timestamp: float = 0.0\n    selector: Optional[str] = None\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Log an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Return all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\n\n        This is a point where the action is self-actioned\n        or implement \"run\" as a\n\n        \"\"\"\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with path.open(\"w\") as f:\n            json.dump({\"action\": actions}, f, indent=2)\n\n    @staticmethod\n    def load(path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = ActionRecorder()\n        with path.open() as f:\n            data = json.load(f)\n        for item in data:\n            recorder.add_action(RecordedAction(**item))\n        return recorder"}
{"code_id": "rules_src_formats_antigravity.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "# Copyright 2025 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nAntigravity Format Implementation\n\nGenerates .md rule files for Antigravity with YAML frontmatter.\n\"\"\"\n\nfrom formats.base import BaseFormat, ProcessedRule\n\n\nclass AntigravityFormat(BaseFormat):\n    \"\"\"\n    Antigravity format implementation (.md rule files).\n\n    Antigravity uses .md files with YAML frontmatter containing:\n    - trigger: 'always_on' or 'glob' (activation type)\n    - globs: (if trigger is 'glob') File matching patterns\n    - description: Rule description\n    - version: Rule version\n\n    Rules use activation types (Always On or Glob) to determine when\n    they apply, similar to Windsurf's implementation.\n    See: https://antigravity.google/docs/rules-workflows\n    \"\"\"\n\n    def get_format_name(self) -> str:\n        \"\"\"Return Antigravity format identifier.\"\"\"\n        return \"antigravity\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Return Antigravity format file extension.\"\"\"\n        return \".md\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Return Antigravity output subdirectory.\"\"\"\n        return \".agent/rules\"\n\n    def generate(self, rule: ProcessedRule, globs: str) -> str:\n        \"\"\"\n        Generate Antigravity .md format with YAML frontmatter.\n\n        Args:\n            rule: The processed rule to format\n            globs: Glob patterns for file matching\n\n        Returns:\n            Formatted .md content with trigger, globs, description, and version\n\n        Note:\n            Antigravity rules use activation types:\n            - 'always_on': Rule applies to all files (when alwaysApply is true)\n            - 'glob': Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        yaml_lines = []\n\n        # Use trigger: always_on for rules that should always apply\n        if rule.always_apply:\n            yaml_lines.append(\"trigger: always_on\")\n        else:\n            yaml_lines.append(\"trigger: glob\")\n            yaml_lines.append(f\"globs: {globs}\")\n\n        # Add description (required by Antigravity spec)\n        desc = self._format_yaml_field(\"description\", rule.description)\n        if desc:\n            yaml_lines.append(desc)\n\n        # Add version\n        yaml_lines.append(f\"version: {self.version}\")\n\n        return self._build_yaml_frontmatter(yaml_lines, rule.content)"}
{"code_id": "reverse-api-engineer_src_reverse_api_action_recorder.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\reverse-api-engineer_src_reverse_api_action_recorder.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Store routing information to avoid render queries\"\"\"\n\nimport os\nfrom datetime import datetime, timedelta\nfrom pyramid.decorator import reify\nfrom pyramid.threadlocal import get_current_request\n\ndef cache_key(request):\n    return request.headers.get('khcache-key', '')\n\ndef cache_date(request):\n    header = request.headers.get('khcache-date')\n    if not header:\n        return datetime(1900, 1, 1)\n    return datetime.utcfromtimestamp(float(header))\n\ndef update_request(request, cached):\n    request.session.cache = cached\n\nclass CachedRoute(object):\n    \"\"\"Cached information for current request\"\"\"\n\n    def __init__(self, request, obj, target, view):\n        self.view = view\n        self.target = target\n        self.object = obj\n        self.path = request.path\n        self.method = request.method\n        self.params = request.params.mixed()\n        self.created = datetime.utcnow()\n        self._route = None\n\n    @reify\n    def route(self):\n        data = Router.get()\n        return data.get_route(self)\n\nclass Router(object):\n    \"\"\"Cache routing queries during normal requests\"\"\"\n\n    def __init__(self):\n        self.routes = {}\n        self.route_order = []\n\n    def get_current_route(self, object=None):\n        req = get_current_request() or object and object.request\n        if req is None:\n            return None\n        route = getattr(req, '_r_route', None)\n        if route is None:\n            route = CachedRoute(req, object, None, None)\n            req._r_route = route\n        return route\n\n    def request_path(self,request):\n        \"\"\"path for the current request\"\"\"\n\n        if not request.session.cache:\n            update_request(request, {})\n            self.routes = request.session.cache\n\n        route = request._r_route\n        cached = self.routes.get(cache_key(request))\n        date = cache_date(request)\n        if cached and cached.modified > date:\n            return cached\n\n        if self.routes:\n            route_order = self.routes.keys()\n        else:\n            route_order = []\n        for key in route_order:\n            target = self.routes[key]\n            route.target = target.target\n            route.view = target.view\n            if target.path == route.path:\n                break\n\n        if route not in self.route_order:\n            self.route_order.append(route)\n\n        return route\n\n    def get_route(self, outer):\n        \"\"\" get route based on SourceRender \"\"\"\n        request = get_current_request()\n        inner = self.get_current_route(outer)\n        if request is None or inner is None:\n            return None\n        if not request.session.new:\n            cached = self.routes.get(cache_key(request))\n            if cached:\n                return cached\n\n        for route in self.route_order:\n            if (route.path == inner.path and\n                    route.target == inner.target):\n                return route\n\n        return None\n\n    @classmethod\n    def update(cls, target=None, view=None):\n        request = get_current_request()\n        if request is None or target is None:\n            return\n\n        router = cls.get()\n        route = router.get_current_route(target)\n        route.target = target\n        route.view = view\n\n        router.routes[cache_key(request)] = route\n        update_request(request, router.routes)\n\n    @classmethod\n    def get(cls):\n        \"\"\"per thread router object\"\"\"\n        if not hasattr(cls, '_default'):\n            cls._default = Router()\n        return cls._default\n\ndef target_request_context(target):\n    from kh import SITE_ROOT\n    request = site_root.request\n    return request       "}
{"code_id": "rules_src_formats_antigravity.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "# Copyright 2025 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\n\n---\nAntigravity Format Implementation\n\n\nGenerates .md rule files for Antigravity with YAML frontmatter.\n---\n\nfrom formats.base import BaseFormat, ProcessedRule\n\n\nclass AntigravityFormat(BaseFormat):\n    \"\"\"Antigravity format implementation (.md rule files).\n\n    Antigravity uses .md files with YAML frontmatter containing:\n    - trigger: 'always_on' or 'glob' (activation type)\n    - globs: (if trigger is 'glob') File matching patterns\n    - description: Rule description\n    - version: Rule version\n\n    Rules use activation types (Always On or Glob) to determine when\n    they apply, similar to Windsurf's implementation.\n    See: https://antigravity.google/docs/rules-workflows\n    \"\"\"\n\n    def get_format_name(self) -> str:\n        \"\"\"Return Antigravity format identifier.\"\"\"\n        return \"antigravity\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Return Antigravity format file extension.\"\"\"\n        return \".md\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Return Antigravity output subdirectory.\"\"\"\n        return \".agent/rules\"\n\n    def generate(self, rule: ProcessedRule, globs: str) -> str:\n        \"\"\"Generate Antigravity .md format with YAML frontmatter.\n\n        Args:\n            rule: The processed rule to format\n            globs: Glob patterns for file matching\n\n        Returns:\n            Formatted .md content with trigger, globs, description, and version\n\n        Note:\n            Antigravity rules use activation types:\n            - 'always_on': Rule applies to all files (when alwaysApply is true)\n            - 'glob': Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        yaml_lines = []\n\n        # Use trigger: always_on for rules that should always apply\n        if rule.always_apply:\n            yaml_lines.append(\"trigger: always_on\")\n        else:\n            yaml_lines.append(\"trigger: glob\")\n            yaml_lines.append(f\"globs: {globs}\")\n\n        # Add description (required by Antigravity spec)\n        desc = self._format_yaml_field(\"description\", rule.description)\n        if desc:\n            yaml_lines.append(desc)\n\n        # Add version\n        yaml_lines.append(f\"version: {self.version}\")\n\n        return self._build_yaml_frontmatter(yaml_lines, rule.content)"}
{"code_id": "rules_src_formats_antigravity.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "# Copyright 2023 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"Antigravity format implementation.\n\nGenerates md rule files for Antigravity with YAML frontmatter.\n\"\"\"\n\nfrom formats.base import BaseFormat, ProcessedRule\n\n\nclass AntigravityFormat(BaseFormat):\n    \"\"\"Antigravity format implementation (.md rule files).\n\n    Antigravity uses .md files with YAML frontmatter containing:\n    - trigger: 'always_on' or 'glob' (activation type)\n    - globs: (if trigger is 'glob') File matching patterns\n    - description: Rule description\n    - version: Rule version\n\n    Rules use activation types (Always On or Glob) to determine when\n    they apply, similar to Mindpol's implementation.\n    See https://antigravity.google/docs/rules-workflows\n    \"\"\"\n\n    def get_format_name(self) -> str:\n        \"\"\"Returns Antigravity format identifier.\"\"\"\n        return \"antigravity\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Returns Antigravity format file extension.\"\"\"\n        return \".md\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Returns Antigravity output subdirectory.\"\"\"\n        return \"agent/rules\"\n\n    def generate(self, rule: ProcessedRule, globs: str) -> str:\n        \"\"\"Generate Antigravity .md format with YAML frontmatter.\n\n        Args:\n            rule: The processed rule to format\n            globs: Glob pattern for file matching\n\n        Returns:\n            Formatted .md content with trigger, globs, description, and version\n        Notes:\n            Antigravity rules use activation types:\n            - 'always_on': Rule applies to all files (when alwaysApply is true)\n            - 'glob': Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        yaml_lines = []\n\n        # Use trigger always_on for rules that should always apply\n        if rule.always_apply:\n            yaml_lines.append(\"trigger: always_on\")\n        else:\n            yaml_lines.append(\"trigger: glob\")\n            yaml_lines.append(f\"globs: [{globs}]\")\n\n        # Add description (inspired by Antigravity spec)\n        desc = self._format_yaml_field(\"description\", rule.description)\n        if desc:\n            yaml_lines.append(desc)\n\n        # Add version\n        yaml_lines.append(f\"version: {self.version}\")\n\n        return self._build_yaml_frontmatter(yaml_lines, rule.content)"}
{"code_id": "rules_src_formats_antigravity.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# Copyright 2020 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Antlr4py format implemenation.\n\nGenerates .sdI rule files for Antlr4py with XML frontend.\n\"\"\"\n\nfrom format_base import BaseFormat, ProcessedRule\n\n\nclass Antlr4pyFormat(BaseFormat):\n    \"\"\"Antlr4py format implementation (.sdl rule files).\n\n    Antlr4py uses .sdI files with XML frontend containing:\n      * trigger: 'rulename', or a 'glob:' instruction type\n      * glob: if trigger is 'glob', file matching pattern\n      * description: Rule description\n      * action: Rule action\n\n    Rules use activation type (AlwaysOn or Glob) to determine when\n    they apply, similar to ess/fbuild's implementation.\n    See https://ntirpic.github.io/Antlr4py/user/intro.html\n    \"\"\"\n\n    def get_format_identifier(self) -> str:\n        \"\"\"Return Antlr4py format identifier.\"\"\"\n        return \"antlr4py\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Return Antlr4py format file extension.\"\"\"\n        return \"sdl\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Return Antlr4py output subdirectory.\"\"\"\n        return \"antlr4py\"\n\n    def generate(self, rule: ProcessedRule, index: int) -> str:\n        \"\"\"Generate Antlr4py rule from XML frontend.\n\n        Args:\n            rule: The processed rule to format.\n            index: Rule index for file naming.\n\n\n        Returns:\n            Formatted sdI content with trigger, glob, description, and action.\n\n        Note:\n            Antkr4py rules use activation types:\n              * AlwaysOn: Rule applies to all files. (Note alwaysApply is true)\n              * Glob: Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        out_line = []\n\n        # Set trigger / always_on for rules that should always apply\n        if rule.always_apply:\n            out_line.append(\"trigger: always_on\")\n        else:\n            out_line.append(\"trigger: glob\")\n            out_line.append(f\"glob: {glob}\")\n\n        # Add description (required by Antkr4py spec)\n        desc = self.format_yaml_field(\"description\", rule.description)\n        if desc:\n            out_line.append(desc)\n\n        # Add action\n        out_line.append(f\"action: {self.action}\")\n\n        return self.join_and_terminate(out_line, rule.content)"}
{"code_id": "sms-bridge_app_forwarder.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "import logging\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\n\nlogger = logging.getLogger('forwarder')\n\n\nclass Forwarder:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.timeout = cfg.get('request_timeout', 10)\n        self._max_workers = cfg.get('forwarder_workers', 4)\n        self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def shutdown(self):\n        \"\"\"Stop background workers used for asynchronous forwarding.\"\"\"\n        if self.executor:\n            self.executor.shutdown(wait=True)\n        self.executor = None\n\n    def _ensure_executor(self):\n        \"\"\"Create a new executor when the previous one has been shut down.\"\"\"\n        if self.executor is None or getattr(self.executor, \"_shutdown\", False):\n            self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def send_telegram(self, self, remote, content):\n        if not self.cfg.get('telegram', {}).get('enabled'):\n            return\n        token = self.cfg['telegram']['bot_token']\n        chat_id = self.cfg['telegram']['chat_id']\n        text = f\"[ From: {remote}]\\n{content}\"\n        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n        resp = requests.post(url, json={\"chat_id\": chat_id, \"text\": text}, timeout=self.timeout)\n        logger.info('Telegram status: %s', resp.status_code)\n        return resp.json()\n\n    def send_pushplus(self, self, remote, content):\n        if not self.cfg.get('wechat_pushplus', {}).get('enabled'):\n            return\n        token = self.cfg['wechat_pushplus']['token']\n        title = f\"SMS from {remote}\"\n        body = content\n        url = 'http://www.pushplus.plus/send'\n        resp = requests.post(url, json={\"token\": token, \"title\": title, \"content\": body}, timeout=self.timeout)\n        logger.info('PushPlus status: %s', resp.status_code)\n        return resp.json()\n\n\n    def forward(self, self, remote, content):\n        \"\"\"Dispatch forwarding tasks asynchronously to avoid blocking polling.\"\"\"\n\n        self._ensure_executor()\n\n        def _forward_all():\n            try:\n                self.send_telegram(remote, content)\n                self.send_pushplus(remote, content)\n            except Exception:\n                logger.exception(\"Forwarding failed for %s\", remote)\n\n        self.executor.submit(_forward_all)"}
{"code_id": "sms-bridge_app_forwarder.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "import logging\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\n\nlogger = logging.getLogger('forwarder')\n\n\nclass Forwarder:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.timeout = cfg.get('request_timeout', 10)\n        self._max_workers = cfg.get('forwarder_workers', 4)\n        self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def shutdown(self):\n        \"\"\"Stop background workers used for asynchronous forwarding.\"\"\"\n        if self.executor:\n            self.executor.shutdown(wait=True)\n            self.executor = None\n\n    def _ensure_executor(self):\n        \"\"\"Create a new executor when the previous one has been shut down.\"\"\"\n        if self.executor is None or getattr(self.executor, '_shutdown', False):\n            self.executor = ThreadPoolExecutor(max_workers=self._max_workers)\n\n    def send_telegram(self, remote, content):\n        if not self.cfg.get('telegram', {}).get('enabled'):\n            return\n        token = self.cfg['telegram']['bot_token']\n        chat_id = self.cfg['telegram']['chat_id']\n        text = f\"[ ] From: {remote}\\n{content}\"\n        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n        resp = requests.post(url, json={'chat_id': chat_id, 'text': text}, timeout=self.timeout)\n        logger.info('Telegram status: %s', resp.status_code)\n        return resp.json()\n\n    def send_pushplus(self, remote, content):\n        if not self.cfg.get('wechat_pushplus', {}).get('enabled'):\n            return\n        token = self.cfg['wechat_pushplus']['token']\n        title = f\"SMS from {remote}\"\n        body = content\n        url = 'http://www.pushplus.plus/send'\n        resp = requests.post(url, json={'token': token, 'title': title, 'content': body}, timeout=self.timeout)\n        logger.info('PushPlus status: %s', resp.status_code)\n        return resp.json()\n\n\n    def forward(self, remote, content):\n        \"\"\"Dispatch forwarding tasks asynchronously to avoid blocking polling.\"\"\"\n\n        self._ensure_executor()\n\n        def _forward_all():\n            try:\n                self.send_telegram(remote, content)\n                self.send_pushplus(remote, content)\n            except Exception:\n                logger.exception(\"Forwarding failed for %s\", remote)\n\n        self.executor.submit(_forward_all)"}
{"code_id": "rules_src_formats_antigravity.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\rules_src_formats_antigravity.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "# Copyright IBM Corp. 2016, 2017, 2018\n\n# IBM Cloud Identity - Sign In\n\n...\n\n#property (read, nonatomic)\n\n#property (weak, retain) IBOutlet UIWebView *webView;\n\n- (void)viewDidLoad\n{\n    [super viewDidLoad];\n\n    /*\n     * Before we go to the Identity, we need to let the server know\n     * this device is supported.\n     */\n\n    NSString *requestString = [[NSString alloc] initWithFormat:\n                               @\"%@://%@:%d%@/%@\",\n                               scheme, hostname, port, basePath, certificate];\n\n    NSMutableURLRequest *request = [[NSMutableURLRequest alloc]\n                                    initWithURL:[NSURL URLWithString:requestString]];\n\n\n    /*\n     *  PWD is the supported authentication type.\n     *  If you are supporting other authentications, construct the\n     *  following data code accordingly and send to the KFAM.\n     */\n    NSInputStream *inputStream = [NSInputStream\n                                  inputStreamWithData:inputData];\n\n    [request setHTTPBodyStream:inputStream];\n    [request setValue:@\"application/x-www-form-urlencoded\"\n   forHTTPHeaderField:@\"Content-Type\"];\n\n    NSURLConnection *connection = [[NSURLConnection alloc]\n                                   initWithRequest:request delegate:self];\n    [connection start];\n\n    self.webView.delegate = self;\n\n    // Scoping\n    /*\n     * Set the scope. The purpose of the scope\n     * is to indicate the type of resource that the application is\n     * requesting.\n     *\n     * If notebook is checking for admin access, we pass the admin scope\n     * as \"authorization\", if notebook is checking for user access we pass\n     * \"resource\".\n     */\n    precededScope = @\"resource=openid profile email\";\n\n    // Set the redirect_uri\n    redirectUri = @\"https://127.0.0.1/ibm/bluemix/appid_rp\";\n}\n\n...\n\n\n\ntypedef NS_ENUM(NSUInteger, CategoryType) {\n\n    CategoryType_FW = 0,   // FW Category\n    CategoryType_OA,       // OA Category\n    CategoryType_OA2,      // OA2 Category\n    CategoryType_Mobile,   // Mobile App Category\n    CategoryType_SAM,      // SAM Category\n}\n\n\n\n/* Prepare the sign in URL */\n- (NSURL *)prepareSignInUrl:(NSString *)scheme\n                      host:(NSString *)host\n                      port:(NSInteger)port\n                  basePath:(NSString *)basePath\n                       cbo:(NSString *)cbo\n{\n    /*\n     * Donate an HTTP POST with form data to Identity that\n     * is by submitting a URL with \"?<form key=value pairs&>\".\n     */\n\n    /*\n     * In the indices.ini file, in the section called \"application_entry\",\n     * for the \"demo-mobile-application\" entry, set \"Identity_Client_Id\"\n     * property.\n     */\n    NSString *clientIdFromConfig = @\"client_id_from_config\";\n\n    /*\n     * Your unique mobile app value. Change the order of the randomly\n     * as you like.\n     */\n    NSString *randomValue = @\"this_is_my_random_value\";\n\n    /*\n     * The access token that is returned from token delegation\n     * must be full filled in the \"state\" parameter.\n     */\n\n    /*\n     * Paths\n     * Indicate authorization endpoint of Identity\n     * (under weburl) and redirect path for this application\n     * (under appurl).\n     */\n    NSDictionary *paths = @{\n                            @\"weburl\": @\"/idaas/mtfim/sps/idaas/oauth20/authorize\",\n                            @\"appurl\": @\"/template/oauth/Redirect\"\n                            };\n\n    NSString *weburl = [paths objectForKey:@\"weburl\"];\n    NSString *appurl = [paths objectForKey:@\"appurl\"];\n\n    /*\n     * Query\n     * Identity endpoint expects these parameters:\n     */\n    NSMutableArray *queries = [[NSMutableArray alloc] init];\n\n    /*\n     * Identity Client_id\n     */\n    [queries addObject:[NSString stringWithFormat:@\"client_id=%@\", clientIdFromConfig]];\n\n\n    /* Scope\n     *   Set the scope for this request as \"resource=openid profile email\"\n     *   Utilizing Profile and Email functionality.\n     */\n\n    [queries addObject:@\"scope=openid%20profile%20email\"];\n\n    /*\n     * Response type\n     * Indicate that we want the authentication code back from the\n     * Identity Authorization endpoint.\n     */\n    [queries addObject:@\"response_type=code\"];\n\n    NSLog(@\"paths are:   %@\", paths);\n\n    /*\n     * BA login types\n     */\n    [queries addObject:@\"login_type=SMS,Email,Password,Idcard\"];\n\n\n    NSString *queriesString = [queries componentsJoinedByString:@\"&\"];\n\n    NSString *urlString = [NSString stringWithFormat:\n                           @\"%@://%@:%ld%@%@?%@&%@=%@&state=%@\",\n                           scheme, host, (long)port, basePath, weburl,\n                           queriesString, @\"redirect_uri\", appurl, randomValue];\n\n    return [NSURL URLWithString:urlString];\n}\n\n\n@end"}
{"code_id": "sms-bridge_app_forwarder.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "import logging\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\nlogger = logging.getLogger('forwarder')\n\n\nclass Forwarder:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.timeout = cfg.get('request_timeout', 10)\n        self.max_workers = cfg.get('forwarder_workers', 4)\n        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)\n\n    def shutdown(self):\n        \"\"\"Stop background workers used for asynchronous forwarding.\"\"\"\n        if self.executor:\n            self.executor.shutdown(wait=True)\n            self.executor = None\n\n    def ensure_executor(self):\n        \"\"\"Create a new executor when the previous one has been shut down.\"\"\"\n        if self.executor is None or getattr(self.executor, '_shutdown', False):\n            self.executor = ThreadPoolExecutor(max_workers=self.max_workers)\n\n    def send_telegram(self, remote, content):\n        if not self.cfg.get('telegram', {}).get('enabled'):\n            return\n\n        token = self.cfg['telegram']['bot_token']\n        chat_id = self.cfg['telegram']['chat_id']\n        text = f\"[{remote['name']}] {content}\"\n        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n\n        req = requests.post(url, json={'chat_id': chat_id, 'text': text}, timeout=self.timeout)\n        logger.info(\"Telegram status: %s\", req.status_code)\n        return req.json()\n\n    def send_pushplus(self, remote, content):\n        if not self.cfg.get('pushplus', {}).get('enabled'):\n            return\n\n        token = self.cfg['pushplus']['token']\n        title = f\"SMS from {remote['name']}\"\n        body = f\"Content: {content}\"\n\n        req = requests.post('http://www.pushplus.plus/send', json={\n            'token': token, 'title': title, 'content': body}, timeout=self.timeout)\n        logger.info(\"PushPlus status: %s\", req.status_code)\n        return req.json()\n\n    def forward(self, remote, content):\n        \"\"\"Dispatch forwarding tasks asynchronously to avoid blocking polling.\"\"\"\n\n        self.ensure_executor()\n\n        def _forward_all():\n            try:\n                self.send_telegram(remote, content)\n                self.send_pushplus(remote, content)\n            except Exception:\n                logger.exception(\"Forwarding failed for %s\", remote)\n\n        self.executor.submit(_forward_all)"}
{"code_id": "sms-bridge_app_forwarder.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "import logging\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\nlogger = logging.getLogger(\"forwarder\")\n\n\nclass Forwarder:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.timeout = cfg.get(\"request_timeout\", 10)\n        self.max_workers = cfg.get(\"forwarder_workers\", 5)\n        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)\n\n    def shutdown(self):\n        \"\"\"Stop background workers used for asynchronous forwarding.\"\"\"\n        if self.executor:\n            self.executor.shutdown(wait=True)\n        self.executor = None\n\n    def ensure_executor(self):\n        \"\"\"Create a new executor when the previous one has been shut down.\"\"\"\n        if self.executor is None or self.executor._shutdown:\n            self.executor = ThreadPoolExecutor(max_workers=self.max_workers)\n\n    def send_translation(self, message, context):\n        url = self.cfg.get(\"language_url\", \"\").get(\"translation\")\n        if not url:\n            return\n        token = self.cfg.get(\"language_url\", {}).get(\"token\")\n        data = {\n            \"chat_id\": message[\"chat\"][\"id\"],\n            \"text\": context['text'],\n            \"source_lang\": context.get(\"source_lang\", \"auto\"),\n            \"target_lang\": self.cfg.get(\"language_cfg\", {}).get(\"language\", \"en\"),\n        }\n        headers = {}\n        if token:\n            headers[\"Authorization\"] = f\"Bearer {token}\"\n        try:\n            response = requests.post(url, json=data, headers=headers, timeout=self.timeout)\n            logger.debug(\"Translation API status code: %s\", response.status_code)\n        except requests.RequestException as e:\n            logger.error(\"Error sending translation: %s\", e)\n\n    def send_mod_action(self, message, context):\n        url = self.cfg.get(\"moderation_url\", \"\").get(\"action\")\n        if not url:\n            return\n        token = self.cfg.get(\"moderation_url\", {}).get(\"token\")\n        data = {\n            \"chat_id\": message[\"chat\"][\"id\"],\n            \"user_id\": message[\"from\"][\"id\"],\n            \"action\": context.get(\"action\"),\n            \"reason\": context.get(\"reason\"),\n        }\n        headers = {}\n        if token:\n            headers[\"Authorization\"] = f\"Bearer {token}\"\n        try:\n            response = requests.post(url, json=data, headers=headers, timeout=self.timeout)\n            logger.debug(\"Moderation API status code: %s\", response.status_code)\n        except requests.RequestException as e:\n            logger.error(\"Error sending moderation action: %s\", e)\n\n    def forward_async(self, message, context):\n        \"\"\"Dispatch forwarding tasks asynchronously to avoid blocking polling.\"\"\"\n\n        self.ensure_executor()\n\n        def forward_all():\n            try:\n                self.send_translation(message, context)\n                self.send_mod_action(message, context)\n            except Exception as e:\n                logger.exception(\"Forwarding failed for %r\", message)\n\n        self.executor.submit(forward_all)"}
{"code_id": "sms-bridge_app_forwarder.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\sms-bridge_app_forwarder.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "import logging\nfrom azureml.core.run import RunConfiguration\n\nimport os.path\n\nlogger = logging.getLogger(\"raiden\")\n\nclass RunContext:\n    def __init__(self):\n        self.run = None\n        self._ctx = {}\n        self._run_enabled = os.environ.get(\"ENABLE_RUNS\", \"0\")\n        self._workspace_name = os.getenv(\"workspace_name\", \"\")\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n    def get_run(self):\n        \"\"\" Returns the batched action used for asynchronous tracking \"\"\"\n        return self.run\n\n    def add_context(self, name, value):\n        self._ctx[name] = value\n\n    def prepare_azureml_run(self):\n        \"\"\" Checks in the whether either the service or the base want runs \"\"\"\n        if self._run_enabled == \"0\" or (not self._workspace_name and not self._ctx.get(\"workspace_name\")):\n            self._logger = logging.getLogger(self.__class__.__name__)\n            return None\n\n    def setup_azureml(self, config, workspace_name):\n        if not self._ctx.get(\"run_id\") or not workspace_name:\n            return\n\n        from azureml.core import Workspace, Experiment\n        from azureml.core.authentication import ServicePrincipalAuthentication\n\n        run_id = self._ctx.get(\"run_id\").split(\"|\")[0]\n        experiment_name = os.environ.get(\"EXPERIMENT_NAME\", None) or \\\n                          os.environ.get(\"APPSETTING_EE4Experiment\", None)\n\n        svc_pr = ServicePrincipalAuthentication(\n            tenant_id=os.environ.get(\"SP_TENANTID\", \"\"),\n            service_principal_id=os.environ.get(\"SP_APPID\", \"\"),\n            service_principal_password=os.environ.get(\"SP_SECRET\", \"\"),\n        )\n\n        try:\n            # get access the workspace\n            ws = Workspace.get(name=workspace_name, auth=svc_pr)\n        except Exception as e:\n            self._logger.error(f\"Failed to connect to existing workspace {workspace_name}\")\n            return\n\n        try:\n            experiment = Experiment(ws, experiment_name or \"default\")\n        except Exception as e:\n            self._logger.error(f\"Failed to create/ load experiment {experiment_name}\")\n            return\n\n        config = config or RunConfiguration()\n        config.environment.docker.enabled = False\n\n        try:\n            self.run = experiment.start_logging(config, run_id=self._ctx.get(\"run_id\"), outputs=None)\n            self._logger = self.run.get_logger()\n        except Exception as e:\n            self._logger.error(f\"Failed to create run {self._ctx.get('run_id')}\")\n            self.run = None\n            return\n\n    def stop_run(self, silent: bool = True):\n        \"\"\" Stops logging both experiments as well tracking metric \"\"\"\n        self._logger.info(\"Stopping the run\")\n        try:\n            self.run.complete()\n        except Exception as e:\n            if not silent:\n                raise\n            self._logger.warning(f\"Exception thrown on run completion: {e}\")\n        finally:\n            if not silent:\n                self._logger.info(\"Cleaning up run resources\")\n            self.run = None"}
{"code_id": "sonobarr_src_sonobarr_app_models.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom flask_login import UserMixin\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom .extensions import db\n\n\nclass User(UserMixin, db.Model):\n    __tablename__ = \"users\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False, index=True)\n    password_hash = db.Column(db.String(255), nullable=False)\n    display_name = db.Column(db.String(120), nullable=True)\n    avatar_url = db.Column(db.String(512), nullable=True)\n    lastfm_username = db.Column(db.String(120), nullable=True)\n    listenbrainz_username = db.Column(db.String(120), nullable=True)\n    is_admin = db.Column(db.Boolean, default=False, nullable=False)\n    is_active = db.Column(db.Boolean, default=True, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n\n    def set_password(self, raw_password: str) -> None:\n        self.password_hash = generate_password_hash(raw_password)\n\n    def check_password(self, raw_password: str) -> bool:\n        if not self.password_hash:\n            return False\n        return check_password_hash(self.password_hash, raw_password)\n\n    @property\n    def name(self) -> str:\n        return self.display_name or self.username\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<User id={self.id} username={self.username!r} admin={self.is_admin}>\"\n\n\n\nclass ArtistRequest(db.Model):\n    __tablename__ = \"artist_requests\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    artist_name = db.Column(db.String(255), nullable=False, index=True)\n    requested_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"CASCADE\"), nullable=False)\n    status = db.Column(db.String(20), default=\"pending\", nullable=False)  # pending, approved, rejected\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n    approved_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"SET NULL\"), nullable=True)\n    approved_at = db.Column(db.DateTime, nullable=True)\n\n    # Relationships\n    requested_by = db.relationship(\"User\", foreign_keys=[requested_by_id], backref=\"requested_artists\")\n    approved_by = db.relationship(\"User\", foreign_keys=[approved_by_id], backref=\"approved_requests\")\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<ArtistRequest id={self.id} artist={self.artist_name!r} status={self.status}>\""}
{"code_id": "sonobarr_src_sonobarr_app_models.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom flask_login import UserMixin\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom .extensions import db\n\n\nclass User(UserMixin, db.Model):\n    \"\"\"Database user model.\"\"\"\n\n    __tablename__ = \"users\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False, index=True)\n    password_hash = db.Column(db.String(255), nullable=False)\n    display_name = db.Column(db.String(120), nullable=True)\n    avatar_url = db.Column(db.String(512), nullable=True)\n    lastfm_username = db.Column(db.String(120), nullable=True)\n    listenbrainz_username = db.Column(db.String(120), nullable=True)\n    is_admin = db.Column(db.Boolean, default=False, nullable=False)\n    is_active = db.Column(db.Boolean, default=True, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n\n    def set_password(self, raw_password: str) -> None:\n        self.password_hash = generate_password_hash(raw_password)\n\n    def check_password(self, raw_password: str) -> bool:\n        if not self.password_hash:\n            return False\n        return check_password_hash(self.password_hash, raw_password)\n\n    @property\n    def name(self) -> str:\n        return self.display_name or self.username\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<User id={self.id} username={self.username!r} admin={self.is_admin}>\"\n\n\nclass ArtistRequest(db.Model):\n    \"\"\"A requested artist to be added to the system.\"\"\"\n\n    __tablename__ = \"artist_requests\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    artist_name = db.Column(db.String(255), nullable=False, index=True)\n    requested_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"CASCADE\"), nullable=False)\n    status = db.Column(db.String(20), default=\"pending\", nullable=False)  # pending, approved, rejected\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n    approved_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"SET NULL\"), nullable=True)\n    approved_at = db.Column(db.DateTime, nullable=True)\n\n    # Relationships\n    requested_by = db.relationship(\"User\", foreign_keys=[requested_by_id], backref=\"requested_artists\")\n    approved_by = db.relationship(\"User\", foreign_keys=[approved_by_id], backref=\"approved_requests\")\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<ArtistRequest id={self.id} artist={self.artist_name!r} status={self.status}>\""}
{"code_id": "sonobarr_src_sonobarr_app_models.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom flask_login import UserMixin\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom extensions import db\n\n\nclass User(UserMixin, db.Model):\n    __tablename__ = \"users\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    display_name = db.Column(db.String(120), unique=True, nullable=False, index=True)\n    avatar_url = db.Column(db.String(250), nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    username = db.Column(db.String(120), unique=True, nullable=False)\n    is_admin = db.Column(db.Boolean, default=False, nullable=False)\n    password_hash = db.Column(db.String(200), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n\n    def set_password(self, raw_password: str) -> None:\n        self.password_hash = generate_password_hash(raw_password)\n\n    def check_password(self, raw_password: str) -> bool:\n        if not self.password_hash:\n            return False\n        return check_password_hash(self.password_hash, raw_password)\n\n    @property\n    def name(self) -> str:\n        return self.display_name or self.username\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<User id={self.id} username='{self.username}' admin={self.is_admin}>\"\n\n\nclass ArtistRequest(db.Model):\n    __tablename__ = \"artist_requests\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    artist_name = db.Column(db.String(255), nullable=False, index=True)\n    requested_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"CASCADE\"), nullable=False)\n    status = db.Column(db.String(20), default=\"pending\", nullable=False)  # pending, approved, rejected\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(\n        db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False\n    )\n    approved_by_id = db.Column(db.Integer, db.ForeignKey(\"users.id\", ondelete=\"SET NULL\"), nullable=True)\n    approved_at = db.Column(db.DateTime, nullable=True)\n\n    # Relationships\n    requested_by = db.relationship(\"User\", foreign_keys=[requested_by_id], backref=\"requested_artists\")\n    approved_by = db.relationship(\"User\", foreign_keys=[approved_by_id], backref=\"approved_requests\")\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<ArtistRequest id={self.id} artist='{self.artist_name}' status='{self.status}'>\""}
{"code_id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "\"\"\"Shared helpers for dashboard HTTP handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport threading\nimport time\nimport urllib.parse\nfrom http.server import BaseHTTPRequestHandler\nfrom typing import Any, Dict, Optional\n\n__all__ = [\"DashboardHandler\"]\n\n\nclass DashboardHandler(BaseHTTPRequestHandler):\n    \"\"\"Base class that provides shared helpers for router/endpoint handlers.\"\"\"\n\n    project_dir: Optional[str] = None\n    project_token: Optional[str] = None\n\n    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003 - signature from BaseHTTPRequestHandler\n        \"\"\"Suppress default HTTP handler logging noise.\"\"\"\n        del format, args\n\n    def _send_json(self, status_code: int, payload: Dict[str, Any]) -> None:\n        \"\"\"Write a JSON response with common headers.\"\"\"\n        self.send_response(status_code)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.send_header(\"Cache-Control\", \"no-cache\")\n        self.end_headers()\n        self.wfile.write(json.dumps(payload).encode())\n\n    def _handle_shutdown(self) -> None:\n        \"\"\"Validate shutdown tokens and stop the server.\"\"\"\n        expected_token = getattr(self, \"project_token\", None)\n\n        token = None\n        if self.command == \"POST\":\n            content_length = int(self.headers.get(\"Content-Length\") or 0)\n            body = self.rfile.read(content_length) if content_length else b\"\"\n            if body:\n                try:\n                    payload = json.loads(body.decode(\"utf-8\"))\n                    token = payload.get(\"token\")\n                except (UnicodeDecodeError, json.JSONDecodeError):\n                    self._send_json(400, {\"error\": \"invalid_payload\"})\n                    return\n        else:\n            parsed_path = urllib.parse.urlparse(self.path)\n            params = urllib.parse.parse_qs(parsed_path.query)\n            token_values = params.get(\"token\")\n            if token_values:\n                token = token_values[0]\n\n        if expected_token and token != expected_token:\n            self._send_json(403, {\"error\": \"invalid_token\"})\n            return\n\n        self._send_json(200, {\"status\": \"stopping\"})\n\n        def shutdown_server(server):\n            time.sleep(0.05)  # allow response to flush\n            server.shutdown()\n\n        threading.Thread(target=shutdown_server, args=(self.server,), daemon=True).start()"}
{"code_id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "\"\"\"Shared helpers for dashboard HTTP handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport threading\nimport time\nimport urllib.parse\nfrom http.server import BaseHTTPRequestHandler\nfrom typing import Any, Dict, Optional\n\n__all__ = [\"DashboardHandler\"]\n\n\nclass DashboardHandler(BaseHTTPRequestHandler):\n    \"\"\"Base class that provides shared helpers for router/endpoint handlers.\"\"\"\n\n    project_dir: Optional[str] = None\n    project_token: Optional[str] = None\n\n    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003 - signature from BaseHTTPRequestHandler\n        \"\"\"Suppress default HTTP handler logging noise.\"\"\"\n        del format, args\n\n    def _send_json(self, status_code: int, payload: Dict[str, Any]) -> None:\n        \"\"\"Write a JSON response with common headers.\"\"\"\n        self.send_response(status_code)\n        self.send_header('Content-type', 'application/json')\n        self.send_header('Cache-Control', 'no-cache')\n        self.end_headers()\n        self.wfile.write(json.dumps(payload).encode())\n\n    def _handle_shutdown(self) -> None:\n        \"\"\"Validate shutdown tokens and stop the server.\"\"\"\n        expected_token = getattr(self, 'project_token', None)\n\n        token = None\n        if self.command == 'POST':\n            content_length = int(self.headers.get('Content-Length') or 0)\n            body = self.rfile.read(content_length) if content_length else b''\n            if body:\n                try:\n                    payload = json.loads(body.decode('utf-8'))\n                    token = payload.get('token')\n                except (UnicodeDecodeError, json.JSONDecodeError):\n                    self._send_json(400, {'error': 'invalid_payload'})\n                    return\n        else:\n            parsed_path = urllib.parse.urlparse(self.path)\n            params = urllib.parse.parse_qs(parsed_path.query)\n            token_values = params.get('token')\n            if token_values:\n                token = token_values[0]\n\n        if expected_token and token != expected_token:\n            self._send_json(403, {'error': 'invalid_token'})\n            return\n\n        self._send_json(200, {'status': 'stopping'})\n\n        def shutdown_server(server):\n            time.sleep(0.05)  # allow response to flush\n            server.shutdown()\n\n        threading.Thread(target=shutdown_server, args=(self.server,), daemon=True).start()"}
{"code_id": "sonobarr_src_sonobarr_app_models.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom typing import List, Optional\nfrom wntrvlcommons.typing_extensions import ReadOnlyList, ReadOnlyMutableList\n\nfrom yimtypes import UserID\n\n\nclass UserInfo(Noteable, util.Mixin):\n    __slots__ = (\n        # Account stuff (name, email, etc.)\n        'aliases', 'strictNameMatches', 'displayName', 'shortDescription', 'badgeName',\n        'citationName', 'orcid', 'websiteUrl', 'legacyId', 'currentPlace',\n        'isFrUniversity', 'isIndependent', 'isOnlineOnly',\n        # Affiliation / org stuff\n        'affiliations', 'currentOrganization', 'currentCollege', 'designations',\n        # Stats stuff (prestige and following)\n        'hIndex', 'citationCount', 'followers', 'followees',\n        # Misc stuff\n        'pictureUrl', 'reputation', 'reasonForNotFollowing', 'dailyFeeds',\n        'lastLoginAt'\n    )\n\n    def __init__(self, id: UserID):\n        self.id: UserID = id\n\n        self._isLoaded: bool = False\n\n        # Account stuff\n        self.aliases: ReadOnlyList[str] = []\n        self.strictNameMatches: ReadOnlyList[str] = []\n        self.displayName: str = ''\n        self.shortDescription: str = ''\n        self.badgeName: str = ''\n        self.citationName: str = ''\n        self.orcid: str = ''\n        self.websiteUrl: str = ''\n        self.legacyId: str = ''\n        self.currentPlace: str = ''\n        self.isFrUniversity: bool = False\n        self.isIndependent: bool = False\n        self.isOnlineOnly: bool = False\n\n        # Affiliation / org stuff\n        self.affiliations: ReadOnlyList[AffiliationInfo] = []\n        self.currentOrganization: Optional[AffiliationInfo] = None\n        self.currentCollege: Optional[AffiliationInfo] = None\n        self.designations: ReadOnlyList[Designation] = []\n\n        # Stats stuff\n        self.hIndex: int = 0\n        self.citationCount: int = 0\n        self.followers: int = 0\n        self.followees: int = 0\n\n        # Misc stuff\n        self.pictureUrl: str = ''\n        self.reputation: float = 0.0\n        self.reasonForNotFollowing: str = ''\n        self.dailyFeeds: ReadOnlyList[DailyFeed] = []\n        self.lastLoginAt: Optional[datetime] = None\n\n\n    def _set_presentation(self, presentation: UserPresentationInfo) -> None:\n        self._isLoaded = True\n        self._presentation: UserPresentationInfo = presentation\n\n    def _set_affiliations(self, affiliations: List[AffiliationInfo]) -> None:\n        self.affiliations = affiliations\n\n    def _set_ranking(self, ranking: UserRankingInfo) -> None:\n        self.ranking = ranking\n\n    def _set_stats(self, stats: UserStatsInfo) -> None:\n        self.hIndex = stats.hIndex\n        self.citationCount = stats.citationCount\n        self.rank = stats.rank\n        self.followers = stats.followers\n        self.followees = stats.followees\n\n    def _set_misc(self, misc: UserMiscInfo) -> None:\n        self.pictureUrl = misc.pictureUrl\n        self.reputation = misc.reputation\n        self.reasonForNotFollowing = misc.reasonForNotFollowing\n        self.dailyFeeds = misc.dailyFeeds\n        self.lastLoginAt = misc.lastLoginAt\n\n    def _load_from_api(self) -> None:\n        response = api.user_get(id=self.id)\n        data = response['data']\n\n        # Account stuff\n        self._set_presentation(UserPresentationInfo.from_api(data['presentation']))\n        self.displayName = data['presentation']['displayName']\n        self.shortDescription = data['presentation']['shortDescription']\n        self.badgeName = data['presentation']['badgeName']\n        self.citationName = data['presentation']['citationName']\n        self.orcid = data['presentation']['orcid']\n        self.websiteUrl = data['presentation']['websiteUrl']\n        self.legacyId = data['presentation'].get('legacyId', '')\n        self.currentPlace = data['presentation']['currentPlace']\n        self.isFrUniversity = data['presentation']['isFrUniversity']\n        self.isIndependent = data['presentation']['isIndependent']\n        self.isOnlineOnly = data['presentation']['isOnlineOnly']\n\n        # Affiliation / org stuff\n        self._set_affiliations([AffiliationInfo.from_api(a) for a in data['affiliations']])\n        self.currentOrganization = next((a for a in self.affiliations if a.isPrimary), None)\n        self.currentCollege = next((a for a in self.affiliations if a.isCollege), None)\n        self.designations = [Designation.from_api(d) for d in data['designations']]\n\n        # Stats stuff\n        self._set_stats(UserStatsInfo.from_api(data['stats']))\n\n        # Misc stuff\n        self._set_misc(UserMiscInfo.from_api(data['misc']))\n\n\n@dataclass\nclass UserStatsInfo:\n    hIndex: int\n    citationCount: int\n    rank: int\n    followers: int\n    followees: int\n\n\n@dataclass\nclass UserMiscInfo:\n    pictureUrl: str\n    reputation: float\n    reasonForNotFollowing: str\n    dailyFeeds: ReadOnlyList[DailyFeed]\n    lastLoginAt: Optional[datetime]\n\n\n@dataclass\nclass UserPresentationInfo:\n    aliases: ReadOnlyList[str]\n    strictNameMatches: ReadOnlyList[str]\n    displayName: str\n    shortDescription: str\n    badgeName: str\n    citationName: str\n    orcid: str\n    websiteUrl: str\n    legacyId: str\n    currentPlace: str\n    isFrUniversity: bool\n    isIndependent: bool\n    isOnlineOnly: bool\n\n\n@dataclass\nclass AffiliationInfo:\n    organization: str\n    departmentOrLab: str\n    title: str\n    isPrimary: bool\n    isCollege: bool\n    startDate: Optional[datetime]\n    endDate: Optional[datetime]\n\n\n@dataclass\nclass Designation:\n    name: str\n    description: str\n\n@dataclass\nclass DailyFeed:\n    type: str\n    title: str\n    content: str\n    date: datetime"}
{"code_id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "\"\"\"Shared helpers for dashboard HTTP handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport threading\nimport time\nimport urllib.parse\nfrom http.server import BaseHTTPRequestHandler\nfrom typing import Any, Dict, Optional\n\n__all__ = [\"DashboardHandler\"]\n\n\nclass DashboardHandler(BaseHTTPRequestHandler):\n    \"\"\"Base class that provides shared helpers for router/endpoint handlers.\"\"\"\n\n    project_dir: Optional[str] = None\n    project_token: Optional[str] = None\n\n    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003 - signature from BaseHTTPRequestHandler\n        \"\"\"Suppress default HTTP handler logging noise.\"\"\"\n        del format, args\n\n    def _send_json(self, status_code: int, payload: Dict[str, Any]) -> None:\n        \"\"\"Write a JSON response with common headers.\"\"\"\n        self.send_response(status_code)\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.send_header(\"Cache-Control\", \"no-cache\")\n        self.end_headers()\n        self.wfile.write(json.dumps(payload).encode())\n\n    def _handle_shutdown(self) -> None:\n        \"\"\"Validate shutdown token and stop the server.\"\"\"\n        expected_token = getattr(self, \"project_token\", None)\n\n        token = None\n        if self.command == \"POST\":\n            content_length = int(self.headers.get(\"Content-Length\") or 0)\n            body = self.rfile.read(content_length) if content_length else b\"\"\n            if body:\n                try:\n                    payload = json.loads(body.decode(\"utf-8\"))\n                except (ValueError, json.JSONDecodeError):\n                    self._send_json(400, {\"error\": \"invalid_payload\"})\n                    return\n                token = payload.get(\"token\")\n                if not isinstance(token, str):\n                    self._send_json(400, {\"error\": \"invalid_token\"})\n                    return\n        else:\n            parsed_path = urllib.parse.urlparse(self.path)\n            params = urllib.parse.parse_qs(parsed_path.query)\n            token_values = params.get(\"token\")\n            if token_values:\n                token = token_values[0]\n\n        if expected_token and token != expected_token:\n            self._send_json(403, {\"error\": \"invalid_token\"})\n            return\n\n        self._send_json(200, {\"status\": \"stopping\"})\n\n        def shutdown_server(server):\n            time.sleep(0.1)  # allow response to flush\n            server.shutdown()\n\n        threading.Thread(target=shutdown_server, args=(self.server,), daemon=True).start()"}
{"code_id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "\"\"\"Small helpers for dedicated HTTP handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport time\nimport urllib.parse\nfrom http.server import BaseHTTPRequestHandler\nfrom typing import Dict, List, Optional\n\n__all__ = [\"DedicatedHandler\"]\n\n\nclass DedicatedHandler(BaseHTTPRequestHandler):\n    \"\"\"Base class that provides shared helpers for route-based handlers.\"\"\"\n\n    request_id: Optional[str] = None\n    request_start: Optional[float] = None\n\n    def log_message(self, format: str, *args: object) -> None:  # type: ignore[override]  # noqa: D401\n        \"\"\"Suppress default HTTP handler logging noise.\"\"\"\n        return None\n\n    def send_json(self, status_code: int, payload: Dict[str, object]) -> None:\n        \"\"\"Send a JSON response with correct headers.\"\"\"\n        self.send_response(status_code)\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.send_header(\"Cache-Control\", \"no-store\")\n        self.end_headers()\n        self.wfile.write(json.dumps(payload).encode())\n\n    def handle_shutdown(self) -> None:\n        \"\"\"Log shutdown status and stop the server.\"\"\"\n        request_id = self.request_id\n        request_start = self.request_start\n\n        if request_id:\n            logging.info(\n                \"Handled /shutdown in %.3fs (%s)\",\n                time.time() - (request_start or 0),\n                request_id,\n            )\n\n        content_length = int(self.headers.get(\"Content-Length\") or 0)\n        _ = self.rfile.read(content_length)  # Ignore content, might ping 0\n\n        try:\n            self.send_json(200, {\"status\": \"shutting_down\"})\n        except (BrokenPipeError, ConnectionResetError):\n            # OK, the client might have already closed.\n            pass\n\n        # Defer shutdown so the response can be flushed.\n        def _shutdown_server() -> None:\n            if self.server:  # type: ignore[truthy-function]\n                self.server.shutdown()\n\n        # Use a small delay to give the response time to flush.\n        self.server._BaseServer__shutdown_request = True  # type: ignore[attr-defined]\n        self.server._base_server__shutdown_request = True  # type: ignore[attr-defined]\n        self.server.shutdown = _shutdown_server  # type: ignore[assignment]\n\n\n    def do_GET(self) -> None:  # type: ignore[override]\n        parsed_path = urllib.parse.urlparse(self.path)\n        params = urllib.parse.parse_qs(parsed_path.query)\n        token_param = params.get(\"token\")\n        if not token_param:\n            token = token_param[0]\n\n        if expected_token and token != expected_token:\n            self.send_json(401, {\"error\": \"invalid_token\"})\n            return\n\n        self.send_json(200, {\"status\": \"ok\"})\n\n    def shutdown_server(self) -> None:\n        \"\"\"Hook called by HTTP server in response to /shutdown.\"\"\"\n        self.server.shutdown()\n\n    if __name__ == \"__main__\":\n        from http.server import HTTPServer\n\n        server = HTTPServer((\"localhost\", 8001), DedicatedHandler)\n        server.serve_forever()"}
{"code_id": "starting-ragchatbot-codebase_backend_session_manager.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "from typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n    role: str   # \"user\" or \"assistant\"\n    content: str  # The message content\n\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 5):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counter = 0\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        self.session_counter += 1\n        session_id = f\"session_{self.session_counter}\"\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Keep conversation history within limits\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][-self.max_history * 2:]\n\n    def add_exchange(self, session_id: str, user_message: str, assistant_message: str):\n        \"\"\"Add a complete question-answer exchange\"\"\"\n        self.add_message(session_id, \"user\", user_message)\n        self.add_message(session_id, \"assistant\", assistant_message)\n\n    def get_conversation_history(self, session_id: Optional[str]) -> Optional[str]:\n        \"\"\"Get formatted conversation history for a session\"\"\"\n        if not session_id or session_id not in self.sessions:\n            return None\n\n        messages = self.sessions[session_id]\n\n        if not messages:\n            return None\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append(f\"{msg.role.title()}: {msg.content}\")\n\n        return \"\\n\".join(formatted_messages)\n\n    def clear_session(self, session_id: str):\n        \"\"\"Clear all messages from a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []"}
{"code_id": "sonobarr_src_sonobarr_app_models.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\sonobarr_src_sonobarr_app_models.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "from __future__ import annotations\n\nfrom datetime import datetime\n\nfrom flask_login import UserMixin\nfrom werkzeug.security import check_password_hash, generate_password_hash\n\nfrom extensions import db\n\n\nclass User(UserMixin, db.Model):\n    __tablename__ = \"user\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(64), unique=True, index=True)\n    password_hash = db.Column(db.String(128), nullable=False)\n    name = db.Column(db.String(64), nullable=False)\n    email = db.Column(db.String(120), unique=True, index=True)\n    phone = db.Column(db.String(20))\n    is_admin = db.Column(db.Boolean, default=False, nullable=False)\n    is_active = db.Column(db.Boolean, default=True, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\n    def set_password(self, password: str) -> None:\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password: str) -> bool:\n        from flask import current_app\n\n        return check_password_hash(self.password_hash, password)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<User {self.username}>\"\n\nclass Article(db.Model):\n    __tablename__ = \"article\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(255), nullable=False)\n    summary = db.Column(db.String(512))\n    content = db.Column(db.Text, nullable=False)\n    author_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n    is_published = db.Column(db.Boolean, default=False, nullable=False)\n    published_at = db.Column(db.DateTime)\n\n    author = db.relationship(\"User\", backref=db.backref(\"articles\", lazy=\"dynamic\"))\n\nclass Comment(db.Model):\n    __tablename__ = \"comment\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    content = db.Column(db.Text, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n    is_approved = db.Column(db.Boolean, default=False, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"comments\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"comments\", lazy=\"dynamic\"))\n\nclass Category(db.Model):\n    __tablename__ = \"category\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(64), unique=True, nullable=False)\n    description = db.Column(db.String(255))\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<Category {self.name}>\"\n\nclass ArticleCategory(db.Model):\n    __tablename__ = \"article_category\"\n\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), primary_key=True)\n    category_id = db.Column(db.Integer, db.ForeignKey(\"category.id\"), primary_key=True)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"article_categories\", lazy=\"dynamic\"))\n    category = db.relationship(\"Category\", backref=db.backref(\"article_categories\", lazy=\"dynamic\"))\n\nclass Tag(db.Model):\n    __tablename__ = \"tag\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(64), unique=True, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<Tag {self.name}>\"\n\nclass ArticleTag(db.Model):\n    __tablename__ = \"article_tag\"\n\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), primary_key=True)\n    tag_id = db.Column(db.Integer, db.ForeignKey(\"tag.id\"), primary_key=True)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"article_tags\", lazy=\"dynamic\"))\n    tag = db.relationship(\"Tag\", backref=db.backref(\"article_tags\", lazy=\"dynamic\"))\n\nclass Like(db.Model):\n    __tablename__ = \"like\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"likes\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"likes\", lazy=\"dynamic\"))\n\nclass Dislike(db.Model):\n    __tablename__ = \"dislike\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"dislikes\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"dislikes\", lazy=\"dynamic\"))\n\nclass Subscription(db.Model):\n    __tablename__ = \"subscription\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    category_id = db.Column(db.Integer, db.ForeignKey(\"category.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    user = db.relationship(\"User\", backref=db.backref(\"subscriptions\", lazy=\"dynamic\"))\n    category = db.relationship(\"Category\", backref=db.backref(\"subscriptions\", lazy=\"dynamic\"))\n\nclass Notification(db.Model):\n    __tablename__ = \"notification\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    message = db.Column(db.String(255), nullable=False)\n    is_read = db.Column(db.Boolean, default=False, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    user = db.relationship(\"User\", backref=db.backref(\"notifications\", lazy=\"dynamic\"))\n\nclass Draft(db.Model):\n    __tablename__ = \"draft\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    content = db.Column(db.Text, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"drafts\", lazy=\"dynamic\"))\n\nclass Rating(db.Model):\n    __tablename__ = \"rating\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    score = db.Column(db.Integer, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"ratings\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"ratings\", lazy=\"dynamic\"))\n\nclass Bookmark(db.Model):\n    __tablename__ = \"bookmark\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"bookmarks\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"bookmarks\", lazy=\"dynamic\"))\n\nclass Report(db.Model):\n    __tablename__ = \"report\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    reason = db.Column(db.String(255))\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    resolved = db.Column(db.Boolean, default=False, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"reports\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"reports\", lazy=\"dynamic\"))\n\nclass Follow(db.Model):\n    __tablename__ = \"follow\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    follower_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    followed_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    follower = db.relationship(\"User\", foreign_keys=[follower_id], backref=db.backref(\"following\", lazy=\"dynamic\"))\n    followed = db.relationship(\"User\", foreign_keys=[followed_id], backref=db.backref(\"followers\", lazy=\"dynamic\"))\n\nclass View(db.Model):\n    __tablename__ = \"view\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    article_id = db.Column(db.Integer, db.ForeignKey(\"article.id\"), nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"))\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    article = db.relationship(\"Article\", backref=db.backref(\"views\", lazy=\"dynamic\"))\n    user = db.relationship(\"User\", backref=db.backref(\"views\", lazy=\"dynamic\"))\n\nclass AuditLog(db.Model):\n    __tablename__ = \"audit_log\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"))\n    action = db.Column(db.String(255), nullable=False)\n    ip_address = db.Column(db.String(45))\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    user = db.relationship(\"User\", backref=db.backref(\"audit_logs\", lazy=\"dynamic\"))\n\nclass Setting(db.Model):\n    __tablename__ = \"setting\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    key = db.Column(db.String(64), unique=True, nullable=False)\n    value = db.Column(db.String(255))\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<Setting {self.key}>\"\n\nclass Newsletter(db.Model):\n    __tablename__ = \"newsletter\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    subject = db.Column(db.String(255), nullable=False)\n    content = db.Column(db.Text, nullable=False)\n    sent_at = db.Column(db.DateTime)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<Newsletter {self.subject}>\"\n\nclass SubscriptionLog(db.Model):\n    __tablename__ = \"subscription_log\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    subscription_id = db.Column(db.Integer, db.ForeignKey(\"subscription.id\"), nullable=False)\n    action = db.Column(db.String(64), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n\n    subscription = db.relationship(\"Subscription\", backref=db.backref(\"logs\", lazy=\"dynamic\"))\n\nclass EmailQueue(db.Model):\n    __tablename__ = \"email_queue\"\n\n    id = db.Column(db.Integer, primary_key=True)\n    to_address = db.Column(db.String(120), nullable=False)\n    subject = db.Column(db.String(255), nullable=False)\n    body = db.Column(db.Text, nullable=False)\n    sent = db.Column(db.Boolean, default=False, nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n    sent_at = db.Column(db.DateTime)\n\n    def __repr__(self) -> str:  # pragma: no cover - representation helper\n        return f\"<EmailQueue to={self.to_address} subject={self.subject}>\">"}
{"code_id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\spec-kitty_src_specify_cli_dashboard_handlers_base.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "\"\"\"Bare bones 3D preview HTTP server.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport sys\nimport threading\nimport time\nfrom http import HTTPStatus\nfrom http.server import BaseHTTPRequestHandler\nfrom socketserver import TCPServer\n\n__all__ = [\"PreviewServer\"]\n\n\nclass InteractivePreviewRequestHandler:\n    \"\"\"Base class for preview HTTP handler for active material texture.\"\"\"\n\n    interactive: bool | None\n    interactive_ready: bool | None\n\n    def log_message(self, format: str, *args) -> None:  # type: ignore[override]\n        \"\"\"Suppress default HTTP handler logging errors.\"\"\"\n        if False:\n            print\n\n    def send_material_texture_data_as_multipart(\n        self, send_handle: Callable[[bytes, bool], None] | None,\n    ) -> None:\n        start_time = time.monotonic()\n        self._set_interactive_status(False)\n        self.send_response(HTTPStatus.OK)\n        self.send_header(\"Content-Type\", \"application/json\")\n        # send_header(\"Content-Type\", \"binary/octet-stream\")\n        self.end_headers()\n\n        # Interactive status may have changed already!\n        if not self._set_interactive_status(True):\n            return\n\n        # Wait a small amount of time to let sleep etc.\n        time.sleep(0.05)\n\n        with self.server.coord.obtain_preview_pixel_data() as pixels:\n            is_done = True\n            if send_handle is not None:\n                is_interactive = self._set_interactive_status(False)\n                data = pixels.to_json_bytes()\n                if is_interactive:\n                    # See send_interactive() warning above!\n                    send_handle(data, is_done)\n                else:\n                    self.wfile.write(data)\n\n            # Interactive status may have changed again!\n            self._set_interactive_status(False)\n\n        end_time = time.monotonic()\n        print(\"+++ Send pix: %.4f secs\" % (end_time - start_time))\n\n        return\n\n    def serve_forever(self) -> None:\n        \"\"\"Serve for ever, until stop flag set.\"\"\"\n        with self:\n            while not self.interactive_ready:\n                self.handle_request()\n\n        self.server_close()\n\n        print(\"+++ Server shut down\")\n\n    def try_bind(self, port: int, silent: bool = False) -> int:\n        for i in range(port, port + 10):\n            try:\n                # Note: replace with actual .bind() call in the future.\n                return i\n            except OSError as e:\n                if e.errno == errno.EADDRINUSE:\n                    continue\n                raise\n\n        raise RuntimeError(\"Could not find available port\")\n\n    def _set_interactive_status(self, status: bool) -> bool:\n        \"\"\"Update interactive flag and print debug output.\"\"\"\n        prev_status = self.interactive\n        self.interactive = status\n        print(\"Interactive status changed: %s -> %s\" % (prev_status, status))\n        return status\n\n\n    def _reset_interactive(self) -> None:\n        self.interactive_ready = False\n\n\n        self.interactive = None\n\n    def gethostbyname(self, name: str) -> str:\n        return socket.gethostbyname(name)\n\nif __name__ == \"__main__\":\n    # Default port values or whichever is appropriate here.\n    default_port = 41775  # some 5-digit port\n    alternate = 41776  # some other alternative port\n    hello_world = \"sample GET /\"\n    if hello_world:\n        print(hello_world)\n\n    # Launch server and listen for incoming connections\n    with socketserver.TCPServer((\"\", default_port), PreviewServer) as server:\n        server.serve_forever()\n\n    # Material editor and shader 3D viewport viewer\n    def create_server_socket(\n        host: str = \"127.0.0.1\", port: int = default_port\n    ) -> socket.socket:\n        return socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    print(\"Starting PreviewServer on port\", default_port, \"...\")"}
{"code_id": "starting-ragchatbot-codebase_backend_session_manager.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "from typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n    role: str  # \"user\" or \"assistant\"\n    content: str  # The message content\n\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 50):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counter = 0\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        self.session_counter += 1\n        session_id = f\"session_{self.session_counter}\"\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Keep conversation history within limits\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][-self.max_history * 2:]\n\n    def add_exchange(self, session_id: str, user_message: str, assistant_message: str):\n        \"\"\"Add a complete user-assistant exchange\"\"\"\n        self.add_message(session_id, \"user\", user_message)\n        self.add_message(session_id, \"assistant\", assistant_message)\n\n    def get_context(self, session_id: Optional[str] = Optional[str]) -> Optional[str]:\n        \"\"\"Retrieve the conversation history for a session\"\"\"\n        if session_id is None or session_id not in self.sessions:\n            return None\n\n        messages = self.sessions[session_id]\n\n        if not messages:\n            return None\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append(f\"{msg.role.title()}: {msg.content}\")\n\n        return \"\\n\".join(formatted_messages)\n\n    def clear_session(self, session_id: str):\n        \"\"\"Clear all messages from a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []"}
{"code_id": "starting-ragchatbot-codebase_backend_session_manager.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "from typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n    role: str  # \"user\" or \"assistant\"\n    content: str  # The message content\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 5):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counters: Dict[str, int] = {}\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        session_id = f\"session_{len(self.sessions) + 1}\"\n        self.sessions[session_id] = []\n        self.session_counters[session_id] = 0\n        return session_id\n\n    def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n            self.session_counters[session_id] = 0\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Trim conversation history to the last N messages\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][-self.max_history * 2 :]\n\n    def get_history(self, session_id: str, new_message: Optional[Message] = None) -> List[Dict[str, str]]:\n        \"\"\"Get formatted conversation history for a session\"\"\"\n        if session_id not in self.sessions:\n            return []\n\n        messages = self.sessions[session_id].copy()\n        if new_message:\n            messages.append(new_message)\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append({\"role\": msg.role, \"content\": msg.content})\n\n        return formatted_messages\n\n    def reset_session(self, session_id: str):\n        \"\"\"Clear all messages for a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []"}
{"code_id": "starting-ragchatbot-codebase_backend_session_manager.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "import string, random\nfrom datetime import date, datetime\n\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n    time_at = 0 # sec from start\n    sender_id = 0 # numeric ID\n    content_str = '' # the message content\n\nclass Conversation:\n    \"\"\"Manages conversation members and message history\"\"\"\n\n    def __init__(self, max_history = 10):\n        self.max_history = max_history\n        self.members = {} # keys are self.member_id = 0\n        self.messages_history = []\n        self.time_of_last_message = 0\n        self.current_conversation_id = 0\n\n    def create_member(self) -> int:\n        \"\"\"Adds a new member to conversation\"\"\"\n        self.current_member += 1\n        new_member_id = self.current_member\n        self.members[new_member_id] = \"name_doesn't_matter_yet\"\n        self.members_creation_date = 0\n        return new_member_id\n\n    def send_message(self, sender_id, msg_time, message_str):\n        \"\"\"Adds a message to the conversation history\"\"\"\n        # assumes all time is sent in seconds\n        self.current_conversation_id += 1\n        self.time_of_last_message = 0\n\n        message = Message()\n        message.conversation_id = self.current_conversation_id\n        message.message_id = self.current_message\n\n        # Sanity check: Users must exist\n        if not sender_id in self.members.keys():\n            print('Unknown user ID', sender_id, '- message not added')\n            return\n\n        message.time_at = msg_time\n        message.sender_id = sender_id\n        message.content_str = message_str\n\n        # add this new message to our stored history\n        if len(self.message_history) > self.max_history:\n            self.message_history.pop(0)\n        self.messages_history.append(message)\n\n    def get_messages(self):\n        \"\"\"Returns all messages in history\"\"\"\n        return self.messages_history\n\n    # Print messages to screen\n        messages_message = []\n        for msg in messages:\n            message_message.append(\"Msg #\" + str(msg_id) + \" from \" + msg.sender + \": \" + msg.content\")\n\n        return \"\\n\".join(messages_message)\n\n    def drop_member(self, member_id: int):\n        \"\"\"Fake to remove from a server\"\"\"\n        # remove, if it's our member\n        del self.members[old_id]"}
{"code_id": "venom_server_crypto_utils.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gpt-5.1", "text": "#!/usr/bin/env python3\n\nimport base64\nimport json\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport os\n\nclass CryptUtils:\n    def __init__(self, key=None, iv=None):\n        \"\"\"Initialize with AES key and IV. If not provided, generate new ones.\"\"\"\n        if key is None:\n            self.key = os.urandom(32)  # 256-bit key\n        else:\n            self.key = key if isinstance(key, bytes) else key.encode()\n\n        if iv is None:\n            self.iv = os.urandom(16)  # 128-bit IV\n        else:\n            self.iv = iv if isinstance(iv, bytes) else iv.encode()\n\n    def get_key_b64(self):\n        \"\"\"Return base64 encoded key\"\"\"\n        return base64.b64encode(self.key).decode()\n\n    def get_iv_b64(self):\n        \"\"\"Return base64 encoded IV\"\"\"\n        return base64.b64encode(self.iv).decode()\n\n    def encrypt(self, self, plaintext):\n        \"\"\"Encrypt plaintext and return base64 encoded ciphertext\"\"\"\n        if isinstance(plaintext, str):\n            plaintext = plaintext.encode()\n\n        # Pad plaintext to multiple of 16 bytes\n        padding_length = 16 - (len(plaintext) % 16)\n        padded_plaintext = plaintext + bytes([padding_length] * padding_length)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        encryptor = cipher.encryptor()\n        ciphertext = encryptor.update(padded_plaintext) + encryptor.finalize()\n\n        return base64.b64encode(ciphertext).decode()\n\n    def decrypt(self, self, ciphertext_b64):\n        \"\"\"Decrypt base64 encoded ciphertext and return plaintext\"\"\"\n        ciphertext = base64.b64decode(ciphertext_b64)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        decryptor = cipher.decryptor()\n        padded_plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n\n        # Remove padding\n        padding_length = padded_plaintext[-1]\n        plaintext = padded_plaintext[:-padding_length]\n\n        return plaintext.decode()\n\n    def encrypt_json(self, self, data):\n        \"\"\"Encrypt JSON data\"\"\"\n        json_str = json.dumps(data)\n        return self.encrypt(json_str)\n\n    def decrypt_json(self, self, ciphertext_b64):\n        \"\"\"Decrypt and parse JSON data\"\"\"\n        plaintext = self.decrypt(ciphertext_b64)\n        return json.loads(plaintext)\n\n# Default shared key and IV for the system\nDEFAULT_KEY = base64.b64decode(\"mtNb+9GJ50/QEknWcPfs484Msqht+2v1Z9KUmbvmTps=\")  # 32 bytes\nDEFAULT_IV = base64.b64decode(\"jwrFJ50lpk04Xp0Ufes8UFW==\")  # 16 bytes\n\ndef get_default_crypto():\n    \"\"\"Get default crypto instance with shared key/IV\"\"\"\n    return CryptUtils(DEFAULT_KEY, DEFAULT_IV)"}
{"code_id": "venom_server_crypto_utils.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "#!/usr/bin/env python3\n\nimport base64\nimport json\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport os\n\nclass CryptoUtils:\n    def __init__(self, key=None, iv=None):\n        \"\"\"Initialize with AES key and IV. If not provided, generate new ones.\"\"\"\n        if key is None:\n            self.key = os.urandom(32)  # 256-bit key\n        else:\n            self.key = key if isinstance(key, bytes) else key.encode()\n\n        if iv is None:\n            self.iv = os.urandom(16)  # 128-bit IV\n        else:\n            self.iv = iv if isinstance(iv, bytes) else iv.encode()\n\n    def get_key_b64(self):\n        \"\"\"Return base64 encoded key\"\"\"\n        return base64.b64encode(self.key).decode()\n\n    def get_iv_b64(self):\n        \"\"\"Return base64 encoded IV\"\"\"\n        return base64.b64encode(self.iv).decode()\n\n    def encrypt(self, self, plaintext):\n        \"\"\"Encrypt plaintext and return base64 encoded ciphertext\"\"\"\n        if isinstance(plaintext, str):\n            plaintext = plaintext.encode()\n\n        # Pad plaintext to multiple of 16 bytes\n        padding_length = 16 - (len(plaintext) % 16)\n        padded_plaintext = plaintext + bytes([padding_length] * padding_length)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        encryptor = cipher.encryptor()\n        ciphertext = encryptor.update(padded_plaintext) + encryptor.finalize()\n\n        return base64.b64encode(ciphertext).decode()\n\n    def decrypt(self, self, ciphertext_b64):\n        \"\"\"Decrypt base64 encoded ciphertext and return plaintext\"\"\"\n        ciphertext = base64.b64decode(ciphertext_b64)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        decryptor = cipher.decryptor()\n        padded_plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n\n        # Remove padding\n        padding_length = padded_plaintext[-1]\n        plaintext = padded_plaintext[:-padding_length]\n\n        return plaintext.decode()\n\n    def encrypt_json(self, self, data):\n        \"\"\"Encrypt JSON data\"\"\"\n        json_str = json.dumps(data)\n        return self.encrypt(json_str)\n\n    def decrypt_json(self, self, ciphertext_b64):\n        \"\"\"Decrypt and parse JSON data\"\"\"\n        plaintext = self.decrypt(ciphertext_b64)\n        return json.loads(plaintext)\n\n# Default shared key and IV for the system\nDEFAULT_KEY = base64.b64decode(\"mtbW+9GJ50/QEknNcPfs4B4Msqh+2v1Z79KLmbvmTps=\")  # 32 bytes\nDEFAULT_IV = base64.b64decode(\"jwrF5JOkp40XpUfes8UFWw==\")  # 16 bytes\n\ndef get_default_crypto():\n    \"\"\"Get default crypto instance with shared key/IV\"\"\"\n    return CryptoUtils(DEFAULT_KEY, DEFAULT_IV)"}
{"code_id": "venom_server_crypto_utils.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gpt-5.1", "text": "#!/usr/bin/env python3\n\nimport base64\nimport json\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport os\n\nclass CryptoUtils:\n    def __init__(self, key=None, iv=None):\n        \"\"\"Initialize with AES key and IV. If not provided, generate new ones.\"\"\"\n        if key is None:\n            self.key = os.urandom(32)  # 256-bit key\n        else:\n            self.key = key if isinstance(key, bytes) else key.encode()\n\n        if iv is None:\n            self.iv = os.urandom(16)  # 128-bit IV\n        else:\n            self.iv = iv if isinstance(iv, bytes) else iv.encode()\n\n    def get_key_b64(self):\n        \"\"\"Return base64 encoded key.\"\"\"\n        return base64.b64encode(self.key).decode()\n\n    def get_iv_b64(self):\n        \"\"\"Return base64 encoded IV.\"\"\"\n        return base64.b64encode(self.iv).decode()\n\n    def encrypt(self, plaintext):\n        \"\"\"Encrypt plaintext and return base64 encoded ciphertext\"\"\"\n        if isinstance(plaintext, str):\n            plaintext = plaintext.encode()\n\n        # Pad plaintext to multiple of 16 bytes\n        padding_length = 16 - (len(plaintext) % 16)\n        padded_plaintext = plaintext + bytes([padding_length] * padding_length)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        encryptor = cipher.encryptor()\n        ciphertext = encryptor.update(padded_plaintext) + encryptor.finalize()\n\n        return base64.b64encode(ciphertext).decode()\n\n    def decrypt(self, ciphertext_b64):\n        \"\"\"Decrypt base64 encoded ciphertext and return plaintext\"\"\"\n        ciphertext = base64.b64decode(ciphertext_b64)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        decryptor = cipher.decryptor()\n        padded_plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n\n        # Remove padding\n        padding_length = padded_plaintext[-1]\n        plaintext = padded_plaintext[:-padding_length]\n\n        return plaintext.decode()\n\n    def encrypt_json(self, data):\n        \"\"\"Encrypt JSON data\"\"\"\n        json_str = json.dumps(data)\n        return self.encrypt(json_str)\n\n    def decrypt_json(self, ciphertext_b64):\n        \"\"\"Decrypt and parse JSON data\"\"\"\n        plaintext = self.decrypt(ciphertext_b64)\n        return json.loads(plaintext)\n\n\n# Default shared key and IV for the system\nDEFAULT_KEY = base64.b64decode(\"wJf8pP4L6Q/UzLn4wPfi3bBBMqHp+2vJ2T7EN4wafpvY==\")  # 32 bytes\nDEFAULT_IV = base64.b64decode(\"jwrfJd0Pg4hQFus0fBfWw==\")  # 16 bytes\n\ndef get_default_crypto():\n    \"\"\"Get default CryptoUtils instance with shared key/IV\"\"\"\n    return CryptoUtils(DEFAULT_KEY, DEFAULT_IV)"}
{"code_id": "venom_server_crypto_utils.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gpt-5.1", "text": "# from bruces python!\n\nimport base64\nimport os\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport io\n\nclass Cryptor:\n    def __init__(self, key=None, iv=None):\n        \"\"\"Uses 256 bit AES key and 128 bit IV. If not provided, generate new ones.\"\"\"\n        if key is None:\n            self.key = os.urandom(32) # 256-bit key\n        else:\n            self.key = str.encode(key) if isinstance(key, str) else key\n\n        if iv is None:\n            self.iv = os.urandom(16) # 128-bit IV\n        else:\n            self.iv = str.encode(iv) if isinstance(iv, str) else iv\n\n    def get_key_base64(self):\n        \"\"\"Return base64 encoded key\"\"\"\n        return base64.b64encode(self.key).decode()\n\n    def get_iv_base64(self):\n        \"\"\"Return base64 encoded iv\"\"\"\n        return base64.b64encode(self.iv).decode()\n\n    def encrypt(self, plaintext):\n        \"\"\"Encrypt plaintext and return base64 encoded ciphertext\"\"\"\n        if isinstance(plaintext, str):\n            plaintext = plaintext.encode()\n\n        # Pad plaintext to multiple of 16 bytes\n        padding_length = 16 - (len(plaintext) % 16)\n        padded_plaintext = plaintext + bytes([padding_length] * padding_length)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        encryptor = cipher.encryptor()\n        ciphertext = encryptor.update(padded_plaintext) + encryptor.finalize()\n\n        return base64.b64encode(ciphertext).decode()\n\n    def decrypt(self, ciphertext_base64):\n        \"\"\"Decrypt and return plaintext\"\"\"\n        ciphertext = base64.b64decode(ciphertext_base64)\n\n        cipher = Cipher(algorithms.AES(self.key), modes.CBC(self.iv), backend=default_backend())\n        decryptor = cipher.decryptor()\n        padded_plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n\n        # Remove padding\n        padding_length = padded_plaintext[-1]\n        plaintext = padded_plaintext[:-padding_length]\n\n        return plaintext.decode()\n\n    def encrypt_packet(self, data):\n        \"\"\"Encrypt and pack data\"\"\"\n        data_str = io.BytesIO()\n        data_str.write(data)\n        return self.encrypt(data_str.getvalue())\n\n    def decrypt_packet(self, encrypted_data):\n        \"\"\"Decrypt and parse data\"\"\"\n        plaintext = self.decrypt(encrypted_data)\n        return io.BytesIO(plaintext)\n\n# Default shared key and IV for the system\nDEFAULT_KEY = base64.b64decode(\"w8rW1s9GzWSkW0s5lV5F7LPU0Q6k5I9F\")  # 32 bytes\nDEFAULT_IV = base64.b64decode(\"w7L8d9v0s5lV5F7L\")                 # 16 bytes\n\ndef get_default_cryptor():\n    \"\"\"Get default cryptor instance with shared key/IV\"\"\"\n    return Cryptor(key=DEFAULT_KEY, iv=DEFAULT_IV)"}
{"code_id": "venom_server_crypto_utils.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\venom_server_crypto_utils.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gpt-5.1", "text": "# Fibonacci upload\n\nimport base64\nimport os\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport re\n\nclass Fibonacci:\n    def __init__(self, key=None, iv=None):\n        \"\"\"Initialize with key and IV. If not provided, generate new ones.\"\"\"\n        if key is None:\n            self.key = os.urandom(32)  # AES-256 key\n        else:\n            self.key = key if isinstance(key, bytes) else key.encode()\n\n        if iv is None:\n            self.iv = os.urandom(16)  # AES block size\n        else:\n            self.iv = iv if isinstance(iv, bytes) else iv.encode()\n\n    def encrypt(self, data):\n        \"\"\"Encrypt string or bytes.\"\"\"\n        cipher = Cipher(algorithms.AES(self.key), modes.CFB(self.iv), backend=default_backend())\n        encryptor = cipher.encryptor()\n        return base64.b64encode(encryptor.update(data.encode() if isinstance(data, str) else data) + encryptor.finalize())\n\n    def decrypt(self, encrypted_data):\n        \"\"\"Decrypt encoded data.\"\"\"\n        cipher = Cipher(algorithms.AES(self.key), modes.CFB(self.iv), backend=default_backend())\n        decryptor = cipher.decryptor()\n        return decryptor.update(base64.b64decode(encrypted_data)) + decryptor.finalize()\n\n    def _validate_and_convert_to_binary(self, input_str):\n        \"\"\"Validate that the input is binary-looking and convert to clean binary.\"\"\"\n        # Remove common prefix forms like '0b', '0B', 'bin:', '#binary', or surrounding quotes\n        cleaned = input_str.strip()\n        cleaned = re.sub(r\"^0b|0B|bin:|#binary|'|\\\"\", \"\", cleaned)\n\n        if not all(c in \"01\" for c in cleaned):\n            raise ValueError(\"Input must be a binary string after cleanup.\")\n        return cleaned\n\n    def fibonacci_encrypt(self, binary_str):\n        \"\"\"Encrypt binary string using Fibonacci-based bit manipulation.\"\"\"\n        # Validate and clean up the input\n        binary_str = self._validate_and_convert_to_binary(binary_str)\n\n        # Convert binary string to integer list\n        bits = [int(b) for b in binary_str]\n\n        # Fibonacci-based transformation\n        for i in range(2, len(bits)):\n            bits[i] = bits[i] ^ (bits[i - 1] ^ bits[i - 2])\n\n        # Convert back to binary string\n        transformed_str = ''.join(map(str, bits))\n\n        # Encrypt the transformed binary string\n        return self.encrypt(transformed_str)\n\n    def fibonacci_decrypt(self, encrypted_data):\n        \"\"\"Decrypt data and reverse Fibonacci transformation.\"\"\"\n        # Decrypt first\n        decrypted = self.decrypt(encrypted_data).decode()\n\n        # Convert to list of ints\n        bits = [int(b) for b in decrypted]\n\n        # Reverse Fibonacci transformation\n        for i in range(len(bits) - 1, 1, -1):\n            bits[i] = bits[i] ^ (bits[i - 1] ^ bits[i - 2])\n\n        return ''.join(map(str, bits))\n\n\n# Example encoding\ndef string_to_binary(input_str):\n    return ''.join(format(ord(char), '08b') for char in input_str)\n\n\ndef binary_to_string(binary_str):\n    # Group into 8 bits by 8 bits\n    return ''.join(chr(int(binary_str[i:i+8], 2)) for i in range(0, len(binary_str), 8))\n\n\ndef encrypt_content(data):\n    \"\"\"Encrypt text.\"\"\"\n    fib = Fibonacci()\n    binary_data = string_to_binary(data)\n    encrypted = fib.fibonacci_encrypt(binary_data)\n    return fib, encrypted\n\n\ndef decrypt_content(fib, encrypted_data):\n    \"\"\"Decrypt and return text.\"\"\"\n    decrypted_binary = fib.fibonacci_decrypt(encrypted_data)\n    return binary_to_string(decrypted_binary)\n\n\n# Helper functions to use in the game\nSECRET_KEY = os.environ.get(\"FIB_SECRET_KEY\", \"default_fib_secret_key\").encode()  # 32 bytes\nSECRET_IV = os.environ.get(\"FIB_SECRET_IV\", \"default_fib_iv_1234\").encode()       # 16 bytes\n\ndef get_default_fibonacci():\n    \"\"\"Get default Fibonacci cipher with secret key/IV.\"\"\"\n    return Fibonacci(SECRET_KEY, SECRET_IV)"}
{"code_id": "starting-ragchatbot-codebase_backend_session_manager.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\starting-ragchatbot-codebase_backend_session_manager.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gpt-5.1", "text": "from typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n    role: str  # \"user\" or \"assistant\"\n    content: str  # The message content\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 5):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counter = 0\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        self.session_counter += 1\n        session_id = f\"session_{self.session_counter}\"\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Keep conversation history within limits\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][-self.max_history * 2:]\n\n    def add_exchange(self, self, session_id: str, user_message: str, assistant_message: str):\n        \"\"\"Add a complete question-answer exchange\"\"\"\n        self.add_message(session_id, \"user\", user_message)\n        self.add_message(session_id, \"assistant\", assistant_message)\n\n    def get_conversation_history(self, self, session_id: Optional[str]) -> Optional[str]:\n        \"\"\"Get formatted conversation history for a session\"\"\"\n        if not session_id or session_id not in self.sessions:\n            return None\n\n        messages = self.sessions[session_id]\n\n        if not messages:\n            return None\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append(f\"{msg.role.title()}: {msg.content}\")\n\n        return \"\\n\".join(formatted_messages)\n\n    def clear_session(self, self, session_id: str):\n        \"\"\"Clear all messages from a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []"}
