{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-70d991679cdd4fe88e234232330996b4", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 3251, "prompt_tokens": 2324, "total_tokens": 5575, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom ..logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config['world_size']\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(f\"Loading SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(0, width - 1)\n            y = self.rng.randint(0, height - 1)\n            agent['position'] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, 'w', encoding='utf-8') as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")", "text_len": 2391}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-d1ffb99f4ea34f11af4361b3ed2729d8", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5375, "prompt_tokens": 2324, "total_tokens": 7699, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom ..logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(f\"Running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(0, width - 1)\n            y = self.rng.randint(0, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")", "text_len": 2391}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-dd9ee91a53464b4fbae032df761c8342", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6498, "prompt_tokens": 2324, "total_tokens": 8822, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom ..logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = No\nne):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(f\"running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(1, width - 1)\n            y = self.rng.randint(1, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")", "text_len": 2392}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-6fbab8ced1be43ab9275d5321b4c7aac", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5807, "prompt_tokens": 2327, "total_tokens": 8134, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom ..logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(f\"running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(1, width - 1)\n            y = self.rng.randint(1, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"]\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")", "text_len": 2390}
{"code_id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-e8dd1208b4df4aab8c8f2e14668cfb9e", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6786, "prompt_tokens": 2324, "total_tokens": 9110, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom .logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpawnGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, spawn_config: Dict[str, Any], output_path: str, seed: int = None) -> None:\n        \"\"\"\n        Initialize the SpawnGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            spawn_config (Dict[str, Any]): The spawn configuration.\n            output_path (str): The path to the output file for agent spawn info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.spawn_config = spawn_config\n        self.world_size = self.spawn_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(\"Creating SpawnGenerator\")\n\n    def _parse_agent_profiles(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._parse_agent_profiles()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(0, width - 1)\n            y = self.rng.randint(0, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                spawn_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"]\n                }\n                f.write(json.dumps(spawn_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")", "text_len": 2398}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-bef7002554ab4651a9babe0d3a92d5eb", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4086, "prompt_tokens": 2327, "total_tokens": 6413, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom TRM: Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self._stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self._stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n        k = min(5, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self._stable_count += 1\n            if self._stable_count >= self.patience:\n                return True\n        else:\n            self._stable_count = 0\n\n        return False", "text_len": 1555}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-65ca0a48e55b4b4a80884f2eff57aeed", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7372, "prompt_tokens": 2327, "total_tokens": 9699, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom TRM: Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self.stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self.stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n        k = min(3, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self.stable_count += 1\n            if self.stable_count >= self.patience:\n                return True\n        else:\n            self.stable_count = 0\n\n        return False", "text_len": 1549}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-f585e13295e4431aa77dcf76ef9ab534", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7914, "prompt_tokens": 2327, "total_tokens": 10241, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom TRM: Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\n\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self._stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self._stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(-prev_scores)\n        curr_order = np.argsort(-curr_scores)\n\n        is_stable = False\n        k = min(5, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self._stable_count += 1\n            if self._stable_count >= self.patience:\n                return True\n        else:\n            self._stable_count = 0\n\n        return False", "text_len": 1555}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-3266f0bb150c4cec9a9b6a87ee29c24d", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7045, "prompt_tokens": 2327, "total_tokens": 9372, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"\nConfidenceEstimator - Estimates confidence to enable early stopping.\n\nFrom PWA Q-learning inspired halting - stop when improvement is minimal.\n\"\"\"\nimport numpy as np\n\nfrom scripts.rerank_recursive.state import RefinementState\n\nclass ConfidenceEstimator:\n    \"\"\"\n    Estimates confidence to enable early stopping.\n\n    Uses patience to avoid stopping on noisy single-step improvements.\n    \"\"\"\n\n    def __init__(self, patience: int = 1, min_improvement: float = 0.01):\n        self.patience = patience\n        self.min_improvement = min_improvement\n        self.stable_count = 0\n\n    def reset(self):\n        \"\"\"Reset state for a new query.\"\"\"\n        self.stable_count = 0\n\n    def should_stop(self, state: RefinementState) -> bool:\n        \"\"\"Check if we should stop refining based on score stability.\"\"\"\n        if len(state.score_history) < 2:\n            return False\n\n        prev_scores = state.score_history[-2]\n        curr_scores = state.scores\n\n        prev_order = np.argsort(prev_scores)\n        curr_order = np.argsort(curr_scores)\n\n        is_stable = False\n        k = min(5, len(prev_order))\n        if np.array_equal(prev_order[:k], curr_order[:k]):\n            is_stable = True\n\n        improvement = np.abs(curr_scores - prev_scores).mean()\n        if improvement < self.min_improvement:\n            is_stable = True\n\n        if is_stable:\n            self.stable_count += 1\n            if self.stable_count >= self.patience:\n                return True\n        else:\n            self.stable_count = 0\n\n        return False", "text_len": 1546}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-eddf49f4f9e443188660b0cd0e31b69e", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7643, "prompt_tokens": 2324, "total_tokens": 9967, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cost:.5f})\"\n        msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]", "text_len": 2354}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-51429b3e9af84fb1bc59d65cd8576a58", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 8416, "prompt_tokens": 2324, "total_tokens": 10740, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cost:.5f})\"\n            msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens}))\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]", "text_len": 2359}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-32488ef4f5804b3d9a2ce7a73e3b29f5", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 8736, "prompt_tokens": 2324, "total_tokens": 11060, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"Wiki generation completed successfully!\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.5f} (LLM: ${self.llm_cost:.5f}, Embedding: ${self.embedding_cos\nt:.5f})\"\n        msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.5f}, Tokens={total_tok\nens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]", "text_len": 2356}
{"code_id": "Context-Engine_scripts_rerank_recursive_confidence.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Context-Engine_scripts_rerank_recursive_confidence.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-cf8f743f1293412eb3226752105336cd", "response_model": "gemini-3-pro-preview", "finish_reason": "length", "usage": {"completion_tokens": 16381, "prompt_tokens": 2327, "total_tokens": 18708, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "", "text_len": 0}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-a4a5d16a578a4a558834c7787f4fed9e", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7672, "prompt_tokens": 2327, "total_tokens": 9999, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, 'CostTracker'] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g., Wiki generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        \n        msg = \"Wiki generation completed successfully!\"\n        \n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.2f} (LLM: ${self.llm_cost:.2f}, Embedding: ${self.embedding_cost:.4f})\"\n            msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n        \n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"[Task {self.task_id}] Cost Summary: Total=${self.get_total_cost():.2f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]", "text_len": 2382}
{"code_id": "HGM_tree.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-1d9db93d79ba426f9c4e658f761691be", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7067, "prompt_tokens": 2327, "total_tokens": 9394, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgm_utils\nfrom hgm_utils import eval_agent, sample_child\n\n\ndef get_num_total_evals():\n    return hgm_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None:  #\n            self.id = len(hgm_utils.nodes)\n        else:\n            self.id = id\n        hgm_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_decendant_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_decendant_evals(self, num_pseudo=10):\n        decendant_evals = self.get_pseudo_decendant_evals(num_pseudo)\n        for decendant in self.get_sub_tree()[1:]:\n            decendant_evals += decendant.utility_measures\n        \n        return decendant_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return np.inf\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }", "text_len": 2049}
{"code_id": "HGM_tree.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-cd9c11f972f744149264133f660f2660", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6574, "prompt_tokens": 2327, "total_tokens": 8901, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgm_utils\nfrom hgm_utils import eval_agent, sample_child\n\ndef get_num_total_evals():\n    return hgm_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None: #\n            self.id = len(hgm_utils.nodes)\n        else:\n            self.id = id\n        hgm_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_decendant_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_decendant_evals(self, num_pseudo=10):\n        decendant_evals = self.get_pseudo_decendant_evals(num_pseudo)\n        for decendant in self.get_sub_tree()[1:]:\n            decendant_evals += decendant.utility_measures\n        \n        return decendant_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return np.inf\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }", "text_len": 2046}
{"code_id": "DeepV-Ki_api_cost_tracker.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\DeepV-Ki_api_cost_tracker.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-46a62a9b92d04d8384daddb01b745965", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 8006, "prompt_tokens": 2327, "total_tokens": 10333, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import logging\nfrom typing import Dict, Optional, Any\n\nlogger = logging.getLogger(__name__)\n\n# Global dictionary to store cost trackers in memory\n_cost_trackers: Dict[str, \"CostTracker\"] = {}\n\nclass CostTracker:\n    \"\"\"\n    Tracks costs for a specific task (e.g. PRD generation).\n    Accumulates LLM and embedding costs.\n    \"\"\"\n    def __init__(self, task_id: str):\n        self.task_id = task_id\n        self.embedding_tokens = 0\n        self.embedding_cost = 0.0\n        self.llm_tokens = 0\n        self.llm_cost = 0.0\n\n    def add_embedding_cost(self, tokens: int, cost: float):\n        \"\"\"Add embedding usage and cost.\"\"\"\n        self.embedding_tokens += tokens\n        self.embedding_cost += cost\n\n    def add_llm_cost(self, prompt_tokens: int, completion_tokens: int, total_tokens: int, cost: float):\n        \"\"\"Add LLM usage and cost.\"\"\"\n        self.llm_tokens += total_tokens\n        self.llm_cost += cost\n\n    def get_total_cost(self) -> float:\n        \"\"\"Get total accumulated cost.\"\"\"\n        return self.embedding_cost + self.llm_cost\n\n    def get_cost_message(self) -> str:\n        \"\"\"\n        Generate a message summarizing the cost.\n        This is used to update the task status message.\n        \"\"\"\n        total_cost = self.get_total_cost()\n        total_tokens = self.llm_tokens + self.embedding_tokens\n\n        msg = \"With generation completed successfully\"\n\n        if total_cost > 0:\n            msg += f\" Total cost: ${total_cost:.4f} (LLM: ${self.llm_cost:.4f}, Embedding: ${self.embedding_cost:.4f})\"\n        msg += f\" Total tokens: {total_tokens} (LLM: {self.llm_tokens}, Embedding: {self.embedding_tokens})\"\n\n        return msg\n\n    def log_summary(self):\n        \"\"\"Log a summary of the costs.\"\"\"\n        total_tokens = self.llm_tokens + self.embedding_tokens\n        logger.info(f\"Task {self.task_id} Cost Summary: Total=${self.get_total_cost():.4f}, Tokens={total_tokens} (LLM={self.llm_tokens}, Embedding={self.embedding_tokens})\")\n\ndef get_cost_tracker(task_id: str) -> CostTracker:\n    \"\"\"Get or create a cost tracker for a task.\"\"\"\n    if task_id not in _cost_trackers:\n        _cost_trackers[task_id] = CostTracker(task_id)\n    return _cost_trackers[task_id]\n\ndef clear_cost_tracker(task_id: str):\n    \"\"\"Remove a cost tracker from memory.\"\"\"\n    if task_id in _cost_trackers:\n        del _cost_trackers[task_id]", "text_len": 2349}
{"code_id": "HGM_tree.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-839a3f512ee3439c862883dcabe525af", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4982, "prompt_tokens": 2327, "total_tokens": 7309, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgm_utils\nfrom hgm_utils import eval_agent, sample_child\n\ndef get_num_total_evals():\n    return hgm_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None: #\n            self.id = len(hgm_utils.nodes)\n        else:\n            self.id = id\n        hgm_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_decendant_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_decendant_evals(self, num_pseudo=10):\n        decendant_evals = self.get_pseudo_decendant_evals(num_pseudo)\n        for decendant in self.get_sub_tree()[1:]:\n            decendant_evals += decendant.utility_measures\n        \n        return decendant_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return np.inf\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }", "text_len": 2046}
{"code_id": "HGM_tree.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-9f3526777fd54b3c91617f14acd33e39", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4939, "prompt_tokens": 2327, "total_tokens": 7266, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgps_utils\nfrom hgps_utils import eval_agent, sample_child\n\ndef get_num_total_evals():\n    return hgps_utils.nodes[0].get_sum(lambda node: node.num_evals)\n\nclass Node:\n    def __init__(\n        self,\n        commit_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None,\n    ):\n        self.commit_id = commit_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None:\n            self.id = len(hgps_utils.nodes)\n        else:\n            self.id = id\n        hgps_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, fn=lambda self: self):\n        if len(self.children) == 0:\n            return [fn(self)]\n        else:\n            nodes_list = [fn(self)]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree(fn))\n            return nodes_list\n\n    def get_pseudo_descendant_evals(self, num_pseudo):\n        return self.utility_measures if self.num_evals < num_pseudo else [self.mean_utility] * num_pseudo\n\n    def get_descendant_evals(self, num_pseudo=0):\n        descendant_evals = self.get_pseudo_descendant_evals(num_pseudo)\n        for descendant in self.get_sub_tree()[1:]:\n            descendant_evals += descendant.utility_measures\n\n        return descendant_evals\n\n    @property\n    def num_evals(self):\n        return len(self.utility_measures)\n\n    @property\n    def mean_utility(self):\n        if self.num_evals == 0:\n            return -np.inf\n        return np.sum(self.utility_measures) / self.num_evals\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def save_as_dict(self):\n        return {\n            \"commit_id\": self.commit_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"num_evals\": self.num_evals,\n        }", "text_len": 2049}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-b1ff297afc664263ae18666f329aaf4f", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6696, "prompt_tokens": 2327, "total_tokens": 9023, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# ------------------------------------------------------------------\n# Heads for downstream tasks\n# ------------------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = {8: 3, 12: 4, 24: 8}[croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                croconet.enc_embed_dim\n                if hook < croconet.enc_depth\n                else croconet.dec_embed_dim\n            )\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out", "text_len": 3013}
{"code_id": "HGM_tree.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\HGM_tree.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-af441e9b42f143daaab8088222a2c784", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 10553, "prompt_tokens": 2324, "total_tokens": 12877, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import os\nimport pickle\nfrom statistics import stdev\n\nimport numpy as np\n\nimport hgs_utils\nfrom hgs_utils import root_agent, nodes_dict\n\ndef get_node_total_score(node):\n    return hgs_utils.nodes[0].get_pseudo_dependent_score(node, node.raw_score)\n\n\nclass Node:\n    def __init__(\n        self,\n        agent_id,\n        utility_measures=None,\n        parent_id=None,\n        id=None\n    ):\n        self.agent_id = agent_id\n        self.children = []\n        if utility_measures:\n            self.utility_measures = utility_measures\n        else:\n            self.utility_measures = []\n        self.parent_id = parent_id\n        if id is None:\n            self.id = len(hgs_utils.nodes)\n        else:\n            self.id = id\n        hgs_utils.nodes[self.id] = self\n\n    def get_sub_tree(self, include_self=True):\n        if len(self.children) == 0:\n            return [self]\n        else:\n            nodes_list = [self]\n            for child in self.children:\n                nodes_list.extend(child.get_sub_tree())\n            return nodes_list\n\n    def get_pseudo_dependent_score(self, node, raw_score):\n        return self.utility_measures if self.is_root() else (self.mean_utility * raw_score)\n    def get_dependent_score(self, raw_score):\n        dependent_score = self.get_pseudo_dependent_score(self, raw_score)\n        for descendant in self.get_sub_tree():\n            dependent_score += descendant.utility_measures\n\n        return dependent_score\n\n    @property\n    def mean_utility(self):\n        return stdev(self.utility_measures)\n\n    @property\n    def raw_utility(self):\n        if self.mean_utility == 0:\n            return 1e-10\n        return np.mean(self.utility_measures) / self.mean_utility\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def to_dict(self):\n        return {\n            \"agent_id\": self.agent_id,\n            \"id\": self.id,\n            \"parent_id\": self.parent_id,\n            \"mean_utility\": self.mean_utility,\n            \"raw_score\": self.raw_score\n        }", "text_len": 2026}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-0cfde15789be4e93895da7e38faaa91d", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6196, "prompt_tokens": 2324, "total_tokens": 8520, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# --------------------------------------------------------\n# Heads for downstream tasks\n# --------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = {0: 3, 12: 4, 24: 8}[croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                croconet.enc_embed_dim\n                if hook < croconet.enc_depth\n                else croconet.dec_embed_dim\n            )\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out", "text_len": 2993}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-9168f2defc2c4d089dbce8634bcdd317", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5689, "prompt_tokens": 2324, "total_tokens": 8013, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n\n# ------------------------------------------------------------------\n# Heads for downstream tasks\n# ------------------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(croconet) takes a CroCoNet and set all layers according to the head and croconet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        hooks_idx=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_idx = hooks_idx\n        self.layer_dims = layer_dims\n\n    def setup(self, croconet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_idx is None:\n            if hasattr(croconet, \"dec_blocks\"):  # encoder + decoder\n                step = {8: 3, 12: 4, 24: 8}[croconet.dec_depth]\n                hooks_idx = [\n                    croconet.dec_depth + croconet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = croconet.enc_depth // 4\n                hooks_idx = [\n                    croconet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_idx = hooks_idx\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_idxs={self.hooks_idx}\"\n            )\n        dpt_args[\"hooks\"] = self.hooks_idx\n        dpt_args[\"layer_dims\"] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                croconet.enc_embed_dim\n                if hook < croconet.enc_depth\n                else croconet.dec_embed_dim\n            )\n            for hook in self.hooks_idx\n        ]\n        dpt_init_args = {\"dim_tokens_enc\": dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info[\"height\"], img_info[\"width\"]))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out", "text_len": 3013}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-19281178b18949d1872825b816cf6f3d", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 8151, "prompt_tokens": 2327, "total_tokens": 10478, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright (c) 2022-present Naver Corporation. All rights reserved.\n# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).\n#\n# ---------------------------------------------------------------------------\n# Heads for downstream tasks\n# ---------------------------------------------------------------------------\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\nA method setup(crossnet) takes a CroCoNet and set all layers according to the head and crossnet attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'\n\"\"\"\nimport torch\nimport torch.nn as nn\nfrom .dpt_block import DPTOutputAdapter\n\nclass PixelwiseTaskWithDPT(nn.Module):\n    \"\"\"DPT module for CroCo.\n    By default, hooks_ids will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(\n        self,\n        dpt,\n        hooks_ids=None,\n        layer_dims=[96, 192, 384, 768],\n        output_width_ratio=1,\n        num_channels=1,\n        postprocess=None,\n        **kwargs,\n    ):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.hooks_ids = hooks_ids\n        self.layer_dims = layer_dims\n\n    def setup(self, crossnet):\n        dpt_args = {\n            \"output_width_ratio\": self.output_width_ratio,\n            \"num_channels\": self.num_channels,\n        }\n        if self.hooks_ids is None:\n            if hasattr(crossnet, \"dec_blocks\"):  # encoder + decoder\n                step = {24: 6, 12: 3, 8: 2, 2: 1}[crossnet.dec_depth]\n                hooks_ids = [\n                    crossnet.dec_depth + crossnet.enc_depth - 1 - i * step\n                    for i in range(3, -1, -1)\n                ]\n            else:  # encoder only\n                step = crossnet.enc_depth // 4\n                hooks_ids = [\n                    crossnet.enc_depth - 1 - i * step for i in range(3, -1, -1)\n                ]\n            self.hooks_ids = hooks_ids\n            print(\n                f\" PixelwiseTaskWithDPT: automatically setting hook_ids={self.hooks_ids}\"\n            )\n        dpt_args['hooks'] = self.hooks_ids\n        dpt_args['layer_dims'] = self.layer_dims\n        self.dpt = DPTOutputAdapter(**dpt_args)\n        dim_tokens = [\n            (\n                crossnet.enc_embed_dim\n                if hook < crossnet.enc_depth\n                else crossnet.dec_embed_dim\n            )\n            for hook in self.hooks_ids\n        ]\n        dpt_init_args = {'dim_tokens_enc': dim_tokens}\n        self.dpt.init(**dpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.dpt(x, image_size=(img_info['height'], img_info['width']))\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out", "text_len": 3036}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-25a623f4590549178637465c7c749468", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5721, "prompt_tokens": 2327, "total_tokens": 8048, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Closed-loop orchestrator that drives the five-pillar pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence\n\nfrom ..interfaces import (\n    ContextBundle,\n    Document,\n    EvaluationSignal,\n    EvolutionDecision,\n    IAcquirer,\n    IConstructor,\n    IEvaluator,\n    IEvolver,\n    IProcessor,\n    LLMRequest,\n    ModelResponse,\n)\nfrom ..models import BaseModelProvider, LLMClient\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    acquired_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: ModelResponse\n    evaluation: EvaluationSignal\n    evolution: EvolutionDecision\n\nclass ClosedLoopOrchestrator:\n    \"\"\"Coordinates the five pillars to form a closed CE loop.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        llm: LLMClient | BaseModelProvider,\n        acquirer: IAcquirer,\n        processors: Sequence[IProcessor] | None,\n        constructor: IConstructor,\n        evaluator: IEvaluator,\n        evolver: IEvolver,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client()\n        else:\n            self.llm = llm\n        self.acquirer = acquirer\n        self.processors = list(processors or [])\n        self.constructor = constructor\n        self.evaluator = evaluator\n        self.evolver = evolver\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.acquirer.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = ModelResponse(text=llm_response.text, metadata=llm_response.raw or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        evolution = self.evolver.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            acquired_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            evolution=evolution,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for idx, doc in enumerate(context.references, start=1):\n            lines.append(f\"[ref-{idx}] {doc.content}\")\n        lines.append(\"Question: {question}\".format(question=request.question))\n        if request.context:\n            lines.append(f\"Additional context: {request.context}\")\n        return \"\\n\\n\".join(filter(None, lines))", "text_len": 2900}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-e9e889186512483eacb8048dae9a90fb", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5405, "prompt_tokens": 2324, "total_tokens": 7729, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Closed-loop orchestrator that drives the five-pillar pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence\n\nfrom ._interfaces import (\n    ContextBundle,\n    Document,\n    EvaluationSignal,\n    EvolutionDecision,\n    IAcquirer,\n    IConstructor,\n    IEvaluator,\n    IEvolver,\n    IProcessor,\n    LLMRequest,\n    ModelResponse,\n)\nfrom ._models import BaseModelProvider, LLMClient\n\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    acquired_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: ModelResponse\n    evaluation: EvaluationSignal\n    evolution: EvolutionDecision\n\n\nclass ClosedLoopOrchestrator:\n    \"\"\"Coordinates the five pillars to form a closed CE loop.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        llm: LLMClient | BaseModelProvider,\n        acquirer: IAcquirer,\n        processors: Sequence[IProcessor] | None,\n        constructor: IConstructor,\n        evaluator: IEvaluator,\n        evolver: IEvolver,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client()\n        else:\n            self.llm = llm\n        self.acquirer = acquirer\n        self.processors = list(processors or [])\n        self.constructor = constructor\n        self.evaluator = evaluator\n        self.evolver = evolver\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.acquirer.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = ModelResponse(text=llm_response.text, metadata=llm_response.raw or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        evolution = self.evolver.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            acquired_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            evolution=evolution,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for idx, doc in enumerate(context.references, start=1):\n            lines.append(f\"[ref-{idx}] {doc.content}\")\n        lines.append(\"Question: {question}\".format(question=request.question))\n        if request.context:\n            lines.append(f\"Additional context: {request.context}\")\n        return \"\\n\\n\".join(filter(None, lines))", "text_len": 2902}
{"code_id": "Human3R_src_croco_models_head_downstream.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Human3R_src_croco_models_head_downstream.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-8f9c5e0506a347c2a27134e41d1a4e94", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 12902, "prompt_tokens": 2327, "total_tokens": 15229, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright (c) 2023-present Maze Corporation. All rights reserved.\n# Licensed under CC-BY-NC-SA 4.0 (non-commercial use only)\n# --------------------------------------------------------\n# Heads for downstream tasks\n# --------------------------------------------------------\n\n\"\"\"\nA head is a module where the __init__ defines only the head hyperparameters.\ninit_weights(crossset) takes a CrossSet and set all layers according to the head and crossset attributes.\nThe forward takes the features as well as a dictionary img_info containing the keys 'width' and 'height'.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import GPTOutputAdapter\n\nclass PretrainedHeadWithGPT(nn.Module):\n    \"\"\"GPT Header for ViT...\n    By default, heads_idx will be equal to\n    # for encoder-only: frequent spread layers\n    # for encoder-decoder: last encoder + 3 equally spread layers of the decoder\n    \"\"\"\n\n    def __init__(self,\n                 gpt,\n                 heads_idx=None,\n                 layer_step=[16, 16, 16, 16],\n                 output_width_ratio=1,\n                 num_channels=1,\n                 postprocess=None,\n                 **kwargs):\n        super(PretrainedHeadWithGPT, self).__init__()\n        self.return_all_tokens = True # backbone needs to return all tokens\n        self.postprocess = postprocess\n        self.output_width_ratio = output_width_ratio\n        self.num_channels = num_channels\n        self.heads_idx = heads_idx\n        self.layer_step = layer_step\n\n    def init_weights(self, crossset):\n        opt_args = {\n            'output_width_ratio': self.output_width_ratio,\n            'num_channels': self.num_channels\n        }\n        if self.heads_idx is None:\n            if hasattr(crossset, 'dec_blocks'): # encoder + decoder\n                step = (0, 1, 2, 3) # crossset.dec_depth\n                heads_idx = []\n                crossset.dec_depth = crossset.dec_depth - 1 - 1 # stop\n                for i in range(0, 4):\n                    heads_idx.append(crossset.dec_depth - step[i])\n            else: # encoder only\n                step = (crossset.enc_depth // 4)\n                heads_idx = []\n                crossset.enc_depth = crossset.enc_depth - 1 - 1 # stop\n                for i in range(0, 4):\n                    heads_idx.append(crossset.enc_depth - i * step)\n            self.heads_idx = heads_idx\n            print(f\" PretrainedHeadWithGPT: automatically setting heads_idx={self.heads_idx}\")\n\n        opt_args['heads'] = self.heads_idx\n        opt_args['layer_step'] = self.layer_step\n        self.gpt = GPTOutputAdapter(**opt_args)\n\n        dim_tokens = []\n        if hasattr(crossset, 'dec_embed'):\n            dim = crossset.dec_embed.dim\n        else:\n            dim = crossset.enc_embed.dim\n\n        for head in self.heads_idx:\n            dim_tokens.append(dim)\n\n        gpt_init_args = {'dim_tokens_in': dim_tokens}\n        self.gpt.init_weights(gpt_init_args)\n\n    def forward(self, x, img_info):\n        out = self.gpt(x, img_info['height'], img_info['width'])\n        if self.postprocess:\n            out = self.postprocess(out)\n        return out", "text_len": 3122}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-253dee36d7194bd8846f28b6101f112d", "response_model": "gemini-3-pro-preview", "finish_reason": "length", "usage": {"completion_tokens": 16381, "prompt_tokens": 2327, "total_tokens": 18708, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "", "text_len": 0}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-3ab11775746a469a8f28295365f075df", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7005, "prompt_tokens": 2327, "total_tokens": 9332, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "\"\"\"Bootstrap orchestrator that drives the Frenckian pipeline.\"\"\"\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom ..interfaces import (\n    ContextBundle,\n    Document,\n    EvaluationResult,\n    EvolutionSource,\n    Evaluator,\n    ModelProvider,\n    Observation,\n    Observer,\n    Processor,\n    LLMRequest,\n    ModelResponse,\n)\nfrom ..models import BaseModelProvider, LLMClient\n\n@dataclass\nclass LoopResult:\n    \"\"\"Outputs collected from a single orchestrator run.\"\"\"\n\n    request: LLMRequest\n    prompt: str\n    retrieved_documents: List[Document]\n    processed_documents: List[Document]\n    context: ContextBundle\n    response: ModelResponse\n    evaluation: EvaluationResult\n    evolution: EvolutionSource\n\nclass FrenckianOrchestrator:\n    \"\"\"Orchestrates the Frenckian to form a closed QA loop.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        llm: LLMClient | BaseModelProvider,\n        observer: Observer,\n        processors: Sequence[Processor] | None,\n        constructor: Constructor,\n        evaluator: Evaluator,\n        source: Source,\n    ) -> None:\n        if isinstance(llm, BaseModelProvider):\n            self.llm = llm.client()\n        else:\n            self.llm = llm\n        self.observer = observer\n        self.processors = list(processors or [])\n        self.constructor = constructor\n        self.evaluator = evaluator\n        self.source = source\n\n    def run(self, request: LLMRequest) -> LoopResult:\n        documents = self.source.acquire(request)\n        processed = documents\n        for processor in self.processors:\n            processed = processor.process(processed, request)\n        context_bundle = self.constructor.construct(processed, request)\n        prompt = self._format_prompt(request, context_bundle)\n        llm_response = self.llm.complete(prompt)\n        response = ModelResponse(text=llm_response.text, metadata=llm_response.meta or {})\n        evaluation = self.evaluator.evaluate(request, response, context_bundle)\n        evolution = self.source.evolve(context_bundle, evaluation)\n        return LoopResult(\n            request=request,\n            prompt=prompt,\n            retrieved_documents=documents,\n            processed_documents=processed,\n            context=context_bundle,\n            response=response,\n            evaluation=evaluation,\n            evolution=evolution,\n        )\n\n    def _format_prompt(self, request: LLMRequest, context: ContextBundle) -> str:\n        lines = [context.instructions or \"\"]\n        for doc_idx, doc in enumerate(context.references, start=1):\n            lines.append(f\"[{doc_idx}] {doc.content}\")\n        lines.append(f\"Question: {request.text}\" if request.text else request.query)\n        if request.context:\n            lines.append(f\"Additional context: {request.context}\")\n        return \"\\n\\n\".join(filter(None, lines))", "text_len": 2889}
{"code_id": "OpenCE_src_opence_core_orchestrator.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenCE_src_opence_core_orchestrator.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-9229623ed2ce42f3bb7ae5e56026bd13", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 8031, "prompt_tokens": 2327, "total_tokens": 10358, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "", "text_len": 0}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-579e704ff72946628fa5062063325797", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7083, "prompt_tokens": 2327, "total_tokens": 9410, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright 2025 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\ndef _load_config(self) -> None:\n    \"\"\"\n    Load Kubernetes configuration from kubeconfig or in-cluster.\n\n    Raises:\n        Exception: If configuration loading fails\n    \"\"\"\n    try:\n        if self.config.kubeconfig_path:\n            # Load from kubeconfig file\n            config.load_kube_config(config_file=self.config.kubeconfig_path)\n        else:\n            # Load in-cluster config (when running inside K8s)\n            config.load_incluster_config()\n    except Exception as e:\n        raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\ndef get_core_v1_api(self) -> CoreV1Api:\n    \"\"\"\n    Get CoreV1Api client instance.\n\n    Returns:\n        CoreV1Api: Kubernetes Core V1 API client\n    \"\"\"\n    if self._core_v1_api is None:\n        self._core_v1_api = client.CoreV1Api()\n    return self._core_v1_api\n\ndef get_custom_objects_api(self) -> CustomObjectsApi:\n    \"\"\"\n    Get CustomObjectsApi client instance for CRD operations.\n\n    Returns:\n        CustomObjectsApi: Kubernetes Custom Objects API client\n    \"\"\"\n    if self._custom_objects_api is None:\n        self._custom_objects_api = client.CustomObjectsApi()\n    return self._custom_objects_api", "text_len": 2658}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-c4d9563965574cb39714f709822b0846", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 3538, "prompt_tokens": 2324, "total_tokens": 5862, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright 2023 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self.config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self.config.kubeconfig_path)\n            else:\n                # Load in-cluster config (when running inside K8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api", "text_len": 2791}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-71d9a6c719874c659f1b4a45bcb518e4", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6559, "prompt_tokens": 2327, "total_tokens": 8886, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright 2025 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self.config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self.config.kubeconfig_path)\n            else:\n                # Load in-cluster config (when running inside K8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api", "text_len": 2792}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-4e9311857a8d46f98b28b80a3f7be001", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4705, "prompt_tokens": 2327, "total_tokens": 7032, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "# Copyright 2025 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nKubernetes client wrapper for managing cluster connections and API access.\n\"\"\"\n\nfrom typing import Optional\nfrom kubernetes import client, config\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi\n\nfrom src.config import KubernetesRuntimeConfig\n\nclass K8sClient:\n    \"\"\"\n    Wrapper for Kubernetes API client with configuration management.\n\n    Handles both in-cluster and kubeconfig-based authentication.\n    \"\"\"\n\n    def __init__(self, k8s_config: KubernetesRuntimeConfig):\n        \"\"\"\n        Initialize Kubernetes client.\n\n        Args:\n            k8s_config: Kubernetes runtime configuration\n\n        Raises:\n            Exception: If unable to load Kubernetes configuration\n        \"\"\"\n        self.config = k8s_config\n        self._load_config()\n        self._core_v1_api: Optional[CoreV1Api] = None\n        self._custom_objects_api: Optional[CustomObjectsApi] = None\n\n    def _load_config(self) -> None:\n        \"\"\"\n        Load Kubernetes configuration from kubeconfig or in-cluster.\n\n        Raises:\n            Exception: If configuration loading fails\n        \"\"\"\n        try:\n            if self.config.kubeconfig_path:\n                # Load from kubeconfig file\n                config.load_kube_config(config_file=self.config.kubeconfig_path)\n            else:\n                # Load incluster config (when running inside K8s)\n                config.load_incluster_config()\n        except Exception as e:\n            raise Exception(f\"Failed to load Kubernetes configuration: {e}\") from e\n\n    def get_core_v1_api(self) -> CoreV1Api:\n        \"\"\"\n        Get CoreV1Api client instance.\n\n        Returns:\n            CoreV1Api: Kubernetes Core V1 API client\n        \"\"\"\n        if self._core_v1_api is None:\n            self._core_v1_api = client.CoreV1Api()\n        return self._core_v1_api\n\n    def get_custom_objects_api(self) -> CustomObjectsApi:\n        \"\"\"\n        Get CustomObjectsApi client instance for CRD operations.\n\n        Returns:\n            CustomObjectsApi: Kubernetes Custom Objects API client\n        \"\"\"\n        if self._custom_objects_api is None:\n            self._custom_objects_api = client.CustomObjectsApi()\n        return self._custom_objects_api", "text_len": 2790}
{"code_id": "OpenSandbox_server_src_services_k8s_client.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\OpenSandbox_server_src_services_k8s_client.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-b9659def8002400fa6aa5e62aa64ebb4", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5595, "prompt_tokens": 2327, "total_tokens": 7922, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "", "text_len": 0}
{"code_id": "RealVideo_core_connection.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-d98a80c98bd148ed9ae8cb85ed2f7fb5", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5297, "prompt_tokens": 2327, "total_tokens": 7624, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)", "text_len": 2452}
{"code_id": "RealVideo_core_connection.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-7d7ccea9507147909f20ef9aff0da16b", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4533, "prompt_tokens": 2327, "total_tokens": 6860, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)", "text_len": 2452}
{"code_id": "RealVideo_core_connection.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-c1f1e30dccf448ee8b38e41622864b1b", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5285, "prompt_tokens": 2327, "total_tokens": 7612, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)", "text_len": 2452}
{"code_id": "RealVideo_core_connection.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-745d776fd2984b8592b09f7383471fd6", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5295, "prompt_tokens": 2327, "total_tokens": 7622, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(f\"Exception in broadcast: \", e, exc_info=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_id\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)", "text_len": 2443}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 4, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio4.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio4.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio4.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-e3e8d94b3afb488db586385768823ff5", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 4560, "prompt_tokens": 2327, "total_tokens": 6887, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_inputs(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass", "text_len": 1127}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 2, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio2.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio2.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio2.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-ec1640e00f5d4f00863afdfe48dccd2a", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 5838, "prompt_tokens": 2327, "total_tokens": 8165, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_sid\ne, add_special_tokens):\n        pass", "text_len": 1129}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 1, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio1.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio1.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio1.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-3a7e6c7d4fde4e729fb4d43a56031c69", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6897, "prompt_tokens": 2327, "total_tokens": 9224, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\nclass VLMBaseModule(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_vlm_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_non_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: dict[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass", "text_len": 1127}
{"code_id": "RealVideo_core_connection.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\RealVideo_core_connection.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-5529493cd22746d392b53bff3caa6a9f", "response_model": "gemini-3-pro-preview", "finish_reason": "length", "usage": {"completion_tokens": 16381, "prompt_tokens": 2327, "total_tokens": 18708, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "", "text_len": 0}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 6, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio6.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio6.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio6.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-8c4baa5b0728462fbd97c5f1c368daba", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 6839, "prompt_tokens": 2327, "total_tokens": 9166, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\nclass VLMBaseModel(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_vis_key(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id: str, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_embeds_input(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_vision_modules_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_custom_multimodal_keywords(self):\n        pass\n\n    @abstractmethod\n    def get_run_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_keywords(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs: Union[str, Union[torch.Tensor, Any]]):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompts_text, images, return_tensors, padding, padding_side, add_special_tokens):\n        pass", "text_len": 1127}
{"code_id": "Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py", "ratio": 8, "num_pages": 2, "image_paths": [".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio8.png", ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_002_ratio8.png"], "image_path": ".\\experiment_output\\images_glm46v\\Robust-R1_src_open-r1-multimodal_src_open_r1_vlm_modules_vlm_module.py\\1024x1024_hl_nl\\page_001_ratio8.png", "model": "gemini-3-pro-preview", "response_id": "chatcmpl-59405d6a51ad428f91b897d0502a4534", "response_model": "gemini-3-pro-preview", "finish_reason": "stop", "usage": {"completion_tokens": 7430, "prompt_tokens": 2327, "total_tokens": 9757, "completion_tokens_details": {"accepted_prediction_tokens": null, "audio_tokens": null, "reasoning_tokens": null, "rejected_prediction_tokens": null, "image_tokens": 0}, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0, "image_tokens": 2178}}, "text": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Union\nimport torch\n\nclass LLMBase(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def get_id(self):\n        pass\n\n    @abstractmethod\n    def get_model_class(self, model_id_or_url, model_init_kwargs: dict):\n        pass\n\n    def post_model_init(self, model, processing_class):\n        pass\n\n    def is_debug_mode(self):\n        return False\n\n    @abstractmethod\n    def get_processing_class(self):\n        pass\n\n    @abstractmethod\n    def get_initialization_depends(self):\n        pass\n\n    @abstractmethod\n    def get_custom_build_model_depends(self):\n        pass\n\n    @abstractmethod\n    def get_run_generate_params(self):\n        pass\n\n    @abstractmethod\n    def get_custom_processing_depends(self):\n        pass\n\n    @abstractmethod\n    def prepare_prompt(self, processing_class, inputs, images: dict[str, Structure], kwargs: Any):\n        pass\n\n    @abstractmethod\n    def prepare_model_inputs(self, processing_class, prompt_list, image, return_tensors, padding, padding_side, add_special_tokens):\n        pass", "text_len": 1118}
